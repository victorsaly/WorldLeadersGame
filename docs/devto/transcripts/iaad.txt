
Hello. Hello, everyone. Welcome to
AI driven development
day. This is a free online event
where we'll dive into the state
of AI inside of web development workflows,
get some great tips and tricks, some great,
ideas from experts within the community about how you can
utilize AI
in your,
your web development workflows
today.
My name is Daniel Kelly.
So very excited to be here with you today.
I am
an instructor, first and foremost. I teach
Vue. Js was my primary,
area of concern for for teaching for for quite a
while, but also teach things like AI driven dev now
as I've learned to adopt it into some of my
own workflows and have talked to lots of other devs
about how they've adopted AI into their workflows,
as well.
And, I will be your your guide through today's
today's schedule.
This is gonna be kind of a a laid back
event.
We here at Bitterbrains,
which is
the the,
organization that is sponsoring and putting on today's event,
have a lot of experience
putting on very large scale events. Today's is going to
be a little bit more laid back. We're just gonna
we're gonna be chill. We're gonna have fun together, and,
hopefully, we're going to to learn some good stuff. So
who have we got
on the docket today?
Well, there are several really great speakers. We are excited
to,
welcome Debbie O'Brien. She'll be our first speaker of the
day.
We've also got,
Tejas Kumar,
Kent c Dodds,
Phil Nash,
Justin Schroeder,
Garrison
Snelling,
Benedict,
Benedict Stimelt,
Mustafa Saeed, and I will have a talk later on
in the day as well. So looking forward to sharing
all that with you. Now,
this event is provided free of charge to everyone who
is joining today, and the reason that that is possible
is because
of Bitter Brains.
Once again, Bitter Brains is the sole sponsor of this
event.
It's the organization that several of the speakers are,
are are coordinated with and associated with. It's, it's, an
organization that has done a lot of good in the
development community.
In fact, let me just really quickly here
bring up my,
my screen.
So y'all will have to be a little patient with
me today as I share some of my screen,
and things like that. This is a newer platform that
we are,
putting this event on with, so it might take me
10 seconds or so to do some of the transitions.
I do apologize for that.
Okay. Great. But it's going to be smooth.
Okay.
Let me move myself over to the side.
Excellent. Okay. So let me tell you just really quickly
about bitter Bitter Brains.
So, Bitter Brains is a group of
developers,
content creators, and tech experts,
all in the business of providing developer education,
for for everyone. As I mentioned a moment ago, we
kinda really started out in the view space, but we
have,
expanded
that
quite rapidly.
By everyone, I mean absolutely
everyone. We've got we've we've taught devs from companies around
the world, big names like Microsoft, Google, Comcast,
and small companies, of course, as as well.
So our mission at Bitterbrains,
with this,
event and with all of the content that we put
out is essentially to make developers'
lives easier. Right?
So in BetterBrains, we've been in Vue education from the
very start. Ask any Vue dev if you know any
Vue devs. They've probably heard of us.
We've put on lots
of video courses at viewschool.
Io. We have a number of other dedicated
master classes like Mastering Nuxt, Mastering Pena,
as well as the Vue. Js bundle, which allows you
to group a lot of our products
into into 1.
We've also created a lot of world class developer
certificates, ways to prove to yourself and to potential employers
what your development skills,
or what level your development skills
are on. So we provide a way for you to
show off your skills in things like Vue. Js, Angular,
Nuxt, JavaScript.
We have a React certification
now as as well.
And we've also hosted a lot of the largest online
dev conferences. This is just 1 more we can add
to our list. As I mentioned, we're gonna play it,
relax, play it, chill today, and,
have a good time
because we want y'all to to learn something and get
something out of it. A lot of people have learned
from View School. Don't just or excuse me. From Bitterbrains.
Don't just take our word for it. There are lots
of great reviews online about how we have,
upped people's
game in terms of of web development.
Okay. So that is a little bit about View School,
about better brains. I'm gonna say that,
multiple times probably throughout the event.
Alright. Let me stop sharing my screen now.
Actually, you know what? Let me leave my screen shared.
And before we dive into the first talk,
let's take a look at the platform that we're going
to be living in today.
And, hopefully, a lot of you will join and be
a part of even moving forward after
today.
So what you're looking at now on my screen
is the NextGen
Dev Circle community.
This is a place that we have created
in order to help,
engineers,
developers just like you
grow and learn,
from each other and from official courses
how to work better with AI.
K?
There is
on our homepage here,
you can see,
the different conversations we're already having. So a place where,
you know, kind of like Stack Overflow where you can
talk about different things with other community members,
talk about the content that we're putting out, but also
just learn from each other.
We have this Start Here tab, which kind of gives
you an intro to what NextGen
Dev is.
There is the Forum. This is essentially that same thing
you kind of saw
on the the home page. It's a place to just
talk about all kinds of different content.
And then finally, we have the news and updates tab,
which allows us to talk to our community and provide
updates
for for everybody in the community.
There is the courses tab.
Here, you can see we've already got 3 courses in
the works, most of them,
almost completely finished,
teaching how to use AI in your everyday development
workflows.
Alright?
If you join AIDD,
if you if you purchase that master class, you get
access to these courses plus
even more that we are currently working on.
Awesome. And then finally is the events tab,
which is, of course, kind of where you're at today.
This is the first of many events, I'm sure, that
we will put on here at
AIDD
in order to help the community grow and learn from
experts within the community, but also chat with each other.
That's something we definitely want you, to do.
Use the chat in the,
the bottom of the the page here.
I don't know if I can
if I hit join live, we might have some weird
inception thing going on.
Not gonna do that. But you should see at the
very bottom of your screen, there should be a
chat button, and everybody should be able to see,
see chats from from others and add their own messages,
of course, there as well.
Awesome. Okay. So I'm opening the chat myself just to
see it looks like a lot of people have found
it. We've got hello from Switzerland.
Apologies if I don't see your message or where you're
from. I'm just kind of sampling a few here. We've
got hello from Brazil.
Hi, guys and girls.
Hello from
Utrecht.
I apologize if I butchered that. Greetings from Morocco. So
it looks like we've got people gathering from all over
the world. I myself am from Tuscaloosa,
Alabama.
So if we've got any SEC fans out there,
shout out. Hope you enjoy the games this weekend.
Alright.
So I'm glad everybody has has found the chat, and
they are coming in like lightning now. Hello from Poland.
Hello from Ireland, India, Belgium.
Awesome. Awesome. Awesome. Awesome. Love that energy. Love
everybody's,
everybody's energy for for being here.
Okay.
Let's
hello from yep. Great. Okay. Got a go dogs from
Justin.
Oh, War Eagle here.
Okay.
Alright. So next thing on the list, just a couple
of other,
just kind of,
how do they say it? Just a couple of other
items before we get to the real content about the
platform to make sure everybody,
gets the most out of this.
So we will host,
q and a time
after each of the talks.
So during those individuals' talks,
please be thinking of questions you have for them as
they're speaking.
And and during their session, not in this chat tab
that we just
went crazy with, not in this chat tab, but in
the tab that is labeled q and a. So right
by the chat button at the bottom of the screen,
Yeah. There's a a chat button with a little question
mark and a, like, little chat bubble,
at the bottom of the screen right next to,
the normal chat.
In that q and a tab, that is where you
will put your questions
for each of our speakers.
And if you find other questions in that tab that
you were very keen to hear the answer to, then
make sure you upvote that question because I will be
choosing questions that rise to the top in that tab
for each
speaker.
Okay?
1 other thing here
is do please refrain from raising your hand.
There is a a raise hand button at the at
the bottom of the screen. Please refrain from from raising
your hand during any of the talks or really at
any time during the event.
Why? Because, honestly, it kinda looks like nothing's happening on
your side. You don't get any real feedback that your
hand is raised, but the speakers and the hosts
see all the raised hands, and we just don't want
that, you know, taking attention away from us giving the
you know, us us talking
and sharing that great content with you. So please do
not,
raise your hand at any time during the event.
So,
yeah, all your questions go in that speaker tab or
in that q and a tab. So that's really how
you're communicating with the speakers,
not by raising your hand.
Alright.
Excellent.
Debbie says, you can only press it during my talk
if you use Playwright MCP
to do it for you.
That sounds like a nice little challenge.
Okay. Great, everyone.
We are ready then for today's
first speaker.
So today's first speaker is
Debbie O'Brien. That was my drum roll, by the way,
if if you
weren't sure what that was.
Debbie O'Brien is the technical program manager at Microsoft.
She's a Google developer expert, a Nuxt ambassador ambassador,
and she is a former Microsoft
MVP.
Oh, super cool. We've got a very important person,
in the house. She's worked as a technical lead and
consultant for various
clients.
She specializes in Vue. Js, Nuxt. Js, and end to
end testing with Playwright.
I also personally happen to know she has 2,
super cute little,
twin children.
Debbie is always a pleasure
to be around,
just an absolutely wonderful person as well as a wonderful
dev.
She has done some,
coursework for us at View School. She's done coursework for
other,
education platforms around the web, and we are so
very lucky
to have her with us today. So at this time,
I'm going to invite Debbie
O'Brien to the stage.
Give me just 1 moment.
Alright. Hopefully, I've found the right Debbie O'Brien.
Alright. Give her just a moment to come on.
I told you it would take me just a minute
for some of the transitions.
So I'm gonna sit here awkwardly to make everybody
feel just a little awkward like I feel.
Debbie, this is this I see Debbie in the chat.
She's here.
Oh, no. Debbie, you don't see the invite to join?
Alright. 1 moment.
Let
me try it 1 more time. So I'm gonna invite
cohost,
Debbie O'Brien. Nope. Debbie
O'Brien.
Okay. Debbie, it says we've sent them an in app
notification. Do you see that?
Oh, I can raise my hand. Debbie, raise your hand.
Let's see if I can invite you through that way.
To doing the 1 thing I told you not to
do, Debbie.
Alright. Let's see if I get a
a hand raise here.
I think there's, like, a 5, 10 second lag. So
uh-huh. Oh. 0.
Debbie, I see you.
Debbie raised your hand.
It says you raised your hand, but it doesn't give
me the option to invite you on the stage.
Oh, no.
Okay. What is the issue then? This would happen,
the very first speaker.
So my apologies, John. We will get it figured out
shortly. I,
Kiara, so talking to my support just a little bit
here.
Is there
is as a moderator, are you able to invite Debbie?
Yeah. That's so weird. Okay. Invite to
stage.
Debbie.
Uh-huh. I see Debbie.
I think
she is a cohost.
Debbie, I cannot hear you.
Can you hear? Okay. It's totally the wrong camera, which
isn't isn't ideal,
but I have no settings.
Oh, weird.
Yeah.
Well, you're here. That's the important part.
Device settings. Let me see. Let me change the camera.
Otherwise, I'll be
otherwise, I'll just be looking a different way all the
time.
Oh, I gotcha.
Elgato
Logitech. There we go.
There is. That's a better camera. Woo hoo. That that's
good. That's the 1. Microphone. AirPods. Come on.
Video mic. And that looks good. Okay.
Right. Can everyone hear and see me perfectly?
Raise your hand if you can.
Alright. Stage is yours, Debbie.
Alright. Hi, everyone. I've gotta actually figure out how to
share the screen now as well.
When they ask you to do a talk and then
they tell you 2 minutes before the talk, it's a
new platform that you've never used before, and it's like,
oh my god.
So, anyway,
this is the way it goes.
Right. I am share yeah. Yeah. Yeah. I'm sharing my
screen. Cool. So that's the hard bit over,
and now it's time to do the easy bit, Playwright
on testing.
So, yes,
we're here to talk about AI powered testing on browser
automation
with Playwright.
Right. Let's see if this works. Yes. It does. Okay.
You've already had an introduction to me. My name is
Debbie O'Brien, and, yes, I work at Microsoft now. And
my main role is building up the community around Playwright
and around testing and now, obviously, the Playwright MCP, which
has taken over the world. And, normally, you used to
see me,
like, riding my bike and running all around the world,
but now this is, someone else is there riding the
bike, and I'm just pushing. So, this is my new
life.
But it's great fun. I wouldn't give it up for
the world.
But, yes, I still do sport, but not as much
as I used to. Okay. So what are we gonna
see today? I'm gonna show you that how to use
natural language to book a swimming lesson for my boys.
I actually did this. This is a true. And, generate
a playwright test. The reason I chose this as an
example is because I went to book the swimming lesson,
and it did not work. And I got really frustrated.
And,
of course, as a developer, I just started debugging it,
and I found out it was a Chrome issue. And
then I reported it. And then I said, I'm gonna
record the video of it not working, but they fixed
it. They were very good developers, and they fixed it
on that Monday morning after my chat with them. So,
I then decided, you know what? I'll still go ahead
and do the talk about this anyway because they needed
a test. So I wrote the test for them.
So you're gonna see how to save a trace of
what the agent did, record snapshots of the steps taken
by the agent, and then some self healing tests on
some AI debugging for for those moments when AI doesn't
really do what you want it to do. So let's
dive in.
Obviously,
you should all know what Playwright is. Playwright is basically,
an automation library for automating the web and doing things
for, like, testing that you don't wanna do. It's able
to click and it's able to browse and it's able
to do all that. And then you throw model context
protocol on top of that, and MCP is just a
standardized way to connect the AI models, the LLMs, the
copilots, the agents to data sources and tools. So it's
able to connect it to Playwright, and that basically just
gives it superpowers.
So Playwright MCP enables
the agents to browse web pages and to do those
kind of tasks that you don't wanna do or just
generate tests based on natural language. So it's really cool.
It uses the accessibility snapshots to do it, and it's
fast and lightweight. And if you haven't used it before,
seriously,
I promise you, you're gonna love this talk, and you're
gonna go walk away, and you're gonna start booking swimming
lessons using the MCP server.
So
prompts, chat modes, instructions. This is really important. Some people
use AI to write tests, and then they kinda say,
yes. I am a 4 to be a black belt.
I'm looking at the chat at the same time.
So some people use AI and it doesn't work, and
then they're giving out and they're saying, like, this is
rubbish. This is whatever. You it's really important to the
chats and the prompts and instructions. Let me show you
my setup. Right? Now I probably spent
more time on this than I did actually running the
the the the the getting the test to generate.
And you might say that's takes takes too long and
there's too much work involved, but you've gotta think about
this as this is a start
to later doing so much more. Right? So you're kinda
architecting out things. So I'm in Versus Code here, and
I've got a dot GitHub folder.
Now inside that GitHub folder, I've got chat mode's instructions
and prompts. Let's look at instructions. These are my playwright
instructions.
Right? These are gonna be included automatically because I put
apply to all because I just do testing. And, I'm
basically setting the scene of, like, you know, these are
the tests that this is what you should do. This
is how you write a test. Right? I'm I'm basically
pointing it in the right direction. Then I have a
chat mode. I'm giving it the tools that it has
available to it like the Playwright MCP. I'm telling you
which model I want it to use because that's my
trusted model. And then I'm basically, like, saying, like, this
is your job. This is what you need to do.
This is what you're good at. So do do this.
Don't do anything else. Just do this. Right? And then
prompts. This is the reusable prompt. We're generating a playwright
test from a manual test plan. Right? So this 1
I can use again and again just giving it different
scenarios, giving a different test plan. And this is the
really cool thing about reusing these prompts because you can
create this once. You can spend a lot of time
actually, I used AI to help me generate all this.
But you can spend a lot of time setting this
up, and then you just have loaded different prompts that
you just, like, reuse again and again and again, and
bang, you've got everything tested.
Okay. Let's jump in. The next thing I wanna show
you is exploring and generating.
So
I just broke this video down into little steps just
so we could break it down. But this was me
sitting down and and and doing this.
But what I did before I actually went ahead and
and did this was I said, go ahead and do
this and give me a test plan. I didn't write
the test plan myself. I couldn't be bothered to do
that. So AI wrote me the test plan. And you
know what AI does? It gives you so much information,
too much information.
So, I basically said, okay. AI, can I calm down
a bit? And can you just, like, you know, give
me a little a really short test plan? Because, you
know, just give me the steps. I just wanna book
a swimming lesson in the hotel I'm going to. I'm
flying tonight. So here's my test plan in short version
that I wanna feed into AI. Right? I have no
tests. As you can see here, we're gonna generate them
right now. It's gonna take 5 minutes. So if you've
got anything to do, don't move for the next 5
minutes. Right. I'm gonna open chat,
and I'm literally just gonna put this in the context.
So it has it in there. It's gonna use that
test plan. And then I get my prompt, and I
basically just run it here. I say run it in
the current chat because that's the chat I have, the
swimming, test plan plan in there. And then I go
ahead and run it, and then I go have coffee.
Right? Now this is, basically gonna follow the instructions. I
love that. Use 2 references. The references used there are
the instructions, the playwright instructions and the typical, Copilot
instructions.
They're the 2 automatically included.
You'll see down the bottom, it actually has a test
generator
selected. Here's the to do list that it has created
for itself. So it knows exactly what it needs to
do,
and it's gonna follow those to do in the right
order and get everything done and come with a beautiful
test for me. So that is the plan. So the
first thing it has to do is start with parsing
the test plan and exploring the website.
So it basically has the test plan. There's the test
plan. It knows now what it has to do.
And, it's gonna go ahead and use the Playwright MCP
server to open the website to navigate to this website
and basically complete this test plan. So here we go.
This is the website opened up, and this is the
hotel I'm going to tonight for the talk. I'm giving
a talk in Croatia next week.
So this is where I will be,
from tonight or from tomorrow.
So it I've asked it to take a screenshot. So
as you can see, it's taking a screenshot there using
the Playword MCP to do that. This is really cool.
I just did it for no particular reason, but you
could do it for documentation or just for, like, kinda
show people, say, this is what I tested. But I
just thought it would cool it was cool to add
that in there.
So it can see the menu structure,
and it needs to look more carefully at the main
menu. It says it's having a couple of problems there,
and it's trying a different approach.
And it uses every time it kind of does, like,
a click, it actually takes a page snapshot to kind
of understand,
the scenario of what it needs to do. Now here,
it's using evaluate JavaScript,
to basically be able to click on something. And now
it can see the experience menu item, and it's gone
ahead. Again, it's using evaluate JavaScript here. And now it
has the experience menu items, and it's gonna it sees
the link. So it's kind of finding its way there.
There you go. There's the experience at the academies.
So it's gonna go and find that swimming lesson for
me, and it's gonna basically go ahead and reserve it
for me so that I can take my boys swimming.
That's the whole plan.
It's doing all this while I'm just basically doing nothing.
So there's the lovely swimming academy,
so it's gonna have to go and reserve that.
And, obviously,
you could say, oh my god. This is quite a
slow process. You know, I would do this faster myself.
And, you know, maybe you would. May maybe you could
use CodeGen, and you could totally just, like, you know,
do it yourself. Or you could maybe a really fast
writing test, and you could totally do this yourself. If
that's what you wanna do, totally do it. But think
about this as 1 test to later do a test
for many other things. And I didn't have to write
any codes. This is very low level code here. I'm
really just testing the website.
It is actually working because,
because
they fixed it, but before it wasn't working. So,
here we go. It's now,
it's got the swimming,
details, and it's gonna select the date from the calendar.
And it's gonna pick the date, the correct date, fourteenth
of September.
Yeah. This is using GitHub Copilot, but I'm using,
a particular instead of agent mode, I'm using my specific,
chat mode called test generator that I created myself. The
reason for that is I gave it specific instructions,
and then I basically,
told it what I want it to do and what
tools it's allowed to use. So that's, what you can
do chat modes inside,
Versus Code. Okay. So it's picked, 9 30. So we're
going swimming on the fourteenth of September at 9 30.
And, obviously, I have 2 babies, so it's gonna have
to choose the party size of 2.
Okay. Perfect. Yes.
It's clicked it. And
once it does this, it then has to go and
generate a test on everything. Now here we got the
usual Copilot. That was just to show you that it
proved to you that it's Copilot because you guys were
asking there in the chat.
So we continue working there, and we gotta choose 2.
And once it finishes this, it's gonna generate the test.
Now why is this better at generating a test than
just basically asking Copilot or any other agent to just
go and generate the test? Because it's taking the page
snapshots under behind the scenes so it has a better
idea of the actual locators that it needs to use.
And therefore, it can create a much better test than
just guessing. And also,
I don't work for this company. I'm just testing the
website. I don't have access to their code. So it
would have to, like, make things up and hallucinate to
try and figure things out. So by using the MCB
server, it's navigating just like a user would. It's doing
the steps that it was told to take, and then
it's recording into its memory what,
what it has done, what it has clicked on to
be able to go and generate that test. So all
the exploration is now done. The manual testing steps are
complete, and it needs to create the swimming,
booking dot spec dot t s file with the proper
playwright test structure. And remember, we gave all that to
him
to basically
know how to structure the test. So there's my test
just created.
And
just even looking at it straight off, it looks pretty
good. It looks like it's doing things. Now there'll always
be something that that doesn't work, and there'll always be
something that you're like, oh, that should be improved on.
But in general,
like,
it's pretty good. This is like it's a simple test
for sure, but it's pretty good.
Pretty happy with that. So that took, what, 5 minutes,
5 and a half minutes? And we've got a test
working, but now we've also got a whole structure set
up. Now because I set I I told,
in my instructions that it has to go and run
the test and and then keep working and iterating on
it until it passes, it's gonna go ahead and do
that. So that's really, really important. Now I can go
ahead. I've tested swimming. I can now go ahead and
test so many other things.
So the next thing I wanna show you is self
healing tests. So, basically,
same scenario here. We're just continuing,
and it's gonna keep iterating over it until it's
able to fix itself. And it kind of opens up
the HTML report as you see there in the terminal.
So it's able to then see from the context all
the kind of error messages. So it's got in here,
the error messages from here, and and, basically, there's the
copy prompt button there. You could copy that. You could
actually stop yourself and dive in and go, actually, do
you know what I'll do myself? I'll debug. Or you
could let AI figure it out by the actual,
prompt messages
and
let it run and kind of just reiterate over itself.
So that's 1 way of just basically,
self healing and letting it continue.
Okay. Cool. So I'm just gonna skip this, and basically
do it myself because I love figuring out, you know,
what's going on, what's wrong, etcetera. And you can see,
like, here's the summary of success. You complete the task,
and it, you know, does this whole thing and tells
you that that he's he's amazing, and he's done all
this cool things.
So, basically,
this is good. And remember the screenshots. We'll look at
them in a few minutes, but it's actually numbered out
all the screenshots, everything that it's done. So it's really,
really cool. So once you're happy with that test, you
can go ahead and keep that, of course. And then
you can go ahead and go through the code yourself,
and I highly recommend you do go through the code.
I use the trace viewer here, so I'm I'm using
it just in the Versus Code extension. You could also
run UI mode, and I can go through my test
and basically see every step of the of the way,
what's going on, what's happened, is everything working, is there
anything I need to fix, does everything look good. Looks
pretty good. It's a really nice way of, like, you
know,
going over things and making sure that they're the steps
that should have been taken. And is there anything there
that needs to be fixed,
any problems? There's a cookie banner there that that comes
up. So does that need to be handled, etcetera?
Cool. So let's
let's go on to the next thing, which is record
a cursor. So what happens when
you have something that like the cookie banner there and
it didn't pick it up? In this in this instance,
it just wrote a comment, handle the potential cookie banner
or overlays.
So we can go along, and we can click on
record at cursor, which will open up a browser window.
Look at the difference. It's a different browser window than
the browser that the MCP opened. So we gotta put
in the URL in there, which will record it, which
has been annoying. That's fine. And then we click on
that accept cookies button. And now that's given us the
exact recording of what
the the test needs to do to be able to
get rid of the cookie banner. And that's just in
the a simple way of basically just, you know, coming
along and adding or modifying some of the test as
you're doing things. And, of course, a cookie banner, you
might wanna put that in an if because maybe the
cookie banner is not always gonna show, because of persistent
context, etcetera, etcetera. So,
you can work and then obviously just remove that pay
await page too that's duplicated,
from the recording.
Okay.
The next is copy prompt. So this is another thing.
We saw that in the HTML reporter, but this is
also available in the trace viewer. So if you're, in
UI mode or in the trace viewer, you basically have
all your error messages here. I don't know about you.
I hate reading error messages. I just can't read them.
I just look at them and go, ugh, can't read
them. Just press the copy button. That's me. Copy and
then paste it into your agent of choice. I'm using
Copilot here, but you could be using whatever you're using.
It's totally fine. You can paste it anywhere,
into any agent. And then, like, look. There's my agent.
I'll just stick to agent mode instead of test generator
mode. And this is the prompt that it gets. Right?
It gets full instructions. It gets, like, the whole test
info, and it gets, the whole page snapshot, the error
details. So it's got a lot of info in there.
And that's how it's able to do a really good
job at debugging and figuring out. What's also really nice
is it's able to give you a much easier to
read, and and you can understand, you know, what went
wrong here, what it's suggesting as it fix. And there's
a couple of options there, you know, that it says,
you know, it can use this. You can use that.
And so that's kind of really cool and really good
for learning as well.
Okay. Next is fixed with AI. That's another way to
debug your tests and improve them. So,
as you're working in Versus Code, you can just get
this beautiful error message here. And, again, it's like, oh
my god. But look at this little sparkly icon. You
just click on that, and then, basically, it's just gonna
fix it right there in line for you. And it's
saying, like, strict mode violation. That's a common problem that
AI sometimes, can't figure out. So, you know, it needs
to kinda, like,
redo over itself. So if you're having this problem,
really easy fix.
And you could say accept here. Or you could say,
do you know what? I don't like actually that locator.
Can you just get my test ID instead? And you
could totally just, like, reiterate in the ask or edit
mode there and just kind of, get it to do
a better job.
Excellent.
Sorry. Am I talking too fast? I'm always I just
yeah. I re I've always talked too fast.
Screenshots and traces. Also, I'm like, I've got a clock
here, and I'm kinda, like, going I've only got so
many minutes. So they need to give me more time.
That's the problem.
Right. We talked about screenshots and traces, and we asked
the AI to give us the screenshots and traces. Another
reason I talk so fast is so that you'd know
it's not AI. It's really me.
So in here, AI created a screenshot for every step
of the test it took. Now this is really cool
because you can really just go through those as well.
Maybe you had to document them. Maybe you had to
send it to someone as a report and say, like,
we tested this. This is fully tested.
So this is kind of cool.
So definitely,
look at that. And then we get a trace. Right?
This is a trace from the AI, not the trace
trace viewer that we see in while debugging.
So this is the actual trace viewer that you basically,
I'm gonna just open finder there and I'm gonna zip
it, and then I'm gonna open it up so you
can see the trace that comes from the agent. So
this is quite handy if, like, you just I don't
know. You walked off and had coffee, and you let
AI do its job, and then you're like, I didn't
watch the screen. Did it really do what it was
meant to do? I have no idea if it actually
did it. So you can kinda, like, really go ahead
and check.
So I'm just putting the the zip file here, but
you could have just left it where it was and
and then, you know, wrote the root. And, basically, you
can just start up the, trace,
in the terminal
in here. Once you have Playwright running, right, so if
I need I need to be in a rep where
that Playwright is actually running. So,
there we go. Show traces. Did I spell that wrong?
Typical.
Okay. So this should open now.
And
once that does, we can go to now it's not
a it's not a great trace compared to the trace
we saw when we were debugging. Right? It's not full
complete.
But there's there's a trace that the agent did. Right?
And we can kinda, like, see you can see it
took a screenshot. Right? That's what the agent did. The
agent took a screenshot. Like, this is cool. So if
there was no screenshot there, you could say, hey. It
missed the screenshot for that. You know?
There there is clicking on the dates. You can see
it fully clicked on that.
It clicked on the baby goldfish option.
It clicked on the right time. So this is a
really, really nice way of just checking over
what the agent did and making sure that, yeah, that's
exactly
what I wanted and what I want it to do.
And you even have access to the network requests in
the console. I don't really wanna highlight too much of
the console errors in there because it's not my website.
But you can actually see everything that's in there, which
is kind of really cool as well.
So
how do I just do all that? How do I
get those,
screenshots?
Screenshots are actually by default. You just have to ask
it in your prompt. It just works out of the
box with the MCP server. But traces, you do have
to configure. So I'm in Versus Code here. Just go
to configure the Playwright MCP server. Just in case you
didn't know where this was, it's very easy to find
in here in the extensions tab, and you can just
click on playwright there and click on that gear icon
to open the JSON file. But if you're using whatever
else you're using, just open your JSON file where you
have Playwright installed or Playwright MCP installed. And you can
see dash dash save trace is what you need to
save a trace. And now I did dash dash image
responses omit because I didn't want the LLM to have
access to my images. That removes it from context, which
just makes things a little bit quicker and and stops
it from getting stuck on having to, like, retrue kind
of images and adding that extra things in there because
the images were for me and not for the LLM.
But if you wanted
the LLM, the agent to have access to the images,
then obviously don't put that in there. It's by default
the the agent will have access to all your images.
So just when you're doing a lot of screenshots, be
careful and think about, do I want the agent to
be reading through those screenshots as well and taking up
that much time? Or, actually, do you know what? They're
just for me. They're just for documentation. I'll just, you
know, omit them. So that's kinda handy to know. Cool.
So that was really easy to set up. Right?
And this is kind of it. Right? We've just published
new guides in Playwright. You'll see a new tab. It's
literally brand new, a new tab called agents. And I
want you to just, obviously, just go through it and
get started. You just gotta click on that button there.
If you're not using Versus Code, just open the read
me, and there's links to every other,
1 way of installing Playwright MCP server, so go ahead
and do it. And there's some lovely guides here on
how to browser automate, how to explore and generate tests,
how to generate tests, how to debug with AI. We
covered that a little in the talk. So you can
go ahead in your own time and go through this.
You can emulate browsers.
You can open up the Chrome extension or an Edge
extension,
for browser profiles and states. So there's so many options
in here that you basically can just have so much
fun. But now we have documentations,
so it's really, really cool.
There's the connecting to the browser extension. This is really
cool. So if you basically had a test user and
then you're already logged in, then you can just basically
be using that profile
for running your test, and it creates a little bridge
between the profile that that browser that's going to use.
So that's kind of cool.
And the configuration file, if you did have a lot
of things to configure, then you can do that as
well. And you got traces and screenshots. So I showed
you how to do saves trace in case you forget.
This is where it is documented,
how to save a log for the m p MCP
session. And the screenshots, they just work out of the
box, but in case you wanted to and I put
some prompts in there as well to make it easy
for you. You could also save PDFs just in case
you wanna do that, and you could set it to
a different directory. And then if you're using agents on
CI, just use headless. And then just some little things
about network and security. And we've also got a new
tool that I haven't even documented,
which is about secrets.
So you can check out the read me as well
for new things every time. We'll keep updating the docs,
and we'll keep adding new videos and new things.
But, basically, there's so much to explore, and it's really
easy. So you have absolutely 0 excuse. So now what
I'm gonna do is I'm gonna challenge you to basically,
say
15 minutes,
maybe 7. I'll give you 7. 7 minutes to install
the Playwright MCP server and get it to do 1
thing and generate a test. Do it. I'm gonna need
7 minutes. That's all you need of all your time
to be able to do that. So that is it
from me.
Again, check out our docs. Check us out on YouTube
for all the videos that we record, and we have
our Discord server. And I seriously encourage you
to give it a try. We're in this world right
now where AI is taking over. It's kinda like making
us all scary. It's like, oh my god. If if
it does all that, what am I gonna have a
job? What am I gonna do? Like, use AI to
your advantage. Get it to do the boring things that
you don't wanna do. How many websites out there need
testing? There are so many websites. Like, I couldn't book
that swimming lesson. I had to do it my phone.
I had to contact the company and tell them that
their site was broken, and they went and fixed it.
Nobody else is gonna actually call the company and tell
them that. Only a developer would.
So we need you we need you all to be
using these tools. We need the web to a better
place. So and, also, you can just fill out forms
and do boring stuff that, like, take those boring tasks
away from you. You don't have to write tests. You
can also just browser do a browser automation, scrape things,
get some data. Just have a lot of fun with
the MCP server. I really you're you're just gonna love
it. So,
that is all from me. And,
if you're in Croatia, I'll be in Croatia next week.
Well, tonight, I'll be in Croatia, giving a talk there.
I'm gonna be in Portugal in a month's time. Do
come and say hello to me at any event, and
do reach out to me if you do have questions.
You'll find me on LinkedIn.
Send me messages.
Find me on Twitter. Send me messages. Tell me what
you're doing. Share with me what you're creating with Playwright
MCP server. And, yeah, happy testing.
Thank you so very much, Debbie. Loved
it. Thank you.
Alright.
Let's take just a,
maybe 3 questions from the audience really quickly.
So our first question here
is, can you share the GitHub repo to look up
and understand the markdown instructions you configure for the AI
context?
Okay. So this 1,
I created specifically for this event,
and it's brand new. So I haven't actually published it.
It's on my computer. But I do have other similar
ones. Right? So this 1 was tailored
to this company website. I think when,
when you
do something, like, with instructions, you need to tailor it
to the company you're working for. Right? And you might
have, like, 3 or 4 different ones. So I found
that every time I've given a talk and given a
presentation,
it's been on a something different, and I've had to
modify things. So you can in my GitHub repo, if
you go to debbie dot codes and click on my
GitHub icon,
there is a repo in there. I'll I'll just see
if I can find it, but there's a repo in
there for Playwright MCP prompts, I think it's called. And
that has everything in there.
There's also the official Microsoft 1, which is awesome Copilot,
and there's some in there as well that you can
get, not just Playwright, but, like, many others. So,
definitely,
if you can't find it, just ping me on on
LinkedIn or something. I'll share you share the link with
whoever asked that question. But yeah. And I will share
this repo when I have time to actually when I
get off the plane, maybe next week, left after I've
been on the beach. I probably should rate lower my
hand.
I noticed that earlier, but, no worries.
Awesome. Alright. Thank you, Jeremy, for that question.
The next question
let's see.
So our next question is when testing a web app
that requires login credentials,
where do we pass in the credentials to give access
to Playwright?
Fantastic question. Never give anything to the LLM that you
would not
like to share with anyone. Like, your personal stuff's your
personal stuff. Don't give it to the LLM. So,
like I said, we have things like the,
the extension that you can create a, you know, a
test profile or whatever. But we now just have and
it's not documented because it's brand new, but it is
in the Playwright MCP Read Me, which is secrets. And
you can now save a secrets,
file with the the actual. So it's just like end
variables. Nice. And the LLM,
basically, you you tell it to, like I think it's
called x underscore password, x underscore user, and then it
can read the end file, but it doesn't actually read
the end file, but it can actually log in and
stuff. And that is brand new as of, like, literally,
I need to create a video on it, but I'm
getting on a plane, so I can't do that. So
you're gonna have to just wait a week, but you
can find the documentation under the Playwright MCP for now.
Okay.
Thank you, Damien, for that question.
Let's now move to 1 last question here. Let's see.
This looks like,
that one's got some good upvotes, but is pretty
long. Let's try to go for a little bit short
1 here. Okay.
We already have a playwright test in place using page
objects.
Where can I tweak the generation to also use these
page objects and or create new ones in the prompt
or in the instructions?
Yeah. So it's always confusing, isn't it? Like, what's the
difference between a prompt and instruction in the chat mode
and things like that? So remember, like, a prompt well,
let's start with the instructions. Instructions are something that's gonna
be included
all the time.
So if that's something that's like, you're instructing it to
always follow these practices. If page objects are something that
you're always gonna use, then maybe it might make sense
to put it in the instructions because that's how you
do your test. If page objects are only something you're
gonna do on 1 specific
test, then that would be in the prompt perhaps because
it's like, in this specific 1, I want you to
use page object. That's not kind of a common scenario.
I would I would say more in the instructions would
make more sense,
because page objects would be across the project. And then
the chat mode, you
you you could put in there as well, but I
I think the instructions are probably a clearer 1.
Awesome. Alright.
Pat, thank you for that question. And, Debbie, thank you
so very much, for your time and the wonderful
talk you've given us today. Really appreciate it.
Great. It was fun. And everyone I'm saying,
test it out. Go and use it. Go and have
fun. You have absolutely no excuse.
Daniel, I hope you've done it as well.
Awesome. Take on the challenge.
I I've done a little bit, but not near as
deep as as you have gone. A hundred a hundred
percent.
Thank you, Debbie. Alright.
Bye, everyone. Alright. See you later.
If I can get you off the
I'll leave. I'll press the button. I can I can
leave?
Awesome.
Okay.
Alright, everyone. So next up,
let me,
let me actually
let me answer just a couple other questions here that
are related to the conference just very quickly before our
next speaker comes on stage.
So this question asked by Carson
says, Is this session recorded and how
can we get this afterwards?
So the answer is that yes,
This session is recorded,
and we will,
be following up with,
with an email to that recording. And that recording will
be available inside of the Circle platform,
after the conference. I'm not sure exactly how long after,
but it will be available afterwards.
Okay?
And
we have 1 other question that I thought might be
important to answer.
Let's see. Where did it go?
So is there any way to get notified,
when the guest that I wanted to see comes on
stage? Are we sending emails for each each speaker and
when they start? We are about 5 minutes behind right
now, but we're mostly on schedule. So we don't have
any alerts
for each speaker. My recommendation
is that you visit the schedule,
on our website. There's gonna be a link to that
in the chat here in just a moment.
So do just kind of check back in periodically. We'll
probably end up being 5 or 10 minutes behind Each
speaker's allotted time, that's typically what happens. But do take
a look at these schedules. The link for that is
left in the chat,
right now.
Okay.
That's it there.
Fred says Daniel is wearing the 1 ring to rule
them all.
Yeah. My wedding ring switches fingers sometimes. Okay. Next up.
Let me share
my screen
and hide this question.
Excellent.
So next up, I am very excited to,
invite Phil Nash to the stage. So Phil is a
developer relations engineer at Laing Flow. He's working on AI,
agents, and MCP.
He spent,
a number of years in DevRel with IBM,
with DataStax,
with Sonar. Wow. Some really big names there and is
a regular on stages around the world.
Phil says when he's not working on code, you'll probably
find him writing, enjoying a good beer, or walking his
his sausage dog.
Would love to meet that little sausage dog. Alright. Thank
you very much, Phil, for being here with us. I
am going
to invite you on the stage.
Phil
Nash.
Invite to stage.
Here we go. I like to narrate my button clicks.
It helps me
get things done.
Phil is going to be joining us. Hopefully, this time,
the link will work correctly.
Oh, Phil Phil is typing in the chat.
This does not bode well.
Okay. Cool. So Phil's not seeing anything quite yet. It
seems like it takes just a moment. So perhaps from
now on, what we'll do is I will invite the
speaker to stage
before I announce them,
so we can avoid this awkward,
little moment.
Allen, o for 2? Yeah. That's fair.
I love all the suggestions in the chat. This is
great.
This is great.
Okay. Now it truly is getting awkward.
Yeah. Gotta prepare some jokes to fill the time. You
know, I I'm definitely not a comedian, and the last
time I asked ChatGPT to make jokes for me, oh,
it's a worse comedian than I am.
I do have 1 joke.
Phil does let's see.
Phil said he scared us. Turn off. Still nothing, Phil?
No invite?
Let me try it 1 more time,
and
then I'll get back to my 1 joke that I
have.
Let's see.
So invite cohost Phil Nash.
So we got Phil,
and we're inviting him to stage.
Y'all tune into the chat, guys. Stop looking at me
and tune into the chat. It's good stuff.
Okay. So here's
here's my 1 joke while the invite is still pending
to fulfill.
So I definitely do have the invite out, Phil.
Cohost, moderators.
Uh-huh. Fellow's
fellow's getting something.
We've got an incoming.
Alright.
Okay. So my 1 joke is,
I told,
I told a co coworker,
1 time that I,
I was planning on being a comedian 1 day or
or my desire was to be a comedian 1 day.
And, she she told me that that was the funniest
thing she'd ever heard me say,
which was,
very ironic and made me
made for a great joke, but it made me feel
really sad.
Great. Alright. We have Phil. Hey. How's it going, Phil?
Hello. Good. Yes.
So let me tell you what's going on.
When I'm in the,
watching the thing,
and you thing,
there's no little notification bar. The notification is out out
of the room.
So I was waiting for it whilst watching you, and
that was not the place to
Okay. That'll help as well.
Absolutely. I'll I'll make sure everybody knows behind the scenes
here. We'll get it working smooth. But, anyways, the stage
is yours, Phil. Thank you very much, sir. Thank you
very much. Let me also find out how to share
my screen. Here we are.
Okay.
Right.
It's an absolute pleasure to be joining you all at,
AIDD.
Thank you, for waiting for me sitting in the chat,
not knowing what to click,
and finally working it out. But we got there. We
got there.
I also, I love that I'm following, Debbie. Debbie, who's
off to give another talk later today. I'm, based in
Melbourne, Australia,
and therefore, it is, it is 1 in the morning
for me right now, and I have already given a
talk, tonight,
locally in Melbourne, which is, which has been fun. So,
it's, yeah. So it's absolutely great to be here.
My name is Sonash. Yeah. I I am a developer
relations engineer. Well, so, technically, currently, I'm at IBM, but
I work on, the Langflow project,
which is now part of IBM. So, so IBM recently
bought the company I worked for, and, and that's where
I am right now. So I'm a a a developer
relations engineer at IBM, but I work mostly on Langflow,
which I I think is very cool.
If you do wanna find me at all on the
Internet,
please please do. I'm all over the place, as Phil
Nash, basically, wherever,
you can find,
somebody on the Internet,
you'll find me as Phil Nash.
So, yeah, hit me up on on Blue Sky, Twitter,
LinkedIn.
If you have any more questions after this, I'd love
to answer them, because I'll be honest, I probably won't
hang around for the entire day because that will take
me to tomorrow morning, and I I'm not ready for
that.
Cool. I also wanna say I'm afraid, we won't get
to meet the sausage dog this evening because she is,
currently
keeping, keeping my wife company in another room. So, no
sausages.
But here we go.
Cool. So what are we talking about?
Prompt engineering is dead. I'm gonna start with that. Hopefully,
hopefully, that's okay, with all of you,
because,
it's not to say prompt engineering is not cool. In
fact, it's really useful. In fact, we have just seen
a whole load of prompt engineering as part of the
instructions and the and the prompts that Debbie was giving
to,
Copilot,
in Versus Code in her talk just now. A lot
of prompts to drive,
a lot of what we can get agents
and,
and and and models to do for us.
But I think it's dead. I'm afraid. I'm sorry about
that.
Instead,
it turns out, I think it's, we're entering the age
of context engineering.
Context engineering is not just writing prompts. It's not just
big explanations to a model about what,
we want it to do. It's very much,
more than that.
It's more than just writing complex alone,
can achieve.
It's the craft of creating,
the perfect environment for an agent,
so that it has the best chance to help you
create the best outcomes when you're working with it.
And I think this makes sense as a parallel
to how we've worked as developers for years up until
this point.
You know, we pick the tools,
that we are going to help us get the best
job done ourselves.
We're no longer writing machine code. Right. We write with
high level programming engines. We have frameworks that have abstractions
that mean mean that we can think about the problem
that we're trying to solve
and not,
not just about how the syntax works.
We, we have third party libraries and services that we
can use to handle bits of application logic that aren't
core to our business, but are still important as part
of our application.
We gather the documentation that we need,
for the components that we're putting together.
We get,
testing tools, as, as as Debbie was showing with Playwright
to to run automated testing for us.
As developers, we are always surrounding ourselves with the tools
that we need in order to get our job done.
We even build tools to handle things that are, you
know, hard to remember that that will do specific bits
for us as part of,
building an application.
I've never, I swear, never once written a CSS gradient
by hand from memory.
I always I always go and use something like Colorzilla.
This is a bit of a blast from the past,
I think, but this is, this is with gradient generator
I used to use,
and or or 1 of the more modern recent ones.
I've always always used that. In fact,
have you seen, was it Tiny Helpers that's, this this
application? It's built by, Stefan Judas,
and, and it gives a it's got almost 700
different tiny applications that do very specific things for developers.
And I love this. It there's there's so many of
them. There's almost too many.
A hundred and 36 for CSS alone,
because these these things are out there to allow us
to,
accomplish something that, you know, we don't have to necessarily
remember and keep on our heads at all times.
And so, you know, we're no longer in a situation
where we actually have to do all the building. That's
why we're here today. Right? We're doing AI driven development
now.
We are in charge of an agent or even teams
of agents
to do our bidding for us.
So I actually sort of like to think that we're
we're we're
a bit like the secret the secret service, like my
6 in the in the world of James Bond, in
the world of double o 7.
In fact, we, you know, we've we actually get to
play
as as developers today, we get to play somewhere between
the roles of m and q.
Like, we're there to give instructions to our agents.
Like, like, Em, we have to set up a mission.
We have to be clear about the objectives.
We have to try and control our agent when it
starts to go off the rails.
But, but we're also like Q. Like, we we provide
the agents with tools,
with which to go about the mission.
Like, sometimes it's something useful,
anywhere the agent something that's useful anywhere the agent wants
to go,
like a a bulletproof car or,
like the document scanner that you can see in, in
this thing right now.
And sometimes it's, you know, it's highly specific,
like an exploding pen.
This, I think, is the scene from this is,
Goldeneye. Right? That exploding pen is very specific to the,
to the outcome
of the entire
entire,
movie, entire plot. No spoilers, obviously.
But,
1 thing I think, that's interesting about Bond movies is
that no matter what happens,
Bond does seem to find a use for every gadget
he's given,
and that's what we should be taking as an example.
And so I actually think that what we need as,
explosive,
what we need
is,
is to when equipping our agents,
we need to think about what we, what we need
to do to help them out.
I'll get to a demo in a minute. But you
see,
agents,
can get overwhelmed with tools. Right? There are limits. I
think Cursor has a limit of about 40 tools.
GitHub Copilot gives you a a more generous limit of
about a hundred and 28.
But the underlying limitation to this is the, is the
LLM, is the model.
The more tools you provide
to a model,
the longer the context is, the more potential overlap there
is in usage of those tools, and the harder it
is for,
the model to select the right tool for the job.
And so that's why we get hard limits in in
something like Cursor.
But,
those limits are actually there to protect us. Because if
you try and throw a thousand tools at a model,
it is gonna get completely lost.
We don't want that.
And so when you're setting up an agent,
and you're telling them the mission parameters, you're you're setting
out the objectives,
you need to be clear,
in your prompt, but you also need to be clear,
with their entire toolkit,
so they can successfully
complete the mission as well. I think this is 1
of the most important things when developing with AI. You
know, you actually you wouldn't use the the Playwright MCP
server when you're not doing testing.
You use it around other, you know, in in for
a specific job.
And agents come with some built in tools as well.
Right? You have file manipulation. You have web search.
And, of course, there are many prebuilt tools out there
we've seen Playwright. We'll see plenty more, I think, today.
Over the course of today,
in terms of MCP servers that you can use to
make your development better.
But sometimes,
you also need your own custom tools. You need that
1 off,
1 off tool that has a very specific use case,
the exploding pen that's gonna,
make or break the mission.
And, I wanna show you what I mean by this.
I because it's really great when you are building, say,
a front end,
with something like Vue or React or Tailwind.
And there are many, many examples that these models have
seen.
They have,
a lot,
they can already work with,
and they know how to build with that. But if
you're building with something that's a lot newer, something that
the models may not have seen as many examples of
or even maybe none at all, they aren't as good
as what they do.
And so,
as I was saying at the start, I work in
a on a on a tool called Langflow.
Langflow looks a bit like this.
Let's have a quick look.
It's a it's a drag and drop builder that allows
you to build AI workflows. So this is a very
simple,
workflow,
where you get a chat input,
a prompt,
a language model here,
OpenAI in this case, and a chat output. And so
this is just very basic prompting,
but you have an awful lot more, than that. If
you check the left hand side, the components on the
left, you have a lot to choose from in order
to build up 1 of these prompts.
And once you've built it,
you can share it via API access.
That is probably a little too small, for you to
see on screen, but this is just giving some example
code about how to
how to run, this flow,
in Python in this example. And there's a JavaScript tab
and a curl tab.
But if I wanted to create an application that did
deal with that, and run a LangFlow flow,
I could do so. I could try and do so,
by prompting it to do so. That's, I'm in cursor
here,
and I've got myself got myself a TypeScript,
application.
And,
but what can I do? I I'll just ask it
to,
complete,
this function
by,
calling
the Langflow
API with
an input,
input to the function.
Not a very, complicated prompt. I can tell you that.
But,
what it's gonna do normally is,
it's it's thought about some stuff.
It has,
often, this actually goes and does a web search, although
it hasn't done it today.
And it's gone and written out a whole bunch of,
custom code,
using Fetch. And, you know, it's it's built its own
small client here,
And, and it's done the job. Like, that's probably okay.
It's decent. But I'm just gonna undo that all because
there's an easier way to do this.
Like, it turns out that if I go to over
here,
there is a, TypeScript library,
with which to call Langflow, and it's much easier.
It just takes a few lines of code, and you
can just call flow dot run.
But it doesn't know about that.
And,
even when it does a web search, it actually doesn't
find this. It finds an example Python code, and then
it kind of translates that Python,
into,
into JavaScript, which is not a bad approach. It's just
not the best 1 that we want here.
And so what am I gonna do? I'm gonna grab
Langflow again. I'm gonna go back out. And I I
think I saw somebody say saying to Zoom. And I
apologize. Like, this, this current version of this, of the
desktop application here that I have,
does not zoom the text.
I can zoom my, my components, though. So I'll try
and go in on on those.
And so what have we got?
So we're gonna build our own flow here. We're gonna
build our own flow that's gonna help out our agent,
in cursor.
And so I'm gonna start it with the chat input.
All pretty much all these flows start with the chat
input. Zoom in.
I'm going to give it,
an agent actually in this case. I'm gonna grab this,
agent,
which is a,
it's a tool using agent itself inside of Langflow.
I'm gonna stick with OpenAI for this. I am gonna
update its,
agent instructions to say
that it, I'll read this out because that is too
small. You are a helpful assistant that can use tools
to answer questions and perform tasks.
Your tool can fetch data from a URL, use it
to fetch information about the Langflow TypeScript client at this
URL, and then I just gave it the URL to
this documentation page here. And so I'm really just giving
an agent,
giving this
agent, a way to,
point at the right documentation for this.
I do need to give it its tool. So I'm
gonna grab the, I'm gonna grab the URL component,
turn it into tool mode, and pretty much all the
components,
here can be turned into tools that we have into
other agents.
And once we've done that, I just need it to
output as well. So grab the chat output
and connect that up.
And so
this is not a very complicated example because we don't
have a lot of time to show complicated examples. But,
this shows you that we have a sub agent here
that is capable of looking through the Langflow documentation and
finding the right stuff out
for our,
for our,
core agent, our cursor agent in this case.
And so,
I can chat with this right now. I can build
this to make sure it works, and we got a
playground to go and chat with it.
And there it is. It's gone and taught me some
stuff.
Let's just do a neat chat with it because it
knows that
no. It's fine.
It's telling me all of the documentation right now.
But what we're going to do, because now we have
this flow here
that is capable of of being an agent and looking
stuff up and responding to questions itself,
we can turn that into an MCP server itself.
So first off, I will just give it a better
name, line flow and typescript.
And secondly,
I'll just give it a better description
because names and descriptions are super important in our MCP
servers as well.
And so we can take our flows in this project
and turn them into an MCP server.
And even better, actually, we can, we can auto install
it. So I've got the cursor auto install here. And
if I just press plus there, I should be able
to return to cursor,
and find out that, yes, my Langflow MCP server is
here, and I have this Langflow TypeScript
tool available to my, to my model already.
And so let's create ourselves a new chat. Let's get
rid of the old 1. We got rid of all
the old code. And this time around, I'm gonna say,
add code to call the,
Langflow
API in TypeScript.
And if, models and agents are on my side today,
it will,
go ahead and use the MCP server to find out
about stuff.
Yeah. It's not gonna do it, is it? Of course,
it's not.
Although it has written a bunch of types for me,
which is nice.
Agents will never ever ever respond to you correctly. Nope.
It now wants to use Axios. This is cool.
I'm gonna try him 1 more time.
Undo all of that
because I have created, you know,
add code to,
use the Langflow
TypeScript
client,
to call the Langflow
API.
So it's making stuff up. Cool. Well,
very excited to see that it's not paying me attention
today.
Let me give it I tell you what, 1 more
go.
So use your tool to find out about the Langflow
TypeScript client and, complete this code.
Complete the function. Yeah. There we go. Yeah. So, in
this case, it's calling on the tool now, finally, asking
what is Langflow and how do I use the TypeScript
Script client. And so we'll get a response back from
our, flow within, Langflow,
in order to, then complete the code.
So it's gonna take a little time to do that.
It's It's got a bunch of examples back. Now it
knows to install DataStax Langflow client, so I'll let it
do that. And now it's getting the thing right.
Cool.
I always tell people I never work with animals, children,
or, large language models. But,
apparently, my job is to work with large language models.
So,
here it is. Now it is using the Langflow client.
It is calling client dot flow, and I will keep
all of that because I now agree with it.
Cool.
So
like I said at the start, prompt engineering is dead.
Not to say that prompting is dead in any way.
In fact, the
more you prompt it, the better you're going to get
responses out of both your tools and your your agents.
But context engineering, the craft of creating the entire environment
around you,
that or around the agent that is going to give
them the tools to answer your question, your problem the
right way is what you need to keep thinking about,
yourself.
Be a cue for your agent.
Provide the tools, provide the the surgical precision,
they're gonna need for that,
and build the tools that your agent needs.
Langflow is just 1 way to, to build an MCP
server quickly,
that's going to that that that can help you out,
in a in a a little bit like that.
And,
although there are many others as well.
Before I, before I stop this, I will say, I've
been publishing as part of Langflow. We have a a
newsletter called AI plus plus.
It is a, a newsletter that comes out every couple
of weeks, with just everything you need to know about
building,
with AI, with agents, with MCP. So if you do
wanna sign up to that, please hit that QR code
or I'll drop links,
into the chat once I'm done.
And otherwise,
check out LangFlow. It is a free and open source
tool.
You can contribute to it if you want to,
but, it's also there for you to use to build
AI flows, build MCP servers, and, and build context for
your agents. Other than that, thank you so much. Again,
my name is Swan Ash. I'm a developer relations engineer
for LangFlow, and it's been absolute pleasure to chat with
you today.
Thanks very much, Phil. Really appreciate it, sir.
Oh, thank you.
We got you on here eventually.
Sorry about that. I'm glad I got on. I'm glad
the agent eventually listened to me. You know, it's, yeah.
It's
it's too late in the in the night. I don't
know what's going on anymore. Oh, man. Yeah. You are
a hero, sir. That is
good good on you.
So let's, let's do some questions,
from the audience very quickly and then let you go
to bed.
So our first question
is
so from a security standpoint, how do you ensure that
agents accessing external tools via MCP don't expose,
vulnerabilities?
Yeah. That I mean, that is an absolutely huge question.
I I hope you're planning on asking that to everybody
who's coming along today because
you may well hear different,
ounces,
each time. What I think about when, when dealing with
with these things is, is,
what's it called? It's the
it's the lethal trifecta, I think, is what it gets
called. Mhmm. Simon Willison has written about that.
The idea that, like,
if you have,
tools that, can access private data, can access untrusted content,
and access external communication,
that puts you in the danger that the untrusted content,
is is
yeah. Slips a prompt in that that acts, that gets
your private data and then,
leaks it via the external communication.
And so,
it is important
that you,
you don't get all 3 of those, tools mixed up
at any 1 time.
I think it's most important
to,
to ensure
to to be careful with,
external communication, that kind of tool that is is capable
of,
you know, publishing a comment on GitHub or sending an
email. Those are the things that I think are the
most dangerous part of that.
And so either ensure that your models sorry. Ensure that
your agents are,
requesting your access,
to, to use tools like that. Don't just kind of
YOLO mode it when you've got, external communication as part
of your toolset,
and and let them do whatever they want to,
or just kind of do not allow for external communication
whatsoever.
That's that's kind of the easiest 1, especially when we're
developing like this.
You you you are less likely in in a lot
of situations to be to to to post something publicly,
when you're within your your your coding agent.
It's it's a bit more difficult where if you're talking
about more general purpose agents, because if you are, if
you do want your agent to manage your email,
you are you're fully giving them
those things already. You've given them everything, even private data,
untrusted content, and external communication all in just 1 set
of email tools. So, like, you you,
yeah, you wanna be just more on the ball about,
allowing them to do stuff. But just consider that the
lethal trifecta,
is, I'm just gonna drop a link in the chat,
actually.
The lethal trifecta is the, is the thing I would
I would consider,
when you're adding your own tools.
It is a bit of a shame, I think, actually.
With with MCP,
we are a bit bit more,
responsible for our own security,
than perhaps we'd like to be.
But that's at this stage, we you know, you have
to
you have to keep an eye on your security yourself.
Definitely. I, yeah, I heard somebody say 1 time that,
the s in MCP
stands for security.
So you definitely gotta be be careful. Yeah.
Okay. And
just another quick note. If anybody hasn't read the article
from Anthropic about their little case study,
where it eventually, the AI ends up blackmailing 1 of
the fake subjects, and there's it's it's crazy. It really
is. But, yeah, just be careful out there.
Alright.
So we got time, I think, here for 1 more
question, Phil.
Sure.
Is it let me bring this 1 up here. Is
it possible to use local AI models through something like
Alama?
I assume the question is, within Langflow.
And the answer to that is yes. We have components.
So I just used OpenAI today in my example, but,
we have components for both Alarm and
LN Studio.
And I've actually I've actually been using LN Studio more.
I have both Alarm and LN Studio on my, on
my machine.
I I I ran out of half a terabyte of,
disk space the other day because I had too many
models and things knocking around. But, LM Studio has actually
been working really nicely for me recently. And, and, yes,
you can use, both of them, inside LangFlow when running
locally.
Awesome.
Awesome. Alright. Thank you so very much, Phil. Once again,
really enjoyed your talk, sir, and, I hope you have
a great night.
Thanks. Thank you very much. Enjoy the rest of the,
enjoy the rest of the conference, everybody,
and good night. See
you.
Alright.
So,
before we get to our next speaker,
let me just quickly share a little bit about a
project that we've been working on at Bitterbrains.
Today, you've seen so far how powerful
AI driven development can be with tools like LangFlow,
with tools like,
Playwright
when it's applied in the right way.
Well, the AI Driven Development Masterclass
is where you'll learn to dive even deeper into AI
driven dev concepts
over and beyond the things that you've learned in this
event.
You'll learn things like integrating AI into your coding loop
with some automated
agents set up at strategic times.
You'll learn how to,
go faster inside of your IDE
with tips and tricks about how to, you know, use
use,
agents like Cursor's agents or Claude Code.
And,
this AIDD Masterclass
launches September
thirtieth
with early access and an exclusive discount for AIDD Day,
attendees.
So, we've got a link that's going into the chat
here in just a moment. My colleague is gonna drop
that in there. That will allow you to save the
date
to jump on this offer for the AIDD
master class.
I've got to teach a few courses,
or a few lessons in this this, this course. It's
actually a series of of multiple different courses strung together
to make up this master class. And I must say,
I I love the way it's being developed, and I
love the content that we have included so far. And
it is continually
growing.
So we invite you to go ahead and sign up
for that. The launch date is September
thirtieth.
Alright.
Next up in our,
schedule,
we have
Justin Schroeder.
So Justin Schroeder,
the name of his talk is Throw Your IDE Away,
CLI
Agents for the Win.
So, clearly, Justin has a preference for the the CLI,
and I'm looking forward to see how he uses it
in his everyday workflow.
But first, just a little bit about Justin.
So Justin is a partner at Braid LLC
and a full stack engineer.
He's the open source magician
behind FormKit,
which I've used personally in my own projects. Love the
software.
Auto auto
animate and Aero JS.
He's known for mixing sharp engineering with playful creativity.
He now uses AI to supercharge his projects and push
the boundaries of what a,
what a single developer can achieve.
And with that,
Justin, I think if you are,
if you are listening, you do need to whoops. I'm
still sharing my screen y'all. My bad.
Let's go back to Justin.
Let's see if we can invite Justin on stage, hopefully,
more seamlessly this time.
Alright.
Justin, you should have the invite now. I believe if
you exit the actual live stream, you should be able
to see the invite. I think that's what we learned
from,
from Phil in the last session.
Alright.
Wally, no. I didn't learn. I didn't already invite him
because,
this is what happens when you're when you're live and
you're not you're thinking about a thousand different things.
Awesome.
Alright.
Just
thank you, Phil. Sausage dog
while we wait.
Love it.
And
still no word here from Justin.
Awesome. And, Phil, I do see you're still online. So
if there's any other,
questions in the q and a tab
that
you see that you have answers to, would love to
have you answer there.
Awesome. Alright. We have Justin. How's it going, sir?
Yeah. So working on the sound?
Okay. Can you hear me? I can hear you.
Amazing. Amazing. Okay.
Alright.
A little confusing, but I'm here.
Yeah. And then I need to
I need to share my screen too, Daniel.
Yep.
Do you see do you see where you do that
down at the bottom?
Yes. There we go.
Awesome.
Alright, man. I'm gonna hide myself, and you're up.
Okay. Hopefully, you all can see my screen here. And
Daniel's right. This is throw your IDE away CLI agents
for the win. Hopefully, that got your attention because who's
gonna really throw their IDE away. Right?
Real quick, I know, Daniel did a great job of
introducing me. This is me. That's my Twitter handle, JP,
Schrader. I actually pronounce it Schroeder, not Schroeder. Just a
little fun fact.
And there you can see a bunch of open source
projects that I've worked on in the past, some that
I'm working on right now.
And
we're not really talking about any of them because we're
gonna get deep deep down into
AI land. Although you might be interested in that center
1 there, Demux, at the end of this.
Here's the real question that we all wanna know. Is
AI going to replace your job?
Well,
last year, most of you said no,
which is interesting, isn't it? Because I would contend that
probably,
you already have had your job replaced.
Now I don't mean that you're out of work,
but I do mean that what you do for a
living has probably changed. If it hasn't changed, it's starting
to change.
And that is something that we are all kind of
grappling with together and trying to figure out.
So
being efficient with your AI coding is gonna be extremely
important moving into the future.
So let's knock out a few quick questions. First of
all, should I use AI to write my code? At
this point, I can say comfortably, resoundingly,
yes. You should.
And is AI going to replace your job? Well, yes.
It will.
Will I still have a job? Yes. You will probably
still have a job, especially if you learn how to
code with AI effectively.
And then maybe the deepest question of all, you know,
do you still have value even if you lose your
job? Yes. You still have value. We're not gonna talk
about that. Just know that. I just want you to
know that you have value.
We are gonna talk about this whole AI coding thing.
So let's let's kick that off. I remember way back
in, 20 22, over 3 years ago now,
when Copilot first came out. Do you guys remember Copilot,
GitHub Copilot, which is just Microsoft Copilot or Copilot now?
When that came out, I was standing around a computer
with a bunch of my coworkers, and we were browsing
the website. And I literally said,
oh, this is the beginning of the end of what
I do, what we all do.
And,
you know, I my understanding at that moment was was,
was very poor, and I had a grainy image of
what the future was gonna look like. And as time
has gone on, I've been able to really understand and
refine that. And it's true. It was the beginning of
the end,
but this was before the AI revolution
really began, and we didn't understand
what it was gonna mean going forward. Because just a
few months later
is when
ChattGPT came out. Now isn't that interesting?
In a way, you could say that this AI revolution
really began with coding first. It's really been the first
killer app even before the chat applications came.
So
what was GitHub Copilot? Well, back in the day, it
looked something like this. It basically just would autocomplete on
the single line that you were on. And this was
mind blowing because up until that point, there was nothing
that understood my code well enough to give me a
contextual autocomplete.
The you would get maybe TypeScript autocompletes, but that was
it.
But now we had these contextual autocompletes, and that was
incredible.
But it wasn't that long until Cursor came out. So
you can see here from June to March, so just
under a year,
Cursor came out.
And Cursor
really changed the game because it didn't just give you
auto completes on the line you were on, it gave
you suggestions
across the file you were in.
And this was incredibly powerful because now it felt like
somebody was almost coding for you. You know? It would
start to pick up, oh, you're trying to build this
feature? Let me jump ahead and and get that for
you. But, of course, it didn't stop there because in
November of 20 24, Cursor once again pushed the boundary
forward, and that's when agent mode came out. Now cursor
was not the first agent or anything like that, but
this was the first time that developers were really exposed
to a coding agent in a way that actually started
to affect the way you do your job.
And that was powerful.
I mean, agents could go off, and instead of just
operating on the 1 file we were in, now we
could operate again across lots of different files. And I
don't know what your first impression was of agents or
if you even had this moment yet where you realized,
woah. I can relinquish a lot of control to this.
It feels scary. To some degree, it is scary. You
need to be careful. But there was a moment where
we all kind of sat back and said, wait. I
can just say,
make my app better, and then it tries to do
its best to do that.
That's unbelievable,
and it was.
Now
remember, that was in November.
In February, just a little bit later of this year,
Andrej Karpathy says, there's a new kind of coding I
call
do you know what it is gonna be?
Vibe coding.
This is the moment that that term got coined. Now
think about it. How many times have you heard
vibe coding?
It's ubiquitous. I think my wife knows what vibe coding
is at this point. Not from me because I don't
talk about it. She is picking it up just from
the ecosystem around her because it is such a big
deal. That was this year. That was just February.
That's that's how fast things are moving.
So Andre says, there's a new kind of coding I
call vibe coding where you fully give in to the
vibes, embrace exponentials, and forget that the code even exists.
It's possible because the LLMs
are getting too good.
Also,
I just talked to composer with Super Whisper, so I
barely even touched the keyboard. I asked for the dumbest
things like decrease the padding on the left side by
half because I'm too lazy to find it. I accept
all, always. I don't read the diffs anymore. And when
I get error messages, I just copy and paste them
with no comment. Usually, that fixes it.
The code grows beyond my usual comprehension,
and I'd have to really
read through it for a while.
Sometimes the LLMs can fix a bug, so I just
or can't fix a bug, so I just work around
and ask for random changes until it goes away. It's
not too bad for a throwaway weekend projects, but it's
still quite amusing.
I'm building a project or a web app, but it's
not really coding.
I just see stuff, say stuff, run stuff, and then
copy paste stuff, and it mostly works.
So probably your boss, if they were listening to that
description,
it would have a panic attack.
But in just less than a year, this idea of
vibe coding has expanded
dramatically.
This idea that maybe the code itself
isn't the important part.
Maybe it's everything that produces the code.
So, of course, we're talking essentially about cloud code, and
you can do crazy things with cloud code. You can
say, you know, build here here build me an app
with an initial investment of hundred that'll become the first
VibeCoded unicorn.
And this will just go off and do it. And
every now and then, it's gonna ask you for a
little bit of input,
which you can give it. But, of course,
we are using this in our production apps now. So
what's the difference? What has happened since Andre Karpathy had
that and since people like myself are using this for
production code?
What happened? How did we increase the quality, and what
kind of workflows are in place in order for this
to work? That's the important question.
So Claude Code comes out February 20 fifth of 20
1.  Again,
crazy. That was this year, February 20 fifth.
And it's a CLI agent.
This is the most perplexing thing. I remember when I
first heard about Cloud Code, somebody came into our office
and was talking about it, and I was like,
I don't know. I like I I particularly like having
cursor here in my IDE where I can see all
the diffs. It's super nice. It's easy. And they were
like, well, yeah. Yeah. Yeah. Yeah. You you can still
use c diffs and things like that.
So I decided just because I trust this person, I'm
gonna give it a shot.
I gave it a shot, and it changed my work
completely.
And it changed everybody's work because it wasn't that long
until codec CLI came out. Look at that. From February
to April, and all of a sudden OpenAI was releasing
a CLI agent.
And then April 20 first, just a few days later,
is when OpenCode came out, which is sort of an
open source alternative to these 2.
And then just in June, we got Gemini CLI. That
was only in June.
And, of course, most recently now, we actually have a
Cursor CLI where they've taken the agent
that's in Cursor
and pulled it out into the CLI.
Things are changing fast. And what's so interesting
is all of these
are CLI agents.
We're not talking about more Versus code forks.
These are all CLI
agents, which brings up
a really important question. Why?
Like, we are all developers, and we enjoy our IDEs.
Why are we doing things back on the terminal interface
instead of
the integrated developer environment?
Well,
there's a few good reasons.
1 is
you can use any IDE you want. Right? So people
that were maybe in Neo Vin land
or, you know, they were still on, I don't know,
Sublime Text or something like that,
they now can participate too.
And and the reality is there's tons of IDEs out
there. There are so many of them. And so the
idea that you have to switch your entire environment
in order to use an agent was a little unpalatable
to a lot of people.
How about the fact that the actual development of the
agent can be much faster? So Cursor, for example, when
it wants to release new features for its agent, it
needs to make sure that the UI can reflect it
properly, that it's tightly integrated.
There's a lot more work than just having a terminal
interface. That's the lowest bar possible. So the iteration on
this is faster. And if you if you're telling me
if I use a terminal, I can use a better
agent, I'm in. I'm in.
Native APIs. This is a big deal. Like, the terminal
is familiar already for most developers, and that's who we're
talking about here. We're talking about a tool for developers.
So the idea that I have to type in some
commands and a few flags is not intimidating for most
developers.
And then it's portable. You can take your CLI agent,
and you can use it anywhere. I can throw it
on some VPS that I have hosted on DigitalOcean, and
I can spin up cloud code.
I I can use,
use it in CI on GitHub actions. I can use
it right on my OS. I can use it across
OS. It makes it incredibly portable.
If you're still doing development on Windows, some of these
agents don't work there, but you can use WSL or
something like that. The idea, though, is, essentially, you have
this incredibly portable agents that you can move from place
to place very, very easily.
But what if none of those are the real reason?
What if that's not
why
agents
on the command line took off?
And this is an idea I'm still playing with, to
be honest. Like, I'm not sure that I'm even a
hundred percent bought in because it's so dramatic. But what
if we just don't need IDEs anymore?
I know it sounds blasphemous,
but
Corey or or Boris, Cherny kind of predicted this, the
creator of Cloud Code,
way back earlier this year. Here's what he said on
the stage. He said
somebody asked him a great question. They said, why why
did you choose, a CLI agent instead of putting an
agent in an editor? He said, we see, speaking of
anthropic, up close, how fast
the model is getting better.
And I think there's a good chance that by the
end of the year,
people aren't using IDEs anymore.
That was earlier this year. In fact, I believe that
was,
in May of this year. Right? So
just a few months ago,
he's already talking about the end
of IDEs.
Now I'll be honest. I'm not quite there yet. I
still crack open my IDE from time to time, but
I have, on a couple of occasions,
actually shipped projects
without opening my IDE.
I have done that.
So I'm gonna show you my workflow
for how I do that.
Are you skeptical?
You should be skeptical.
I was skeptical.
When I heard this quote, I literally told my coworkers,
I said, I don't believe that at all. That's not
gonna happen.
I think I was wrong.
To prove that I was wrong, I decided to make
a scientific survey of the amount of time
that I was using my IDE. And as you can
see here with this very precise chart, over time, I
started using my IDE less and less and less. And
that is science that's irrefutable,
so
pay attention.
Here's what I do.
Starting from scratch, brand new project,
I don't even scaffold it using Vite or something like
that if I'm gonna do a web project.
I start with the CLI
agent by describing
the project that I wanna build.
Then
it scaffolds it out for me,
creates all the little files, the little git ignore files,
and all of that. Does all that for me.
Then I tell it about the architecture.
This is the high level thinking about it. Like, what
kind of database am I gonna have? How's my API
gonna work? What integrations are gonna have? And I tell
it about it, and I ask it to help
create those little sockets throughout my my project, and I
get it ready.
And then comes the cycle, and the cycle is a
feature addition cycle.
It starts by giving it a prompt for a single
feature, but something comprehensive. So for example, a single feature
might be like I want to create, you know, this
series of API. I wanna create a a user and
I wanna create a way for, for us to be
able to get users out of out of the database
via an API endpoint and put new users in the
database with an API endpoint. That's a pretty comprehensive 1,
but that's the idea. You wanna have a comprehensive feature.
And then after that, you manually test it. This is
where a little bit of human in the loop still
comes in. You actually go in and you make sure
that it's doing that. So if it's an API endpoint,
I usually have,
PAW or Postman or something like that pulled up, and
I'm pinging those endpoints, and I'm making sure it looks
right. I can put the right information in, and the
right information comes back out.
And then I ask the AI once I know I
like it, I ask the AI
to codify that with some tests.
And, generally, I wanna go as much towards an end
to end test as possible. So things like playwright are
great because I wanna test as much of the stack
as I possibly can,
because I'm not gonna be touching this code. So I
wanna know that from end to end, it's gonna work.
So I have it right tests for me. And then
finally,
once I'm sure this is
something that I like and that's well tested,
I update the rule file with a description of how
that works. So in the case of Claude code, that'd
be the Claude MD file.
If it was cursor, they have their own rule files.
So I start to give it a little bit of,
description. So the next time I spin up my agent,
it already knows about this feature. It doesn't have to
spend tokens and rediscover the feature. It already knows it.
And then if it's good,
it goes. It ships. It's done.
Pretty crazy. Right?
So let's go through the process of actually building out
1 of these projects. And I wrote out some actual
prompts that I would use
if I was creating a,
like, a Slack clone for an internal company. Okay? Here's
how I would I would start it. Create a new
project named Acme Talk, a corporate internal real time communication
tool for employees of Acme Corporation.
It functions like Slack with individual channels and direct messaging.
Acme Talk is web based and runs in any employee's
browser, scaffold the project out using TypeScript with Nuxt 4
and Tailwind 4, Initialize the repository and be sure to
include
boilerplate project files like dot editor config, dot ignore, package
JSON, Prettier, etcetera. Finally, create a CloudMD file that describes
the project and its goals.
That's a scaffold.
Now, honestly,
I might even make it longer than that. You wanna
be extremely,
extremely in-depth on the direction you're heading, or it's gonna
start including all kinds of stuff that you don't care
about.
So be very precise in your prompts.
Now that being said, every model is different, and this
is 1 of the things that I can't tell you,
I have to show you,
but models are different. GPT 5 is gonna act very
different from anthropics models.
And the reality is without doing it, you don't build
up an intuition for this. You know, there's a there's
a big debate right now of, is it better
to learn to code with AI now or learn to
code with AI later? And the and the argument goes,
well, if AIs are just getting better, then it'll be
just as easy to jump in later as it is
now. I'm sure you guys have heard an argument somewhat
similar to that.
And I think it's actually wrong, and I think it's
wrong because of this.
There is an intuition
about models, individual models, but about models as a whole
that you need to build up. You personally, It's your
own intuition. It's the same intuition that when you get
a a a bug,
you sort of know where to look in the code
automatically.
And when you were first developing, you didn't have that
intuition. Do you remember how frustrating that was?
When a semicolon was missing or a a quote was
a single instead of a double or something like that?
You would spend hours
trying to interpret
that cryptic error message and find out where to go.
The same thing's happening in AI right now. You get
in there, you give it a prompt that you swear
up and down is a spectacular prompt, and then you
run it,
and it's garbage.
And you don't understand why. And it takes a while
to build up that intuition,
which is 1 of the reasons I think it's more
important than ever as developers that we start collectively and
individually building that intuition for what good prompts
are.
I do agree
that prompt engineering as a discipline is dead. This is
just something that you're gonna need to learn to do.
Like, you learned to Google well. You're gonna learn to
do this well too.
Alright. Then I move on to architecture.
Please set up Drizzle ORM using a SQLite database, create
a migration and a schema for the users table, which
should include an ID, email, password status, last seen, run
the migration, use DrizzleKit,
ensure the results are correct. So here, I'm just this
is a very simple app, so I'm just kinda starting
to set the foundation here for the architecture, but I'm
not really giving it any actual code feature to run.
This is just architectural related.
And then at the end of that, once I've set
up my architecture, I would say something like this. Update
the Cloud MD with details on how to interact with
our database.
Now why don't I go update that CloudMD myself?
Well, the reality is that that is all within the
context window,
everything that just happened. And what the AI did might
be slightly different than what I was thinking of doing
or something like that, and I want that to be
compressed into my CloudMD file. So that way, the next
time I pop it open, it already knows about this
user table, for example, or or at least about how
my schema
is set up, you know, that this is a drizzle
and that there's a schema TS file that it made
somewhere.
So the key here is to always do this thing
where you update the rule files. Always do that in
the same context window.
Right? So after you've performed the operation, that's when you
wanna ask it to make the update because it knows
about what just happened. If you ask it to make
an update to that in a new session or something
like that, it's gonna try to gather some information, but
it'll be missing the most important part of the scope,
which is what just happened.
Alright. Now you go into that feature loop. Right?
So
this would be an example of a prompt. Create the
endpoint
post API users for creating new users. Endpoints should all
be located in the server API directory and use Nitro
JS as the server. Also, create a get API users
endpoint to list the existing users. Password should be stored
only as hashed values and should not be included in
the get API users endpoint.
Use curl to check your work. Now I gave it
a little thing there right at the end. It says
use curl to check your work.
Do that.
Do that in every feature you add. It doesn't have
to be curl. It could be whatever the tool is
to give your model eyes. You want to give it
eyes to be able to see the work that it's
doing. Otherwise, it does its best and it comes back
and says, okay. I did it. And then you run
it and it's not good.
And the reason is these models are just probabilistic.
They are not going to get the right answer every
time. Even the best model in the world won't get
the right answer every time because even the best person
in the world won't get the right answer every time.
So instead, what you wanna do is you wanna give
it some way for it to check its work. And
when you do that, it will be right
almost every time.
Almost every time your models will get your features right.
If you write your your feature clearly and it has
a good understanding of the project it's working on and
what tools are available for it, so on and so
forth, and you give it away to look at itself,
it will do a great job.
Alright. Finally, we're gonna manually test this. Give me a
server URL. I can ping myself. I've started doing this
instead of spinning up the server myself. Isn't that crazy?
It's just literally easier to do that, but, also, then
it can interact with the same server that I'm looking
at. It's very easy. It knows it spun up that
server. I can say I wanna make some change. It
goes and looks at that server that I just spun
up. We're we're working on it together. It's like a
it's like a like a co partner with me.
And then finally, once it's all good,
I get a little pat on the back. Hey. Those
endpoints are great now. Create tests using Playwright for those
endpoints. We specifically need to test the ability to create
new users, that the passwords are hashed,
and that we can list those users back out without
listing the passwords.
This is the artifact that you're left with at the
end of the day to make sure that things don't
regress over time. Okay?
And then
finally, we update the rules. And once again,
you want to do this in the context window that
just created that new feature. Now not every new feature
should end up in the Cloud MD file.
You're gonna get too much in there. And and everything
that goes in there generally needs to be pretty terse
and short. You don't wanna get it too big. But,
for example, it knowing
how to run tests, like, what what the test tests
are that it should run and that it needs to
be running tests while it's working on it or where
my API endpoints are stored, things like that,
very important for that to be in your Cloud MD
file.
Okay. So if you remember,
this was sort of the cycle
for that,
no IDE workflow. Of course, you can use it with
an IDE as well, but you can actually get pretty
good results without 1.
But what if it didn't look like this?
What if instead of there being a single feature cycle,
there were
many
parallel cycles happening at the same time?
Yes. I know that your IDE can do some of
these things,
but your
CLI agents will crush with parallel.
Now this, unfortunately,
is a conversation for another time. The good news is
I am doing an entire course,
on what Daniel talked about, the AIDD course,
and I talk about this kind of stuff. I'm gonna
talk about parallel agents, and I'm gonna talk about workflows
and best practices and all that kind of stuff. This
is just a little a little little sample, a little
teaser.
I'd love to get into more of that, and, and
I'm gonna do so.
Like Daniel said, you can you can go, to the
website and,
get early access. And in fact, if you scan that
QR code,
that will take you to the same link that, Daniel
was talking about. Other homework here, you can follow me
on Twitter. This guy is me. And, and you can
go to a I d d dot I o and
get signed up on the newsletter,
for when all of this stuff is gonna launch. But
that's me,
and, hopefully, I'll get some questions here.
Alright.
Thank you very much, Justin. I do have a couple
questions for you here, sir.
Let's see
if we can pull some up. Do you mind,
taking your screen down there?
Yeah.
See if I can figure out how to do that.
Oh.
Alright.
So first up,
we have, will you be sharing your slides somewhere? I'd
love to go back through them in detail to get
good understanding of the prompting.
That's from Damien.
Great
question.
I don't know.
They're in Keynote, but, yeah, I can probably put them
up somewhere.
Yeah. Throw them out on your x or something afterwards,
man. We'd love to have them.
Awesome.
And what what were you on x again, Justin?
J p schroeder, s c h r o e d
e r.
Alright. So sounds like Justin will share those after,
on on socials.
Alright.
Next up
oh, that's that's an interesting question.
Let's do this 1.
I
I said this question is interesting. Let me pick another
2. That was probably the worst lead in. But let's
go for this, this question next.
Why,
why Cloud is better than than Cursor?
It's It's a good question, and I think everybody sort
of wants to know why.
The reality is that in
AI engineering, like, what not not I'm not talking about
coding with AI,
but in building things that use AI. So, like, when
you're writing cursor or when you're writing Claude,
something that leverages AI. It takes
a tremendous amount of engineering,
like, way more than you think.
And that that amount of engineering, like, just having built
some projects that use AI, I I'm I'm painfully familiar
at this point with how hard it is to kind
of sheep herd the the AI agents to do exactly
what you want them to do. And at this moment,
Cloud Code seems to do the best job of that.
That will probably not be true
tomorrow or the next day.
You know, another strong opinion of mine is you need
to hold these tools really loosely. But at this moment,
it seems to do a better job, and that really
doesn't have as much to do with the model as
it does the engineering that went into creating Cloud Code
itself.
Awesome. And that that sometimes is a little frustrating to
me. It's like I I I think I heard you
say this 1 time, like, keeping up with AI is
kind of like drinking from a fire hose.
It's just so hard to keep up.
Yeah. That is a hundred percent true.
Alright.
So last question. And by the way, Justin will be
joining us for a panel in just a little bit,
so you can ask plenty more questions there as well,
for Justin.
But let's see here.
Let me go for
this question from Eduardo. He asks, how can we write
precise styling instructions
with the CLI?
I mean,
how we implement web applications with a pixel perfect design
without wasting a lot of time on refine again and
again every result that doesn't fit exactly with the designs
provided by the designers?
It's a good question. The good news is that these
models are multimodal,
so they can take images.
Sometimes that means, like, you just literally screenshotting and dragging
that into quad code or, you know, whatever you wanna
use, codex or whatever.
That can that can help. Another really nice 1 is
to just use something like the Playwright MCP
where Playwright can actually boot it up and it can
take a screenshot for you. You can say go use
Playwright, take screenshots, and then and then make refinements.
But in general, what I've done that's worked really, really
well is in my CloudMD file, I have a whole
section on UI
where I give it the rules, like, very precise rules.
Like, if it's going to be something that's z index
up, I want you to use this type of shadow,
and, you know, I want these sorts of padding.
And then, honestly, it helps to use a system like
Tailwind where things are not
quite pixel perfect.
They're pixel approximate.
That does help a little bit as well. You're still
gonna get really good results.
But those those, MD files, they help tremendously
with your UI. And then once you get so imagine
that that cycle that I was talking about. Let's say
you're working on a model.
The model should be a feature that you work on
in this cycle.
And then once your model is good,
you you have tests for that model, and then you
make sure that from then on,
your agent is aware that that model exists and to
use the model every time you're gonna have a popover
of some kind. And that way you get consistency over
time. So it's it's very much you are now the
orchestrator
of the of the, you know, the the junior dev
or whatever who's writing the code for you, but you're
orchestrating and make sure that the the the code quality
is there even if you're not looking at the code
itself.
Yeah. And I saw someone mentioned in the chat actually,
like, that some of these prompts that you're coming up
with, like, it takes a developer
to be able to prompt that 1. 0, yeah. That's
your job. Yeah. And I think that's a really smart
job.
Absolutely.
And and that's that's 1 reason I think that
developers are not it's it's not the end of our
careers. Right? We just kinda have to change our workflow
a little bit.
But it's not the end of our of our jobs.
It's not the end of our careers because our job's
gonna change.
Absolutely.
Awesome.
Alright. Thank you very much, Justin, for, being on with
us today. We will see you again in just a
little bit for the the panel.
Okay.
Nice to see you. Thanks, Justin. See you.
Alright, everyone.
We have had a great morning so far. At least
this morning for me might be nighttime
where you're at. And if so,
I'm glad you stayed on during your your evening time.
You could have been off, you know, I don't know,
watching whatever great show you love to watch in the
afternoon or playing tennis or whatever you,
normally do in the afternoons, but you decided to stay
and further
your career
by joining us here at AIDD. So we're very, very
thankful to have you all here and hope you have
had a good experience
so far.
Next up
Let me share my screen here.
Next up, we are going to take a little coffee
break.
Sometimes that fire hose is,
is turned on even at conferences, and we just need
to give our brains a little bit of a break.
So
15 minute coffee break for me, it is
let me take a look here.
It is 10 57 AM for me. So essentially, we're
at the top of the hour. So,
at at a quarter past the next hour, so that
would be 11 15 for me,
let's all be meet back for the next session. We'll
be about 10 minutes behind, but that's okay. Y'all go
grab some coffee, meet back at a quarter past the
next hour, and we will see you soon.
Hey. Hey, Ken. How's it going, sir?
Go and get oh, okay. I cannot hear you, which
is fine.
I muted myself. Sorry.
Cool deal. Well, I've got you on here just a
couple minutes early while they're finishing up coffee break because
we had a couple issues with earlier speakers getting them
on. And,
yeah, I think thought this would be a a good
way to go, make sure you're you're ready to go.
Yeah. Yeah. Actually, I I have the livestream open right
now. It looks like people can see me. So hi,
everybody.
Indeed they can. Sorry about that. I should have mentioned.
Anyways,
yeah, while they're finishing up their coffee break,
what's what's going on in life, man? What what what
interesting is happening in your world?
Oh, there are, plenty of interesting things going on in
my life right now. I,
probably the most interesting thing is, my wife and I
are expecting our sixth child in November.
So Congratulations. Amazing.
Yeah. Thank you. Yeah. We're very excited about that. It's
gonna be,
it's a pretty busy house.
But,
yeah,
more,
related to folks here, I just launched,
or opened up ticket sales for my, first cohort ever.
So I'm doing a 2 week cohort to teach people
about MCP.
You can go learn more about it at epic a
I dot pro. I'll put a link in the,
in the chat here too. Absolutely. Definitely do that.
Always love to see your your content. It's just top
notch stuff. So Oh, thank you. Yeah. Absolutely.
Been learning from some of your stuff. Not all of
it. I'm sure there's plenty I haven't seen, but, definitely
definitely got some some good knowledge from from your stuff
before. So I appreciate that. Thank you.
Awesome. Alright, Kent. Well, I think we are
timed, out for our coffee break now. I'm going to
quickly introduce you to the audience. You probably need no
introduction,
but it's on my script. So we're gonna introduce you,
and, then I will give you the stage there.
Sounds good. Alright.
So,
next up,
we have Ken c Dodds joining us here today. His,
topic for today is the new user interaction
model. I'm really excited to hear about this 1 because,
yeah, I I I kinda see things changing even not
even in our workflows, but in terms of what we're
producing for the end users. I think this is going
to be,
really, really interesting. So Kent has spent his, career helping
developers grow and push
what's possible on the web. He created epic ai dot
pro that you just heard him talk about just a
minute ago. We will share,
links to that.
And,
epic web dot dev, epic yak dot dev, testing javascript
testing javascript,
dot com, all platforms that have become staples in our
industry for learning,
new concepts.
So alongside his work in open source, he's a Microsoft
MVP,
Google developer expert, and a passionate
advocate for MCPs
and the role of AI in shaping the future
of web development.
Welcome, Kenzie Dons.
Thank you so much. Alright. Let me get my screen
shared, actually.
Hopefully, it doesn't require extra permissions,
that I didn't think about until just now.
I cannot find oh, there it is. Okay. Awesome. Let
me allow
and share
screen.
Here we go.
Okay.
Very good.
Alright.
Let me know if you cannot see it. I'm gonna
assume that you can, and we're gonna get right into
this. I actually just noticed
1 thing that is kind of funny, so let me
fix that. Okay. There we go.
So I wanna talk with you about the future of
user interaction.
My
subtitle to this talk is making it so Jarvis can
do stuff.
If you have not heard of Jarvis oh,
yeah. If you haven't heard of Jarvis before,
Jarvis is,
well, actually, I'll talk about it, during the talk. But,
yeah, we're doing some pretty interesting things in AI. And
if we were here in person, then I would actually
I have this tradition for my talks where I actually
invite everybody in the audience to stand up, and we
do air squats together.
I don't typically do that for remote talks like this
because it's just a little bit awkward. So,
we're going to leave you with that. But if and
we just finished the coffee break too. So, hopefully, your
blood is flowing so your brain can,
can learn stuff that's important.
Okay. So Jarvis, it's just a rather very intelligent system.
This comes from the,
stories of Iron Man. Iron Man is a technical genius,
and,
the Tony Stark is is the name of Iron Man.
And he made an AI that helps him with his
work, and his AI,
has a name. It's Jarvis.
Jarvis can do a ton of things for Tony Stark.
So,
he's a personal assistant, combat assistant, system control, emergency
response,
engineering support, research analysis, companionship,
just about everything.
And I think that that,
would be pretty neat to have a friend, or,
or at least an AI,
that can do all of these things. Maybe not combat
assistance necessarily. I don't really have that, need myself.
But,
I, for a long time, have thought, like, sure would
be nice to have a computer that can just do
a lot of things for me like that.
The, experience of Tony using Jarvis includes
natural language,
multimodal, so he can also do, gestures and show video,
and, Jarvis sees what's going on.
Jarvis immediately responds even if it's going to take overnight.
He's gonna say, I'll start the analysis now.
He's proactive.
I'm using the pronoun he, to describe Jarvis, but Jarvis,
of course, is an AI. So,
it, I suppose. But, yeah. So Jarvis is proactive, letting
Tony know when there's something specific, that he needs to
know.
There's no configuration necessary.
Tony just says do this thing and whatever tools are
necessary, Jarvis just sets up.
Jarvis is everywhere in his suit, in his car, in
his house, in his office, everywhere.
And I think that we are entering a world where
we could actually have something like this. And I think
that we're actually experiencing a paradigm shift right now.
So for the last more than a decade, I've been
teaching people how to build web applications
for the web because
most users consume,
your services
through the web or through mobile apps.
But the basic idea is you've got the browser and
then you have the website, and the website is kind
of the window into
the service. And I think that this interaction model of
a website is completely
changing because of AI and what we can do now
with the technology that we have.
And so I think in the future, browsers are going
to be replaced by or evolve into
Jarvis like clients, so, clients that have all those same
capabilities.
And, the way that we expose our services to users
is through MCP servers.
I think that this is a really exciting feature because
it it means that we can have all of that
Jarvis experience
with,
whatever service we end up need to use.
So,
let's talk a little bit about,
what, like, why don't we have Jarvis already? So what
what can we not do that Jarvis can do?
Pretty much nothing.
Everything that Jarvis does, we have the capability of doing
with the technology that we have today,
with the exception of some things like,
Jarvis,
accesses shield, FBI, and CIA datasets, we probably shouldn't do
that, not for technical reasons. And, also, Jarvis has a
bunch of, like, holographic stuff that, he can do. We
probably don't have the technology for that. But, like, the
important things, the actual tasks that Jarvis,
or use cases that Jarvis serves for Tony,
we have the capability of doing that. So question is,
why don't we already have Jarvis? What's, like, what's the
holdup? What's going on? And, actually, we've been trying for
the last over a decade
to build something that, gets closer and closer to Jarvis,
something that understands our natural language, something that's
proactive, that,
integrates with all the different things. And and actually, that's
the reason why we don't have a Jarvis is because
of the integrations.
So Google will build their APIs and and Microsoft will
build its APIs and Amazon and Apple, and they all
just have their own little platforms
that everybody has to integrate with.
And it just doesn't, doesn't pan out very well,
as far as being able to just do everything,
that you could possibly imagine.
There are websites that exist that do not build integrations
with everybody
for a variety of reasons. Things are always changing. You
don't have a big enough team, whatever.
So these integrations are just killer.
And,
it's just there's just too much. Everything has to integrate
with everything. Okay. Well, you want to accomplish this task.
That means we're gonna have to have all 4 of
these services talk to each other so that that task
can be accomplished.
And it's it's just too much. It's,
so what instead, what I would like is to be
able to have Jarvis and everything integrates with Jarvis,
and then,
Jarvis handles the integration points between the rest of them.
And that is what the model context protocol enables. And
that's why I'm so excited about MCP, about model context
protocol. So let's talk a little bit about MCP and
why it's able to solve this problem and how we
got here in the first place.
So,
for a long time, we've actually had large language models.
This is not a new thing in the last couple
years. We've been able to have large language models that
serve you, for quite some time. What made things really
take off in the last few years is the host
application.
ChatGPT
specifically
made it really easy for,
nontechnical
users to make use of LLMs, and that's the point
where
nontechnical
users discovered that LLMs can actually answer their questions. It
may not always get it right. Like, 3 years ago,
it was hallucinating a lot and stuff, but it does
have the ability to answer questions.
The problem is it can't actually do anything. I remember
when ChatGPT was first released,
you would see people post like, look, I just had
it run a, like,
spin up a Docker container for such and such and,
like, you would see it output all this stuff. And,
I didn't know anything. I was like, no way. It
can't do that, can it? And no. It can't do
that. It's literally just generating tokens, and it's pretending that
it actually is spinning up a Docker container. So,
at this point, it was just like faking it. It
couldn't actually perform any actions. It could just generate tokens.
And to this day, LLMs really just generate tokens. That
is their role. And so it's the host application is
where the innovation is coming. And and then, of course,
the model, like, improving and stuff, but the host application
is where things can actually be done. And so that's
where we get into phase 2, where the host application
lets the LLM know, hey. Guess what? I actually have
the ability to do stuff for you. So based on
the user's prompt, if there's anything you want me to
do, just let me know. And I've I've got this
list of tools, that you can can execute.
And that works because now it can actually do stuff.
The problem is it can't do enough, and the reason
is because of the that integration problem still. So whoever's
building the host application has to build an integration with
every single 1 of these tools. So you've got Slack,
and you've got, your text messaging service, and you've got
your calendar, and your email, and your, like, YouTube, and
whatever.
All of these different,
services need to be integrated.
And that's,
that's great. Like, it works okay for,
the big players, but what about the small time players?
What about the,
like, if I want to reserve a pavilion in the
park for my son's birthday party or something, I can
go to my, municipality's
website and I can go and reserve that on their
website. That's awesome. There's no way I can convince OpenAI
to add an integration for my city's website,
or my my city's,
service so that I can use a natural language to
reserve my,
my city's,
like, park pavilion or whatever. And now, of course, the
municipality
can,
wrap OpenAI and add its own tools, and now I
I can have a natural language control
of my, city's website.
But, like, who's going to do that? Like, why why
would I go through all the effort to talk to
all these different agents? I would much rather like Jarvis,
the Jarvis experiences, I have 1 agent, and I talk
to that 1, and it can do everything for me.
And so that's how we get into phase 3 with
MCP.
So, the idea is let's take this integration layer and
let's invert control. It's it's a classic,
programming principle of inversion of control. So now the integration
responsibility lies on the service provider
and,
the host application just,
hooks up to that integration that the service provider created.
That's what MCP is effectively. It's just this inversion of
control, letting the service provider manage the integration and now,
the host application can integrate with anybody who implements that.
And with this, now we can do anything. Any service
provider can make their own integration
and,
the the problem is that clients aren't ready.
So the and, actually, I should mention also that what
differentiates this from what we have already because, like,
Google has their integrations and stuff. What differentiates this is
the fact that it is a a standard integration,
protocol. And so you your service provider can build 1
integration and now any,
of the different labs or any of the different companies
can integrate directly because it's standard.
This is very much like the web. People build websites,
and they don't care what browser the users are coming
from. Any browser can access the website. Standards are really
powerful for that reason.
So clients are not quite ready yet. They're we're we're
getting there and and they're it's always improving. In fact,
just yesterday, I saw that chat g p t, the,
like, dot com is adding support for MCP
as well. So this is huge because that's, like
for a long time, we've had this in editors and
cloud, but those are a lot more technical users of
those.
Like, for for most nontechnical users, AI equals chat g
p t. Like, the those 2 things are the same
in their mind. And so having chat GPT build in
support for MCP
for the consumer,
that's huge. That is really huge. So,
let's expand this a little bit, to talk a little
bit more about the architecture.
So MCP is mostly concerned with the, client and the
server. That's what's primarily standardized.
And, the service provider is responsible for the MCP server
and the,
host application. So chat GPT or cloud or Versus code
or cursor, they're responsible for the client. And so the
user decides, hey. I wanna have these different,
services hooked up to my,
experience.
And so the host application will spin up a client
for that and connect to that server.
And that works out really super nicely because now as
the service provider changes their offering, they their APIs are
changing whatever, the burden is on them to keep their
MCP server up to date, which is exactly where it
should be. Gives them the power.
So I'm gonna give you a little demo of what
is, possible or what even I I made this demo
a few months ago. So what was possible even months
ago, and it's only getting better.
So I was in a hotel in Poland, so you're
not gonna see the location of my, my home here.
But,
here, I'm asking Claude to write a journal,
entry in Epic Me. That's a service that I've exposed
on, with MCP.
For me at kenzie dot com about my trip with
my son, get my location and weather conditions, make up
a creative story, and add tags. Thanks. So typically, you're
gonna write your own journal entries, but I'm gonna have
the AI do it for me. So first, it's going
to,
authenticate with my MCP server, so I'll allow that.
And that that sent an email to my, email. So
I'm gonna head over to my inbox,
grab that, token
to verify that, yes, I do indeed own that email
address so you can
validate that,
me with the Epic Me service.
And now,
it's also getting my location. So here, let me back
this up a bit, because this is kinda interesting. So
this locationator,
MCP server is a completely different server. So you've got
the epic me and now you've got locationator,
and this is going to, retrieve my location. This 1
is actually running locally on my machine,
so that I can determine my precise coordinates at the
time I recorded this video.
And then I use that,
as input into weather. So this now we're on 3
MCP services to accomplish this 1 task,
where each 1 of them is playing a distinct role.
None of them know about each other. This is the
beauty of MCP is that you just build to this
standard
and then the AI is the integration glue between them.
So integrations, we no longer have to worry about. That's
what makes this so powerful. And so they are technically
integrating, but they don't even have to know about each
other to do that, because the AI is just feeding
outputs from 1 as an input to another.
So now, it's going to, create a couple of tags
to add to this new journal entry,
and, we'll speed this up here a little bit. Then
it it, goes ahead and and creates the entry,
using the information that it got from my location and
my weather. So I I was in Poland at the
time. If you look at the the entry, I was
in Krakow.
And so it it makes up a story about,
my son and me, hanging out there. It was fun.
So then I ask, the AI, hey. Could you please
get me that entry?
And it says, okay. Great. And what it gets back
if you look at the details, it just gets back
a text of JSON. But, of course, it doesn't wanna
show the user that. So instead, it shows,
the title as bolded, and then here's the content, and
then here are the here's the metadata. And it can
structure this however makes most sense. So maybe I'm asking,
hey. Get me that entry so I can read it.
Or maybe what I'm asking is I want a a
summary of this entry. And so, the AI can just
be my assistant, go grab that information, and it has
all the information, but then it just summarizes it for
me. Or maybe,
I like, maybe this is a social platform and I'm
saying, hey. Go get the entry from my friend who
is Japanese and I don't speak Japanese. And so then
it can translate their, entry for me on the fly.
That's just the power of this. Like, none of those
features are things that I thought about when I was
building Epic Me, but are totally possible because we have
this natural language AI assistant,
to to handle not only the integrations, but also the
user experience.
And to take this a step further,
we actually have also,
an effort to add user interfaces
to the output from an MCP server. And so if
you're Uber
and somebody and you have an MCP server and somebody
says, hey, I want to get a ride from the
airport to my hotel,
and they use natural language because it's awesome and Jarvis
rocks,
Uber probably wants to make sure that the person who's
getting the ride knows that it's an Uber instead of
a Lyft.
So, like, having some sort of branding around the response
is probably important to them. In addition, having text as
a response is not always the best. Like, maybe I
wanna have a map and I wanna see the cars
driving around and stuff. Right? So MCP UI is going
to enable that sort of experience right in the context
of the the conversation flow, which I think is super,
super powerful.
So really big fan of MCP UI and excited about
what the direction that is going.
And here's a actually a demo of MCP UI in
action,
very, very early, implementation
in Versus code,
with the MCP support that they have there. So if
you're shopping for an output you outfit, you probably want
more than just text. You wanna see images. You wanna
be able to add it to a cart. You want
to, buy it,
right there. If you're looking at a dashboard of data,
it's just easier to see that visually.
And so, MCPUI is just a really powerful mechanism to
combine all of the stuff we've been doing for the
last decade or more, around,
building web applications and and the visual user experience
with the natural language beautif like, awesomeness that we get
from, an AI agent.
So it's very exciting exciting times.
As far as, like, where we are currently in,
the, like, progress here,
we've got a bunch of clients, and there are probably
more that I well, there are certainly more a lot
more that I am not aware of.
Lots of these are editors. Lots of these are,
services that you're probably familiar with.
This is a tiny fraction of the servers. There are
tens of thousands of servers already. I expect there to
be billions of servers in the future.
Every single website could have a slash m c p
endpoint to it,
to augment the generation,
for the AI. So, here's some additional information to actually
perform mutations. Create an invoice for me, PayPal, please, or
or even, with the Stripe MCP server, you can, in
fact, the the demo yesterday from chat g p t
was using the Stripe server to,
create an invoice for somebody and actually charge them,
for the,
like, for some product and stuff. And then they added,
the Zapier MCP
server to,
issue a refund through Stripe and then send them a
DM on Slack. So, like, it's just it's awesome what
we, can do already.
And, in fact, I have also made, an MCP server
for my workshops. And so as you're working through my
workshops, you can go to your,
your AI
assistant and say, hey. Could you quiz me on exercise
3? I wanna make sure I understand it. And it
will ask you questions about it. It has all the
context of the exercise. It's phenomenal. It really is a
really awesome way, to kind of combine
the experience that you already have with building services
with the natural language and power that we get from
an LLM. It's it's wonderful. It really is. So,
the MCP,
MCP is really the future. I couldn't be more excited
about
where we are headed from an integration standpoint
and the ability to actually control
our environment,
using natural language. I think it's fabulous.
So a couple resources for you. The model context protocol
specification is actually a reasonably,
simple read most of it, relative to other specifications I've
read. So that's a good 1 to take a look
at.
I also have been writing a lot about MCP on
epic a I dot pro, so you can go take
a look at, that and and videos that I published
up there as well.
Here's a whole bunch
of the articles and videos that I've created. If any
of those spark interest, go take a look at epic
ai dot pro.
And then I want to make a special call out
because right now, I have
a cohort that I'm,
selling tickets for. The cohort starts on the 20 second
of September. It runs for 2 weeks, and we will
cover everything that you need to know about the MCP
spec to build a a service
to make your service accessible to users where they wanna
be, which is their AI assistant.
So,
come join us. It's a
a kind of a mix of a self paced thing
as well as a live thing. So I'll be doing
live instruction with office hours,
but it also,
can run at your own pace and fit into whatever
schedule that you have, during that 2 weeks. And it's,
time zone friendly,
live events as well. So it's gonna be really, really
good. Definitely
take a look at that, as well.
And with that, I just wanna say you all are
great. Thank you very much for listening.
Awesome. Thank you very much, Kent,
for your talk. Really appreciate it, sir. Thank you.
We do have just a couple questions
from the audience.
So our
first 1
let me bring it up on the screen here. Do
you mind, unsharing your screen?
Yeah. I'll see if I can figure out how to
do that.
Let's see. Where did that oh, stop sharing.
There we go. Awesome. Okay.
Okay. So the first question is, are there examples or
design principles you use,
to balance automation with explicit user intent so users stay
in the loop? Not sure. I I might have. Yeah.
I I can answer that. So
part of the MCP spec recommends that clients,
do something it it actually is called human in the
loop,
where the, user is approving tool calls and things.
If you watch Tony Stark interact with Jarvis, he almost,
like,
almost never has to say, yeah, Jarvis, go ahead and
do that. Like, Jarvis will just do it, because Jarvis
understands Tony's intent really well, and he he understands Tony
very well.
Sometimes he does say, would you like me to yada
yada, and then Tony will say, no. No. Don't bother
with that or yes, please. So I think that will
always be a part of this user interaction model.
At the very beginning here where we are right now,
I think that it makes a lot of sense for
the AI to ask for just about everything. Like, are
you sure you want me to do this? That said,
like, you probably have already had experiences
like this where you're you're coding up and and then,
your AI assistant wants to run a a shell command
or something.
And so you like, you have to approve that. You
can actually turn off approvals and just say, yeah. Go
do whatever you want, but that's kinda dangerous.
So,
actually, Versus Code just recently,
added support for like, it'll actually parse the, command that's
about to be run to determine whether it's safe, like,
if it's a read only thing or something, and it
will automatically run those.
So, like, it's
the the assistants are getting better at knowing when it's
appropriate to just go ahead and do it and when
it is safer to to ask. So, yeah, this is
very much, an important,
thing. But as a a developer of an MCP server,
you really don't need to worry about that a whole
lot. That's a client problem.
Yeah. Makes sense. By the way, we do have another,
talk later on today that talks about using yellow mode
a little bit more safely, for those who might be
interested kind of weighing in on this topic.
Alright.
Next question is,
will UI be needed at all in the future, and
what does it mean for the front end guys? Yeah.
I think I'm guessing this question was asked before I
brought up MCP UI. So, yes, I think that
UI matters a lot.
If,
let's say that you say, hey, Jarvis, I want to,
have a stopwatch. Like, I need to be able to
time my you know, my kid is doing handstands. I
wanna be able to time him. Right?
So in that case, it makes a lot of sense
to have buttons rather than have, like, sending text to
start and stop. Right? So so,
but at the same time, what if you're actually doing
a workout and so you need a timer and you
say, okay. I can't reach for my thing because I'm,
like, working out over here. So I want a vocal
interface. And so that's the beauty of this, interaction model
is that it supports both.
Yeah. That that is awesome. I would I would love
to be able to,
get things done and be productive, like, while I'm on
a walk or on a run and have that voice
mode be really,
reliable.
Mhmm.
Be very cool.
Okay.
Let's go for 2 more questions here.
What is the difference,
of this cohort that you mentioned starting September 20 second
with the previous MCP workshops that you've done? Yeah. Yeah.
So I have done,
2 I've run 2 workshops before, the MCP fundamentals and
the advanced MCP features workshops. Those will both be included
in the cohort. I am, also adding
the,
MCP UI workshop and an m MCP authorization,
workshop. So an auth workshop to, like, do user data
what most people are gonna actually need for this.
And so, yeah, there's there's twice as much content,
a lot more advanced stuff. And then on top of
that, it's just the format of, of things. Pretty much
every workshop that I give, I get feedback from some
people that it was too fast and feedback from other
people that it was too slow.
And so being able to,
run it at your own pace,
it actually is a lot like the courses that I've
sold that are completely self paced. So you've got videos
that you, are accompany all of the exercises,
and you have the local learning environment. It's really phenomenal.
If you've never experienced it, you've got to. You you
just have to experience it. And I've got some free
stuff that you can try, as well to to just
get experience what that learning environment is like. It's all,
like, local on your own machine, in your own editor
with your own assistant, like, all of that.
But,
the benefit of the cohort beyond that is, that it
actually is live. The material is released incrementally pretty much
every day. So everybody, like, all the hundreds of other
people who are going through it with you are going
through the same material, so you'll have a a hive
of activity going on in the exclusive Discord.
And then my office hours are Mondays and Fridays in
the morning and the evening, so you can choose whatever
is best for your time zone. So that's,
that's the difference.
Okay. Awesome. Sounds very exciting, Kent. It's gonna be awesome.
Last 1 for you, and this 1 is extremely important.
What's your favorite Iron Man movie?
Oh,
that is tough. I I have to say Iron Man
3, probably.
Yeah. But And any reason for that?
What what's the reason behind that? I I don't know.
I I think that his character is a lot more
developed by then. And, like,
I don't know. It's it's hard it's hard to decide,
to be honest. I really like the first 1. And,
honestly, the Iron Man movies, with the exception of Spider
Man, the Iron Man movies
the whole series.
But Spider Man, of course, like, is the best of
everything.
Yeah. But, yeah, I I really liked Iron Man movies.
All 3 of them were good. Nice. Yeah. With with
great power comes great responsibility. Right?
Awesome. Alright, Kent. That is all the questions we have
for today.
Once again, thank you so very much for your time
and your expertise.
Really enjoyed having you. Thank you. I appreciate being a
part of this. Awesome. Take care. Bye, everybody.
Alright.
We are, I think, more than halfway through the day
now. Right?
3 3 yeah. Almost almost halfway through the day, and
it has been a wonderful day so far.
I
just
totally blanked. Okay. Here we go.
Back to reality. Sorry, guys.
So next up,
we have
a panel
between 4,
4 individuals that are a part of the
AIDD
group of instructors,
or at least 3 out of 4 of them are
that are putting on the a I AIDD
master class.
So
let's,
let's waste no time here. I'm not even going to
put out my image. I'm just going to go ahead
and
bring them here onto
the stage.
And you've already met some of these people.
Others,
you will meet a little later on. So let me
get them all invited on stage here.
Alright. They have been invited. I will get a hold
a hang of this inviting people eventually, y'all. I should
have done that,
at the end of Kent's talk, I believe.
Okay.
So,
while we are waiting on
let's see here.
Okay. Here is Justin.
Hey, Justin. Good to have you back, sir.
Oh, you are muted.
I I was saying I figured it out this time,
but I obviously did not figure it out this time.
He he already got 1 round of practice in, so
he was the first 1 in here. Makes sense.
Garrison,
how's it going?
Garrison froze? Is it just me?
No. He's very frozen.
In quite a pose, though. That's a good pose. That
is. It could have been a lot worse. I think
I'm gonna take a screenshot of that 1.
0, he's posing again for us. Also muted. You're def
you're definitely muted at least.
I am muted. Ah, so that's how you work the
computer.
Excellent. Alright. So waiting on Mustafa to come in.
Perhaps.
Mustafa, I think you have to leave and then the
the thing and then come back in and then, like,
a thing pops up. Yeah. Let me also, Mustafa, make
sure maybe you have a couple of different users in
here. Let me make sure.
Mustafa Saeed.
Okay. And then Mustafa
yeah. Mustafa's done a lot of testing on this platform,
so he has a few different users. Maybe I invited
the wrong 1.
Alright.
Okay. Well well, while we are waiting on Mustafa to
join us,
if you could just,
give us a brief introduction about who you are, in
just a sentence or 2 and,
what your experience with AI is. Like, why are you
here?
Justin, would you like to start? Sure.
My name is Justin Schroeder, and,
I hail from Charlottesville, Virginia.
My experiences,
with AI are many and varied. Most
most people don't know me, but if you do know
me, it's probably from a bunch of open source projects
that, I've released over time,
FormKit and AutoAnimate
and a few other ones.
And my
latest and greatest AI stuff is is actually trying to
build things with AI. So, there's a there's an app.
It's not even an app. It's like a it's like
a
it's a fitness trainer. So if you go to bod
dot coach, it's a website, b o d dot coach.
No dot com. Just dot coach. And it's like,
like, you get, like, a contact card on your phone,
and then it, like, plans your workouts for you and,
like, texts you and reminds you. And, like, when you're
not going to the gym, it, like,
gets really obnoxious. And,
and,
when you're in the gym, it, like, has all your
workouts and tells you how to do everything and work
works you through and everything. But, obviously, those are all
agentic systems,
that are that are working behind the scenes. And,
so that's that's my most recent AI stuff, but, obviously,
doing a lot of
AI coding and doing a lot of the coding with
AI as well.
And I'm here as an instructor for this awesome course.
So
that's me.
Thank you very much, Justin. Mostafa, welcome to the, the
feed here, the live feed.
Thank you.
Why don't you, since you've just jumped in, why don't
you introduce yourself quickly and, just tell us, why you're
here? What's your interaction with AI?
Yeah. So,
my name is Mustafa. I'm based in Egypt, and I
am the director of education at Better Brains.
And, basically, I'm here because of,
AIDD and, what we're trying to make with AIDD and,
and how
how this
new thing is
really, really changing everything about how developers
work,
and everyone is not sure of how it's gonna be
like. So, yeah, that's basically why I'm here and why
I'm doing what I'm doing. And,
and, yeah, I guess that's all.
Awesome. And, lastly, Garrison.
Yeah. So I'm I'm Garrison Snelling. I'm the founder of
Compute SDK.
Like Justin, I'm gonna use the word hail. I hail
from Alabama, Opelika, Alabama, which random fact, Daniel may not
want me to say this, but he's actually from my
town.
So we can all get together and have a hangout
if y'all wanna do that. We've also talked about going
to Egypt.
I hope we can really get there. That would be
great, guys.
Please do.
1 of the things, I think,
my AI journey is probably very similar to a lot
of y'all's.
I,
I'm a self taught programmer.
I, you know, obviously, had no formal education.
I just was a 13 year old kid making websites
and turned it into a career.
And 1 of the things that I was really interested
in,
about AI was actually
how much you could teach yourself, how, you were no
longer really
constrained,
by your ability to kinda consume, tutorial articles. You could
actually have customized articles
generated
based on your learning style and your habits,
and sometimes even watch it kinda do the work for
you.
And that was particularly interesting to me. And, when I
when I started down the journey of building, comp compute
SDK, which is essentially like a multi provider,
sandbox toolkit, kinda the toolkit exists around
managing other people's code because this is gonna be such
a big problem for us in the future.
I started to build prototypes with it, and, the the
prototypes themselves, I I was I was actually using I
think it was maybe 1 of the first projects,
that I've used AI,
AI for exclusively. I remember I was using Wind Surf,
and I was just flabbergasted
as somebody who was very new to Kubernetes
at how quickly I could learn,
these core concepts of Kubernetes. And, you know, you at
at this point, you know, I started out as a
front end engineer, and you'd be surprised at the kind
of conversation that I'm able to have about Kubernetes
as somebody who's, you know,
just learned all this in the past year. And I
think that's truly remarkable. And so that that's kinda when
I think of AI and just how amazing it is.
I don't I don't even think I don't want it
to do my work for me. I want it to
teach me. That's what I want.
Very interesting.
I you know, if if somebody had told me a
year ago that 1 of the courses that I was
producing
was going to be,
I was going to have to write some React code,
you would,
I would have never believed you. But here I am,
and it's because of AI that I'm able to do
that. I'm able to learn
more quickly than I ever have,
and the the precise things I wanna learn. Right?
Like Kubernetes.
Like, that's not on my list.
Alright. Great, everyone. So during this time,
of the panel, we will be sourcing questions from that
Q and A tab.
So that Q and A tab, remember, is at the
bottom of the screen. This is the same Q and
A tab that you've used during
all the speaker sessions. Please go ahead and start throwing
any questions you have for the panel in there. It
can be about,
anything about some of these guys' workflows.
It can be anything about the AI driven development master
class that we have been talking about,
throughout the the program.
It can be anything related to,
anything related to AI, something for Justin, like maybe how
he you know, something about integrating AI into the end
application, things like that. We are open to all of
those types of questions.
Please just throw those on the chat,
that Q and A not the chat, the q Q
and A panel there.
Alright. Well, let's start answering a few oh, 1 last
thing.
If you,
if you had a question earlier for especially like somebody
like Justin who already had a talk and he didn't
get to your question, I have removed a lot of
those so that I could keep up better
with the questions for the talk. Throw that back in
the q and a panel again if you did not
have that answered, and we'd we'd be happy to, answer
it for you.
Alright.
Let's start taking questions now.
The first question
is: What's the most important change I can make right
now to start preparing my skill set,
for Actually, this might have been for Ken. I probably
missed this. But I think maybe we might be able
to weigh in on this.
What's 1 1 thing you can do right now to
start preparing your skill set for for being an AI
driven developer?
Let's put it that way.
I actually really like, Kent's take on
sort of the the role of MCP,
being a way to give technical access to nontechnical people.
That's my summary of the most important thing he talked
about.
And and if you think about the way that that's
happened in the past,
you give tech nontechnical people access through very careful and
deliberate designing of UX interfaces.
And that takes a lot of work. It takes a
lot of money,
and you're trying to sort of guess the way that
your users' brains work.
And inevitably,
you guess for some you're trying to guess for the
majority of users.
And so what's really cool about being able to expose,
you know, may maybe technical is the wrong word, but
expose things to,
a a sort of nontechnical interface like a chat app
is we can give
anybody the ability
to to get done what they need to get done.
So, like, a simple example would be a customer service.
Like, imagine you're working for a company doing customer service
or customer support.
What
if instead of the person who has to look up
your account
needing to go through some crazy CRM and search for
the user by their user
ID that's some 20 digit number and all of this
other stuff. Like, what if they they just had a
chat agent
as the employee, and they were able to say, oh,
somebody's on the phone with me. Their name's Garrison.
I don't really know anything else. It would be like,
oh, why don't you ask them what state they're from?
Oh, they're from Alabama. Okay. Oh, we only have 2
people in the database that are from Alabama.
So is it this 1?
Right? Like it could it could provide these really interesting
and new interfaces and you as the developer almost don't
need to create that interface,
you just need to expose the right tools. So I
do think that that is really exciting. And and to
get started,
do it. Like create your own MCP server,
get get working on on building out those skills and
seeing what works and what doesn't. You have to build
the intuition to know what's gonna work and what's
not. Nice.
I I've actually
on the topic of of interfaces and kinda like the
MCP UI that,
that Kent was talking about, I've had some decent experimental
success using,
Nuxt content. And you can have all these custom components,
you know,
in in Nuxt content, and you can just, you know,
tell your AI about those. Let them know what components
are available, what,
properties or what props they take,
and have a similar kind of output to what Kent
was talking about. So for the Vue world,
that might be something,
worth experimenting with if if you're in the Vue.
Well, 1 of the things that I that I really
appreciate about Ken's perspective is that he's looking to MCP
as like a protocol, as something that is, you know,
a constant almost. It's like this isn't going to change.
We kinda feel like, you know, we should see a
good bit of adoption of this. But if you look
at the actual interfaces that we have today, which is
primarily a chat interface, either it's in terminal or it's
I actually think the interfaces themselves
are actually very,
underdeveloped is is is how I describe it. I think
in the next 10 years, we're gonna see, a lot
more stuff kinda like what you've built with, Justin with
bod coach where it's I mean, you know, it it's
a lot more kinda strategic,
as far as getting to moments and and and tracking
and, you know, it's even hard to quantify in language,
to be honest.
And so I think going back to the point that
you just made, Justin, the most important thing I think
is to get started, and it's actually to like, the
way that we build the future is by actually
building the future.
I mean, 1 1 cool project that I've seen,
is Hashbrown, and it's actually a set of,
kinda UI components that you can convert.
It it it allows it's a library that allows you
to convert your already existing front end components into tools
that your agents can use. It's really interesting.
And these I think these are the sorts of things
that we're gonna really wanna play with, because a large
portion
I I don't think
I don't think we're throwing everything away that happened in
the past. You know? This is 1 of the things
that I think is really cool is that you're gonna
take some of these primitives that we built,
from kind of a front end component perspective and maybe
reuse it, and I think that's really interesting.
Awesome. That's cool. I've not heard of that before.
Yeah. Me neither.
Alright.
Any
more,
thoughts on that question? Are we ready to move on
to the next?
Yeah. Let's hear that. Good 1. Yep. Alright.
So next up,
as someone who is quite far into AIDD already,
how do you think the course can shape me? I
felt like the first course, which is released for was
basic,
and I get it. It's an introduction.
Personally, didn't learn anything new there. But how do you
think future courses,
will work out?
So I'll start off on that 1. We did start
from ground 0. So this is specifically discussing the AIDD
masterclass that we're talking about.
As we we did start out on
really the ground level,
you know, really
kind of showcasing
what is possible with AI.
You know, what does it look like integrating for some
AI into your workflow for someone who's
at ground 0?
The courses do
build as you move on. Right?
So, for example, Justin gets into
doing things like working with Claude CLI as opposed to
just having an agent inside of Cursor but actually managing
a CLI agent. He also gets some of the things
like,
you know, MCPs and even building your your own MCP
and other advanced topics like that. Garrison up here is
getting into,
like, really time driven. I I'm talking for you guys.
I guess,
I get to see a lot of the content these
guys are creating which has been great for
me.
But, yeah. I guess probably
either 1 of you. Garrison, would you like to share
maybe some of the things that you're working on there?
Yeah. 1 1 of the things that I can say
to this is that I think
a large portion of what we're kinda seeing what what
my lesson is about is build about building
strategic or reusable agents.
It's not just a chat interface. And I think that
this is actually a pattern,
that is really helpful for people to understand in a
lot of ways. I think a lot of the applications
that we're gonna end up building,
in the future are actually things like this. I kinda
call this, like, when when when I put on, like,
my my,
you know, enterprise hat, I call this the age of
internal tools. I think everybody's you know, we we had
internal tools previously, but I think internal tools kind of
for our workflows are about to explode where you're gonna
have your own set of kind of things that you
actually work against because you built it,
and you're familiar with it and you like it. And
that that's kind of what I see specifically about strategic
or reusable agents and where I think kind of the
course,
where where it's heading is actually, I think, more in
terms of a personalized development experience. I think, you know,
I think in the past, we used a lot of
standardization, and that's still obviously gonna be there. But I
also think that we're gonna have our own sort of
mental pathways, if you will. Like, I I I can
tell you I I think a little bit differently than
than other people when I learn something, and I think
that that's something that's really unique about the thing that
we're building with AIDD is that you're actually going to
be able to customize a lot of these tools,
specifically the strategic agents to meet your needs, not just,
you know, kinda pull something off the shelf.
Yeah.
Our our John? Our John?
I mean, I would just at a high level, there's
tons in here.
Garrison's
building agents. We're gonna go through, like,
everything from how you evaluate models when they come out.
Who should you be following
in the space to know when models are coming out?
Like, are they good or not? Because you're you need
to pick up a lot of signal from the community
in general.
What kinds of channels should you be tuned into? Things
like that. How do you do your own evals on
models? Like, how do you make sure that models are
valuable or not valuable,
specifically
in your application,
to kinda speak to what Garrison is saying, like, creating
your own ecosystem,
for being successful.
So for you to have your own eval, so you're
not just looking at,
you know, the the the benchmarks that are out there,
which are notoriously bad,
at actually saying how well you're going to have your
developer life impacted by it,
so things like that.
Running
swarms of agents,
I'm gonna be talking about that too.
Building your own MCP servers like Daniel said, and specifically
not, like, not
MCP servers so you can run Playwright. Those are well
done.
There's a place where MCP servers
are, not well done, which is at your company.
How do I use MCP servers for my company to
be able to expose important information into my workflow?
So things like that. I anyway, there's there's lots to
come.
I'm sure
you'll you'll all drown in
goodness.
Yeah. And And
yeah. Go ahead, Mustafa.
Yeah. I was gonna say that, on the top of
all of this, we're we're basically yeah. Of course, the
first course,
is is an intro, and
the person who asks already knows that. But going forward,
we're actually going into workflows, like, what do you need
to do first and then next and then next and
so on. Like you saw in, in Justin's
slides,
he has, like, a way of doing things that has
worked for him very well, and that's what he's sharing.
And same goes for Garrison. Same goes for Daniel Kelly.
Same goes for other instructors as well for future courses
in AI driven development. So we're talking here about,
like, a complete coverage of everything we know and all
the experience we have so far about AI driven development
and just transferring that to everyone else to make sure
that everyone
knows this and they can build on it and they
can talk about it in the community and they can,
develop even more,
or better workflows
that works best for them as well. So it's just
like a growing,
set of courses and a growing community. That's what AIDD
is. That is eventually,
its main goal is to
make sure that you have a map, make sure that
you have a way.
Like Justin mentioned, people you follow, community that you're engaging
in,
workflows that you know of and workflows that you invent
and workflows that you teach other people. So it's just
continuous learning journey that we go through together, And we're
covering everything from MCPs, RAG,
all the concepts, all the workflows,
that we can find out there that could be helpful
for everyone. So yeah.
Alright.
Going on to the next question here. It has,
quite a few upvotes, so sounds like this will be
interesting for a lot of people.
I am afraid of the future of junior programmers. If
they rely too much on the AI and they don't
yet have the skills to know how good code should
look,
or work,
how can they review and verify generated code? We
where are they going to pick up the necessary skills?
Is code
writing or reading not necessary anymore?
I bet we all have a different opinion on this.
Yeah. It's a I think I know some of Justin's
opinion here. Yeah. Me too.
You know,
I I think this is part of just how,
our jobs as the experienced devs are transforming. Right?
It's it's our job to do more of this
this review and and things.
But do I still even as an experienced dev, like,
do I still rely sometimes on the output of an
AI?
Yes.
But just as much as I rely on what I
copied from Stack Overflow. Right? Like, neither 1 is a
a hundred percent
reliable source in all cases. Right? Where
everybody is building
products that are the best they can be, the best
that that team or that dev can can build, and
everything is always gonna have,
have bugs. So
I don't know.
I think you still have to know how to read
or write code, but you have to
you have to expect that nobody in the loop, human
or AI like, is ever gonna be, like, the perfect
reviewer. Right?
It's just not gonna happen.
Maybe that doesn't answer the question. I think I went
off the deep end.
The the way I think of it is, like, when
when machines first, like, showed to the world and started
being used instead of humans doing things manually.
So when machines started doing things more automated,
quicker, they don't get tired and all of these things
and they are cheaper and so on,
what happened is that a lot of people were working
on these things manually and now they are out of
job.
They used to know the instructions of doing everything,
like, for example, if it's a boxing machine that is
creating a box and putting things in in it and
then,
closing that. So thinking about something like that,
the worker would need to know how to do that
specifically, like, the steps 1, 2, 3 with all the
details,
everything, and make sure that,
that everything is done according to the instructions that was
was giving to them. But when machines came on, those
people,
they don't need this knowledge anymore, which is, like, the
basic knowledge of how to do things step by step
from the very beginning to the end. And that's how
we think about it with AI. Like, I'm gonna talk
more about that in in my talk after this panel,
but, basically,
things are going to be completely changed, and we don't
know to what extent.
It could be that most of the things that
juniors know would be irrelevant because other things like AI
can do it instead of them, and it could be
that they actually need to learn it more so that
they can audit
and review AI code
properly.
But when you think about it with the same example
that I just gave about knowing the steps, the exact
steps that needs to be taken and studying them and
and and just familiarize yourself with them, if you think
about that example, you're gonna find that you're you're in
between 2
2
I don't know. You're not sure of what what the
future will be like. Is it gonna be like
AI is gonna be too good that these things are
not gonna matter anymore? The programming language used is not
gonna matter anymore?
The the the underlying technologies that is being used is
not gonna matter anymore. What matters the most that it
is very good at its job, that it gets things
done. Is that happening this year? Is that happening in
10 years? No 1 knows. But what we do know
all the time is that
we always find something new to learn and something to
add on the top of whatever technology or whatever tool
that has been brought to us. So,
like, no 1 is sure what's gonna happen or what
is what is that
the the final line that or how how good AI
is gonna be like, but all we know is that
we're not there yet. Like, we're not there yet where
AI or the steps is irrelevant. So the good advice
to give right now is that you must learn the
fundamental step. You must learn them. You must practice. You
must know how,
to do clean code, and AI is not taking that
away. Actually, AI right now is a tool to do
things faster. But now,
and I suspect in the next 5 years, you're still
gonna need to audit what AI does and how it
does it, and you're still gonna need to know the
fundamentals and still gonna need to be a good developer
so that you can get the best out of these
tools. That's that's basically how I think of it. I
think no 1 is just sure how is it gonna
be like.
I like to pretend that I'm sure,
even though I'm not. But it's fun. It's more fun
to pretend that I know everything.
I think junior developers
Right? Yeah.
I think junior developers,
my my opinion on this most of this point has
waffled so many times.
Most recently, I felt like they were in big trouble,
and now I think they're gonna be actually
completely fine.
And the reason is I I think code,
doesn't matter,
like, long term. I'm not saying today,
but I'm saying in the very short term. Right? CloudCo
came out this year,
and it has dramatically changed my own interaction with my
code bases. I mean, I'm managing millions of lines of
code,
and I don't really look at the code anymore.
What I care about is the artifact at the end
and it provability
about can it do the job that it was intended
to do.
And there was an interstitial period there where I cared
a lot about the code, and I cared about the
way that the code looked and the way the code
acted and all of that kind of stuff. And, you
know, it's it's,
those days are already behind for me. So I don't
care about the code,
and I think over time, people will care less and
less about the code. Now that can make you feel
sad,
and I understand that.
But at the same time, I think it's really, really
important to realize, like, things are changing
extremely
fast,
extremely fast. And so what actually matters is what has
always mattered. What is the job of a developer fundamentally?
If you thought the job of a of a developer
was
writing code
and knowing the syntax,
then you probably weren't a great developer already
because the job of a developer has never really been
that. It's been to translate the real world into the
machine world,
and that job is going to exist for a long
time. It just doesn't require the same skill sets.
It reminds me of photography. There was a moment where
being a photographer was being the guy with a camera,
and now everybody has a camera. So what does it
mean to be a photographer? Well, it means to capture
a moment, to have artistic taste, to have a style,
to have value that you're bringing to the world. And
the same is gonna be true in software engineer. Right?
Like, you need to do something valuable, and you're gonna
do that in a different way than it was done
before.
And right now, we're a little bit in between the
2 worlds. We can see both sides, and we can
see, like, oh, yeah. It wrote bad code.
That that problem is just gonna get better. And the
more that you have patterns and and procedures for yourself,
so that way you know your that the output is
good. Sorta like what I was talking about in in
my talk where you have, like, you have some sort
of a cycle, you have a pattern that you're following
to know that the output is right and you can
test it over time and that when it grows, it's
not all just in 1 giant file, for example.
Like, if you can create those patterns for yourself, you'll
be very successful.
And that's, like, what it will mean to be a
developer
is knowing how to do that skill.
So that'd be my answer. That'll be fine.
1 1 of the things that I like about what
you just said, Justin, is that it highlighted the fact
that
our profession is actually an outcome based profession. Like, we
don't actually like, it's not like, you know, we we
we actually get compensated in some way, shape, or form
for how pretty our code is written. We've all worked
with people who actually over indexed
on code quality and struggled to ship features or struggled
to produce business value.
And those were difficult experiences as as as my guess.
It was always a painful interaction. It still is always
a painful interaction when you're talking with somebody who cares
less about the actual value that you're supposed to bring,
and cares more about the code. And I think in
this in this
in this day and age of AI, that's kind of
what we're seeing is that, actually, we're being reminded again
that this is an outcome based profession. This is we
have to produce results, and we actually
and and sometimes, by the way, you know, like, with
computer SDK, we care a lot about code quality because,
you know, we we primarily sell to developers using AI
agents. And so to some extent, you know, yeah, we're
gonna want 1 of our,
you know,
we're gonna want beautiful API layers to so that self
documenting that sort of thing. But, you know, if you're
working on a you know, if you look at
Nomad List from Peter Levels,
he has 1 PHP file that he uses to manage
the whole thing, and he makes a large amount of
money from it. And god bless him. That's a beautiful
thing, And we all can see that, and he doesn't
care about quote quality. And,
hey,
it works.
Justin,
my my, like, workflow timeline when I first started creating
AI, I think, like, if you broke it down into
so there's a hundred percent of my time. Right? And
in the beginning,
a a lot of it was, like,
20 percent of my time,
prompting,
and then,
I don't know, another 20 percent of my time tweaking,
manually doing things, and and the rest of it was
spent actually just really line by line seeing what did
this thing make. Right? And now I find myself spending
a lot more time in the very beginning, say, maybe
50 percent getting
getting the details right of what I actually want and
defining what
a good result looks like. And then just telling the
AI, this is where we're starting. This is what I
wanna get to.
You do everything. And then it can check itself. And
and at the end, I spend maybe 10 percent of
time reviewing code it wrote and more time reviewing,
does the output look like I wanted it to look
like. And that's a that's a lot easier than reviewing
code line by line. That's saying, okay. This box is
checked. This UI element is there. You know, this this
property of this object equals this value. And it's it's,
yeah, it's it's easier to do and it you move
faster, but that's my progression.
Yeah. Yeah.
Interesting. Okay. So next question.
How do you balance number 1, how do you balance
bills, cost of using tools like Cloud, Copilot, and Gemini
with,
building apps?
What a great question.
Pay the money.
That's my recommendation to everybody.
Spend the money.
It's there's no there's no replacement for it.
Like,
I think I think
the fair number right now would be
you should probably be willing to spend around 400 dollars
a month per full time employee on AI credits.
I think that that's probably a pretty good realistic so
that could be like a Claude
Code, Max subscription
and,
you know, some, like, Chat g b t at 20
bucks and something else at 20 bucks, like, kind of
across all the domains,
I think that that's kind of the upper limit of
what of what's a good a good price. I'm talking
about full time engineer. You know?
That's what I would say. Sorry.
Yeah. I I I agree, Justin, because,
when you try to save money on
working with LLMs,
you're really just cheating yourselves in a lot of ways
because you get subpar results.
But when I when I reach for cloud 4 thinking,
like, in cursor and and that's
basically all I do nowadays. I turn on auto for
for smaller tasks.
But the majority of my prompts send to cursor with
cloud 4 with the clouds on it thinking, and it
just gets it right. Like
Yeah.
Yeah. I mean, I'm seeing in the chat hi, chat
people. I see a lot of things in the chat
about, like, you know, code quality is low,
agents don't do a great job at writing code and
stuff like that. And
I would say on
average that that is becoming less and less and less
true over time.
And part of the reason is that people try to
save money on the cost. Mhmm. And I agree that
the lower end models do a poor job, but if
you have good if if you do good prompting
and you have good structure and you have a good
pattern for yourself,
these high end models will write better code than most
senior developers.
They just will.
The the only place where I would say this is
not true is in completely novel things. So if I'm
writing a completely novel piece of code,
an an unsolved problem, like I wrote a a streaming
multiplexer. There is no streaming multiplexer that exists. I looked
or I would've used it. There is no streaming multiplexer
out there. I wrote 1 myself
with, you know, bit shifts and all that kind of
stuff, and the agents couldn't do it, and I could
do it.
Those places are I mean, I I come across those
problems once every 4 years. You know?
Yeah.
Yeah. And I agree. These just keep getting better.
Alright.
I do think on the I mean and we talked
talked about this a little bit, but the way that
you balance your bill cost, I think, is by actually,
like, utilizing it properly. That's essentially your your optimization
strategy
is, like, you don't you don't approach it with incomplete
thoughts. Or if you do, you know that you are
having incomplete thoughts. For instance, in a multi tenant application,
there are several gotchas. Right? If you want to,
you know,
you should kinda know before you start prompting, do you
want a dedicated database,
for your SaaS app per each tenant or do you
not? Because if you, you know, if if you go
back and forth in in your prompting between, you know,
essentially speaking out of both sides of your mouth, you're
gonna get bad results,
because those are 2 very different, situations.
Yeah. What people should not hear me say or anybody
else say is that this is a skillless,
job now. It's not at all. If anything, more skill
and and more wisdom is required now than ever before.
You need you need to have excellent architecture.
You need to have a clearer vision of what you're
building instead of setting out and writing code. And then
while you're writing the code, kind of be like, oh,
I'll solve the problem this way. You almost have to
be able to stand back and solve the problem ahead
of time.
And then the the actual syntax is the thing that
doesn't matter. Like,
you know, I it almost doesn't matter to me if
I'm writing an app now in TypeScript,
Python, Rust, whatever,
other than the fact that each of those languages offer
different benefits. But from an actual syntax perspective, it's less
important than it's ever been before.
So that that's what I would say. Like, the yes.
The code quality, sure, it still matters a little bit
because the AI has to read the code and, you
know, things like that.
But where we're heading in the very short term, like,
1 year away is,
is where that the code the, like, the the idea
of code quality needing to be important is going down,
and and the other ideas are becoming more important. So
your ability to be a, like, wise discerner and a
wise architect
and create,
real value through your ability to understand the real world,
that that's that's the real job. That's the real job.
Alright.
Maybe 2 sentences.
What do you think about a Vibe debugging specialist?
2 sentences. Anybody?
I I think there will be, a need for experts
to help
fix some of the issues of the people who got
in and got started and were moving really quick and
then got over their heads. I'll put it that way.
So is is the question essentially what do you think
about a bug fixer as, like, a a task or
a job or, like, a career?
Yeah. I think somebody who vibe coded a project,
and and turning it into production grid. That's that's my
interpretation.
I'm I'm doing 1 of those right now, actually.
So I guess it exists, but I I don't know
that it's an interesting gap in the market. I don't
know how long that'll exist or
Yeah. I don't know. It's kinda weird.
Yeah.
I understand. But I'm also vibe coding it. That's what
like, go to go back to the previous point. Sorry,
Daniel. Screw the 2 sentences.
To go back to the previous point,
the
the point is not the vibe code. It's the fact
that I can vibe code
the thing the other guy couldn't.
Does that make sense? Like, somebody vibe coded something, they
couldn't get it to the finish line.
They come to a developer who's gonna use the same
tools they were using Right. Who knows how to get
it across the finish line.
Right? That's, like, what you should wanna be empowered to
be able to do. And and it's not about prompt
tricks, fancy stuff. It's it's about your knowledge and and
what you bring to the architecture. Right? What you know
about the architecture and how it should be.
Alright.
So next question.
Coming up. Oh,
there we go.
So where does where do designers fall
in the AI driven workflow?
Yeah. They're out of jobs just like everybody else. Just
like everybody else.
I I it's interesting to me that we all look
at, like,
that there's this
question the consistent question of, like, well, whose job is
going to
lost
And, you know, it just it feels like it creates
so many more jobs.
Yeah.
True. I was I was just talking to, my cofounder
yesterday about this exact thing, and
we were saying this is another thing where you can't
predict the future. But a year ago, we were of
the opinion sort of that, like,
the the the writing's on the wall for that job,
for the for the designer job.
And it it turns out that the writing's on the
wall for, like,
the idea, like like, the
more like the stock photo thing,
but actual design is sort of a distillation of what
we're talking about where it's like you're making these sort
of architectural like decisions and stuff. And and and I
think it's gonna be actually really important for a long
time still. I I I think it's got a decade
on it on that, like, the existing skill set of,
like, designing something in Photoshop. The tools might change a
little bit, but I think it's actually really important.
Maybe more important now, actually.
Yeah.
It it depends on what you mean by design, I
think. If you mean, like,
you can cobble together an existing UI library and make
a nice interface.
Like Right. No offense. AI can do that
pretty well. Right? Yeah. The designers didn't want to do
that. Like, they didn't want to get a starter project
designed. They wanted to actually build an a full application.
That's what they wanted to do. Yeah.
So
there's definitely still room for it, but you you've got
to be making
you're not using the stock stuff. You're you're creating
you're creating marketing sites for Coca Cola or,
for Apple. You're you're doing the crazy, you know, animation
stuff that the Apple's doing,
Conceptually,
you may not even be
making the animation. The AI may be doing the actual
thing, but conceptually,
you're you're making that happen and and drawing out and
figuring out what that looks like. Right?
Yeah. I've been exploring like, the past few days, I've
been exploring the tools that can generate videos. Right? So
I was I was trying to find a way where
I can
create a million dollars at,
I'm gonna be honest. It's for AIDD. So I was
thinking, why don't we make a million dollars ad for
AIDD with all these great
video models out there that can do impressive stuff? And
I did try with few things, and the result is
astonishing. Like, I really don't know what's next, but they
have gone so far. You just give it like, you
you can get part of a video, give it as
an example, or you can describe it, or you can
do whatever it is, and just give it a prompt
to say that make it cinematic
where there is a robot and there is a human
and all of that thing and the quality of the
output is just impressive. That is
scary good. And something like that, in the real world,
people would spend months on producing it. So having it
just like this is kinda
impressive, to be honest. Like, it it gone so far.
I remember when, video models were just basically messing up
the fingers and things go inside each other and they
do all kind of crazy
crazy things on video, and now it's just
it's crazy good. So it's not only designers, not only
developers. It's it's basically
just a a tool that can do things quicker and
faster, and the limitation is gonna be basically not the
limitation. Your skill is gonna be on how
good can you be competing with others using the same
tools. Like, all of us has the same tools. All
of us used to use Versus Code, used to use
whatever the code editor with all the things that is
assisting us, but what matters is the output. What can
you generate with these tools? So I guess it's it's
the same for everyone.
Alright.
Well, we are unfortunately running out of time. We have,
1 more question left that we can get to,
and I'm gonna shoot for the highest upvoted 1 here.
It's a bit of a long 1. There have obviously
been a lot of conversations,
both good and bad, about how effective AI can be
in speeding up products to market.
We are experimenting with a new team,
agent driven team,
to break from the traditional way a team works.
Do you have any advice for us
as we get into this?
I would love to know more about what you're doing,
Dean. That sounds really interesting.
Yeah. It absolutely does.
I I don't know what the
The agent driven team is an interesting idea that I
have not
I have not explored what that would look like across,
like, is it creating to do lists? You know? Like,
is everybody just using an agent? I wonder what it
means. You know? Right.
Yeah. But I I do think there's an interesting concept
here that, like,
this is not just an
AI driven
development team. This is, like, AI driven
team across, I I assume, across multiple you know, communication
between multiple
teams and how they can work together better with with
agents, which is interesting and out of my price range,
I think.
I think to take a stab at answering the question,
do you have any advice for us,
but with just limited information we have? I think
my my advice would be to, you know, put more
thought into the planning stage, and I think this is
something we all can agree agree about. Put more thought
into the planning stage than you would expect,
and try and derisk and debunk as many as many
fallacies or,
problems as, as possible.
Play with it theoretically inside your mind and then kinda
and then finally, when it gets time to actually produce
an output,
spend less time on that. That would be that that
that that's how I'm doing all of my projects because
I'm caring more about, you know, the architecture
than I am about,
the time coding.
Yeah. That's that's right.
That's a real skill too.
Even knowing the granularity
of the feature to give to AI to code,
those are things you have to the the intuition that
you have to build up is part of the part
of this new job,
is is knowing that. I'm I'm really partial at this
point. I don't think this is what you're saying, but
I'm really partial at this point to,
using a
swarm of agents that are all working at the same
time.
And it's a new skill that I'm developing for myself,
where I'll have I usually have probably about 6 agents
running at the same time.
And so
the job is very different,
because it's all about
figuring out what the next important thing is that's discreet
enough that it can be run
in its own way and then,
and then having the right tools and everything to manage
that.
So
I'm having a lot of fun.
Conceptually, that's a much harder problem. That's that is, you
know, it is you literally have to hold the concerns
inside your mind, and it is a lot trickier than
figuring out what is the next method that I need
to import from this library.
Yep.
It's fun, though.
Yeah. Having a good time.
I'm gonna have to pick your brain on that some
more, Justin. I I'm 2 ADD. The I I have
2 agents, and I'm like, ah.
No.
Daniel, you're a I d d. That's right.
He's right there for you.
Alright. Thank you all so very much, for your time
here during the panel.
Sorry to anyone whose questions we did not, get to.
But, guys, if you wanna stick around for just a
few minutes and, hop into the q and a tab
if you have the time,
and throw any answers that you, you know, find valuable
in there, I think that would be super awesome.
And, by the way, there's lots of good comments going
on in the comments section. If you haven't seen those,
go check those out.
So that is all for our time here. Mustafa, did
you, have any closing words, sir?
Yeah. 1 closing word. So, basically,
just for everyone listening out there, so some people ask,
like, what is insiders? What happened? Like, where is where
can I buy this and so on? And, basically,
we did do an insider's
phase, and that was kind of a limited seats launch
that we did where we were still in the early
phases of AIDD, developing AIDD. And those people who joined
are actually the insiders or you can say the founding
members. So these people, with their feedback, they constantly help
us push this further, add more content, and
direct
the the kind like, the the whole,
course and track that we're trying to prepare and so
on. So this insider's phase has been closed, I think,
for a month and a half or 2 months now,
and, we are planning to launch,
and we're launching, hopefully, on the September,
30.
I think that event has been shared somewhere in the
chat. But also,
just like I I guess that also gonna surprise everyone
here, but
you can purchase
AIDD insider pass today still. And, that's something that has
been decided in the last minute before,
doing the panel.
And,
and, yeah, I'll place the link in the chat, and,
basically, all what you need to do is just click
on that link. You're gonna go to the old insider's
page. It is under, a URL,
for the backdoor. You just pick the plan, make the
payment, and this is the cheapest AIDD will ever be,
and, we're closing that,
I guess, we can leave it for today and tomorrow
and close it first thing in on Friday.
Yeah. So, basically, you are
the only people who knows this, Shraddha,
and probably you're not doing any kind of marketing around
it or anything. It's just a chance for you now
to purchase AIDD,
in case you would like to,
get the 50 percent discount that is currently active because
it's not gonna be 50 percent when we launch. So,
yeah, just wanted to add that up and, yeah.
Exciting. Alright. Thank you all. We will see you later.
As I slowly kick people off. Y'all can hop whenever.
I can stay though because Yes. I'm next.
Mustafa can stay here on stage. Let me just quickly
share
my little
slide about Mustafa and give him an intro.
Alright. There's so many buttons to click on this thing
to get the thing to line up where you want
it. I'm,
not great at it,
but that's okay.
Okay. Here we go.
So,
next up on our speaker list, you just heard from
Mustafa during the the panel, but now he's going to
be talking about,
the data that tells a different story, developers
versus
AI.
So Mustafa is the director of education at Bitterbrains,
leading the AIDD project,
Vue School, Mastering Nuxt, and more.
He's spent,
years teaching developers through courses, tutorials, writings. Now he's
focused
on helping them learn how to,
utilize utilize AI to become better developers.
So, yeah, take it away, Mustafa.
Thanks, Daniel.
So,
yeah, that was a a great introduction. Let me try
to share the screen right here.
Allow,
and let's see.
Hopefully
got it. Okay. So, hopefully, you can all see my
screen. Great.
So, yeah, my talk today is kind of complementing
what we were discussing at the panel, basically, because I'm
going to dive deeper into
what the data says about AI impact
on developers,
and, it's not gonna be a technical
talk, though. It's but it's gonna be important for you
to realize how everything is changing around you with data
and not with just opinions and so on. Everyone has
an opinion about AI already, so I'm not gonna talk
about my own opinion. Basically, I'm gonna talk about data,
and I'm gonna talk about facts.
So,
no need here to go through the introduction, but,
basically, just to summarize everything, I'm Stafa, based in Egypt,
director of education at BetterBrains.
And, I'm a web development educator and a full stack
developer.
I
did, for example, the latest course that I did with
the Views 3 master class on View School. And, ever
since I've been working on leading the education unit at
BetterBrains.
So let's get to the talk.
Basically,
when you talk about AI in the developer's world, usually,
you think about AI versus humans, or it's kind of
a fight between both.
AI is taking,
human's job or AI taking developer's job, and it's not
really how it is when it comes to data. Like,
it's not how you
see other influencers talking on Twitter saying that AI is
ending developers or other influencers saying AI is not ending
developers. Actually, developers are still needed to have great
experience,
to know everything, and AI is just a tool that
and you know what? Like, you just hear a lot
of thoughts, but nothing solid, and you're never sure what
to believe because you see conflicting things all the time.
So we're here to talk about numbers. Is it really
human or developers versus AI, or is it that
I don't know? Let's see.
So is AI is just a number another hype cycle?
That's that's a good question that everyone has been thinking
about basically the past, I would say, 3 years.
Is it just the hype? Is it just something that
is new
and then it's just gonna fade away? Something like web
3, for example.
And don't get me wrong. I'm not saying that web
3 is not good. I'm saying that
I remember when
x and it was called Twitter back then. When it
like, my feed was blown up by how many influencers
saying that web b
sorry. Web, web 2 is dead and web 3 is
the future.
Right now, buy this course for x amount to be
part of the future or,
don't mess out this and don't mess out that and
Web 2 died and Web 3 is all what you
need to know and so on. So my feed was
just blown up by these people. And a lot of
companies started their companies and,
started their business on Web 3 saying or thinking that
it's kind
of everything now. Anything else other than Web 3, anything
that you do right now, whatever the front end, back
end framework that you're
using, basically, it's just web 3. And
it's still a very high potential technology, but
none of us felt the impact. None of us felt
like there is something changing or there is something happened
in the industry that affected our jobs or made us
scared or think about,
what should we do next? And so on, nothing happened.
Basically, we're just working. We keep learning. I know absolutely
nothing about Web 3. I'm still kicking it in the,
the industry itself. I'm still finding jobs. I still hire
people that knows nothing about Web 3. So nothing
major changed with Web 3 like influencers and people started
to hype around it. But that's not the case with
AI. So the simple answer he's here is, is it
just a hype cycle? I would say no.
And the reason why is that because AI's real impact
is already here. You can see it. It's not like
we're gonna say AI is the future because
it's not the future anymore. It's the present. We're here.
We see the impact. A lot of people got impacted
by AI. A lot of companies influenced by AI. And
this is something that you cannot deny. It's something real
right now. So it's not just a hype. It's an
actual thing that is affecting our lives and all industries
in general.
So let's take a look at this Stack Overflow 20
25 developer survey. And to see how impactful
AI is already is, surprisingly, you're gonna find 84 percent
of respondents to this survey, which is 33000
developers, said that they are using or planning to use
AI tools
in their development process or workflows,
which is an increase over last year of 76 percent.
So it's actually increasing. It's not a hype that is
going down. It's actually a hype that keep goes up.
And what validates that as well, another,
report here by McKinsey, the state of AI, 20 25,
78 percent of organizations, so we're not talking about individuals
or developers anymore, we're talking about organizations,
used AI at least in 1 business function in 20
24, which is up from 5 55 percent in previous
year. This dark blue line you see on the top,
this is the AI in general. But what you see
here starting from 20 23 is basically generative AI, which
is what we use and what everyone uses now in
their day to day tasks.
And when it comes to,
another report by tech reviewer, AI and software development 20
25,
so you will see that in software engineering specifically,
see that number, 97 percent 0.5
of companies have adopted AI already, rising from 90 percent
in 20 24.
So the impact is real. It's happening. We are here,
and I can go on forever. Like, I can talk
about this report here
says that AI use climbed to 78 percent across functions
like IT, and Stanford I AI index says that 78
percent of organizations used AI 20 24 with global adoption
set to expand at
CAGR, which is, I think, compound,
annual growth rate of 35.9
percent through 20
30, posting consulting group, 26 percent. Like, I can go
on forever giving you numbers of how AI
currently impacting,
our industry and impacting other industries as well.
So
it I mean, all of this is cool, but it
still doesn't prove or it's not a solid evidence that
it's not just a hack. Because all of that,
maybe for some miracle,
everyone agreed that,
it's a trendy thing, so we will keep using it
for years and actually we're gonna use it more and
more.
Because usually trends just trend down and it's just trending
up. But let's say, for example, this doesn't prove that
this is not just a hype.
Well, what proves that this is not a hype is
the real value is already here, not just,
impact due to a trend, but the actually value of
that new element of AI in the current industry.
That's what's true.
So we see Google CEO, for example, stating that up
to 30 percent
of new or existing code is AI generated.
So can you believe that Google reporting 25 percent of
its code base influenced by AI tools as of q
1 in 20 25?
Google, the tool that that we've been using in every
aspect of our lives, basically, and in our phones,
computers,
wherever you go, you just use Google products, And 25
percent of the code base influenced by AI tools in
q 1 20 25,
that's that's definitely mean that there is a value in
that AI generated code. And, also, you see Microsoft CTO,
he previously said that he expects 95 percent of all
code to be generated by AI by 20 20, by
20 30. And,
also, again, this is kinda
I mean, he's he's he's guessing, but at the same
time,
looking at the data, it kinda makes sense because AI
writes more and more code people trusting it more and
more, and it kinda makes sense.
But why this might be surprising? Because the shift has
happening
so quickly.
Right? So if you think about it, when there is
a new technology, someone invented something new, the time that
it takes for everyone to know this new tool and
to adopt it and to use it in production and
to invest in it and to pay for get for
this tool takes years
of kind of,
investigating,
testing, and so on so that everyone knows it and
use it, and it does that major impact. But this
is not the case of AI.
AI, 80 percent of developers use it weekly already and
often running multiple tools in parallel. So you can see
people running cloud code with cursor,
and then maybe even Chagibuty opened on the side to
ask generic questions and maybe another tool and another tool.
And
if you think broader than just developers, you could actually
think
this is this applies to everything. If you think about
the marketing
industry, for example, you're gonna see that AI is automating
most of the marketing tasks right now. You can plan
a whole marketing strategy with AI and just kick it
off, use it, and just enjoy the revenue coming in.
So AI is just
changing things so fast that no 1 knows what's happening
or what's gonna happen tomorrow,
and this is all backed by data and real
life impact that we sense and see in everyday work.
So it will just keep getting better, and that was
what what what was I trying to say in the
panel. AI is just gonna keep getting better. Whatever the
problems that you can think now about agents not writing
good code and so on, you can just look at
the money thrown at these companies or AI assistants,
and you can think about how this money could solve
these problems. You can just hire the most brilliant minds
in the world, give them the problems that AI agents
have now or AI assistants have now that's resulting in
bad code quality, and just tell them, hey. I got
these. For example, Cursor AI raised 900000000
dollars in series c funding at 9900000000.0
valuation in June 20 25. So this money can solve
problems. This money can hire billion people, invest all their
time trying to figure out solutions
for those problems that we see now.
And, also, we see that Windsurf,
which was, in
acquisition talks with OpenAI for 3000000000 in June 20 25
and more. And Tropic Load raised 13000000000
in serious f funding at 183000000000
post money valuation in September 20 25.
Revenue grow from 1000000000 run rate early 20 25 to
over 5000000000
dollars by August.
That's the kind of money that will fix problems, and
that's the kind of money that indicates that these tools
are not dying. These tools are just gonna keep getting
better and better and better.
So how does that impact us as devs? I mean,
all of that sounds like a very bad news for
us because
you're literally telling me that
another tool is just gonna do the exact same thing
that I'm good at. It's gonna do it faster, quicker,
nonstop, and even cheaper.
And it's even gonna get better in time and people
throwing money at it just to make it better and
better. So how does that impact us? This is horrible
news for everyone.
Well, believe it or not, AI is boosting careers, at
least for some.
Take a look at this,
research. So, basically, software developers job projected to grow 17.9
percent by 20 33
with AI jobs up to, 25 percent in q 1
20 25 alone. So you can see here software developers
leading this table right here with expected,
growth rate of 17 percent, let's say, 18 percent
in 20 33. And that's a that's a good indication.
And
hearing that after the data that I just showed you
kinda doesn't make sense. Like, how is it gonna do
better
than me in my job, but still, we're gonna have
more software developers' jobs? And it's expected. Like, all the
reports says that we're gonna have more jobs,
and still,
this tool does our job. So it's kinda
make you think how how how does that work? But,
anyway, let's continue and see what the data tells
us. So 69 percent of devs learned new techniques with
AI help accelerating career growth for adopters. So for example,
Daniel Kerry just mentioned in the panel that he never
thought that he would made a he would make a
course, and in that course, he would write React code.
But he did. Why? Because AI could help them.
So it's the same exact concept. So AI helped 69
percent of developers to learn new techniques and new concepts,
and that's that's even gonna make you think that with
AI, you can actually be more than just a junior
or than just a mid level developer or than just
a senior developer. You can just always be more because
you have unlimited
source of knowledge to learn new things, experiment with new
things, and ask unlimited questions.
So
if you check out the World Economic Forum future of
jobs report 20 25, you're gonna see us taking that
third place. So software and applications developers jobs leading the
fastest growing jobs 20 25 to 20 30. So we
take the fourth place right here. So first, we get
big data specialists, fintech engineers,
AI machine learning specialists, and then software and application
developers.
So that's the fourth place of the fastest growing jobs
from 20 25 to 20 30. So that's also another
great indication.
So while it helps
some to actually boost their careers, it's actually quietly squeezing
other jobs out.
So if you take a look here, you're gonna see
that the top fastest declining jobs, so whatever jobs that
are dying from 20 25 to 20 30, you're gonna
see that we are not listed here. Like,
we are in the fastest growing
job, like, growth rate job. So right here, we see
that we are actually not mentioned in the fastest declining
jobs. All of it are tasks that logically make sense
that it's gonna
disappear.
Right? So we have postal service clerks, bank tellers and
related clerks, data entry clerks, cashier and tickets clerks. All
of these things could be automated, and we're not here.
So it's it's a good indication that that
there is hope for developers, basically.
But
it's not always good news. It's not always bad news.
Postings for entry level jobs in the US overall have
declined about 35 percent since Jan 20 23, since the
revolutionized
the like, the movement of AI started to happen.
And through 20 27,
generative AI,
with gen AI, will spawn new roles in software engineering
and operations,
and 80 percent of software engineers
must upscale by 20 27
to stay relevant in the AI driven landscape, and that's
by Gartner. So according to Gartner,
it's not only entry level. It's not only junior, med
level, whatever. It's basically all developers. But whatever your
experience
level, you're gonna need to adapt
with AI driven
landscape and just explore what's possible with this landscape because
the world is changing so fast. And if you don't
change as fast as it does,
you're just gonna be left behind. And it's not like
the hype of being left behind so that you take
action immediately. It's it's just a real
impact that we're living in right now. So according to
Gartner, 80 percent of software engineers must upscale by 20
27
due to new roles.
And that's
a good answer for
why do we see
AI getting too good at doing our job, but still
see that it's expected for our to have for us
to have more jobs opportunities in the future. So that
solves the mystery. Basically, you're expected to have more job
opportunities because you're gonna have more roles, things that are
gonna come out of nowhere as a new specialized,
area that you can actually take over, be specialized in
it, and just leave it. Like, the question we had
in the panel about specialized
vibe coding, debugging thing, that could be a thing.
That could actually be a thing, and that person could
be a very good developer who knows coding so well
or a very, very good communicator with AI, who just
knows how to talk with AI, debug with AI, provide
AI with the resources that it needs to actually generate
solutions for those things. So
so many job opportunities are coming in and no 1
can anticipate what type of it, how is it gonna
be like, but we just know for a fact that
we need to keep up and learn the new tool
as fast as possible.
And the reason why there is a rush here is
that because how quick it is changing for everyone. Usually,
when there is a new tool, it's few years until
it's widely adopted. But with AI, things are moving too
fast. So that's why we need to adopt.
So
bottom line here is that AI isn't ending jobs. It's
ending the career ladder for the unprepared.
So if you think about it, that's that's
a great description
of of how AI is reshaping the world. It's it's
just that it's ending the career ladder as we know
it. It's just not like junior developer and then they
get promoted to a mid level developer and then get
promoted senior developer and then and so on. Now, basically,
if you're a junior developer and you're very good with
dealing with AI tools, providing it with the right contexts
and dealing with that very, very,
efficiently,
it's just gonna generate good code for you, and you're
no longer a junior developer. You can you consider yourself
a senior developer because you know how to plan with
AI. You know how to provide AI with best practices.
You know how to validate AI output. You know how
to test AI output. So why would anyone call you
a junior developer anymore if you can generate amazing output
in a very short time?
So think about that because that's the key here.
So for junior and mid level developers,
well, they are kind of under pressure,
but they are not disappearing anytime soon. So you would
need to lean into AI tools and AI related expertise
and demonstrate strategic
value beyond code typing because the syntax is not the
problem anymore. It's just the least of anyone's concern right
now. Whatever you're a view developer, react developer, it doesn't
matter as long as you
join me and my team, deal with this code base,
and generate amazing output that is tested, and it works
well in production.
So
yeah. Basically,
after all, it's it's just
not a junior developer anymore. It's not how much you
can learn and how much you can implement and how
come much how much you can achieve with AI tools.
As for senior
developers, so far, your role is evolving and becoming more
critical.
You are the human bridge between AI and whatever real
world thing that you're trying to accomplish. So you're designing
systems with AI, ensuring quality that generated by AI, monitoring
team and guiding them into leveraging AI into doing AI,
generated code, more efficiently, less buggy, and all of these,
and leveraging AI without letting it mess everything up. So
as a senior developer, it's gonna be part of what
you do. So you should start learning how it works,
and you should start learning how to adapt with whatever
changes.
So it's not always simple with AI.
All of that sounds good, but take a look at
this. I received this email a while ago, I think,
a month ago, by the programmatic engineers. It's a great
newsletter, by the way.
And
take a look at that. You're gonna find plenty of
conflicting research paper and analysis on the Internet. Some of
them pro AI, some of them against AI, but look
at this study. An interesting study has been published by
the nonprofit organization,
model evaluation threat research. They recruited 16 experienced developers
who
worked on large open source repositories
to fix 136
real issues. For pay of 150 dollars per hour, some
developers were assigned AI tools to use and others were
not. So some developers used AI tools to help them
code and others didn't. The study recorded the screen and
then examined and analyzed 146
hours of footage, and the takeaway is surprising.
Surprisingly, we find that when developers use AI tools, they
take 19 percent longer than without.
AI makes them slower. This gap between perception and reality
is striking
because developers expected AI to speed them up by 24
percent. And even after experience the slowdown, even after they
got slowed down during doing those tasks, they still believe
the AI have sped them up by 20 percent.
So that's crazy. That's that's a study. Of course, that's
not a lot of people.
16 experienced developers, that's not much. And you don't know
how much how much experience
in AI they are or how
much the input, the context, and the preparation they did
for their AI agents or whatever tools that they are
using. But that that's that's something that needs to be
taken into consideration.
And,
that's basically what drives us to build AI driven development.
That that's the concept of
that we always need to keep up
and that we need to leverage AI the right way
in a time that the right way is not clear
yet for everyone. And everyone has his own right way.
So we try to bring
brilliant
developers together
and come up with a solution that would actually make
us,
I don't know, figure out the best way we can
do that because they figured out the best way they
use the AI. So So we're talking about, for example,
right now, Justin. Justin is an amazing open source developer,
and he did a lot of cool projects. And he
believes, genuinely, that AI is underhyped, and it can do
even more, and it helps him generate more and better
code and so on. So when you think about it,
it's just
it's just that we're we're just trying to solve a
problem that has been there for a while and I
would say for 3 years, and everyone just doesn't know
what is the right way to do it to do
it. And we're not claiming that we have the right
way, but we're claiming that we have the best way
that we can think of, and we are brilliant at
teaching.
So, yeah, I bet, everyone is gonna enjoy AIDD.
And, that's it for my talk today. So it was
full of data, analytics, and so on, but I wanted
to present data to speak rather than, just giving opinions
and so on.
We're launching on September 30. By then, we're gonna have
a lot of the content developed already.
But like I said in the panel, there is a
slight chance for whoever is watching right now and only
who is watching right now to purchase AI driven development,
with a 50 percent discount that was open months ago
for insiders. So you can become 1 of the insiders
and help us found this thing and,
and develop it together. Right now, I think the entire
course 1 is complete.
6 or 7 lessons, of course, 2 completed.
And by end of month, we're gonna have probably 4
courses fully completed, and we're starting with course number 4,
5, and I believe 6.
So,
happy to take on any questions. Sorry for going overtime,
Daniel, and
that's it.
Awesome. No worries, Mustafa.
I think we are pretty part of the course for
a little overtime here at at Bitter Brains.
It's part of our MO
sometimes, but it's all good stuff. So,
that said, only have time for just a couple quick
questions.
Could you unshare your screen there for me? Yep.
And
then we will bring up this question here.
So from the surveys,
you analyzed,
did you get any data on developer tasks that have
seen the largest increase
in AI generated content testing,
boilerplate code, or something else?
Yeah. So, basically, right now, AI is mostly trusted with
the things you mentioned. So boilerplate code, testing,
basic logic things, and,
adding some padding to the left, padding to the right,
and so and so on. So that's the most reliable
thing that most probably AI is not gonna rule things.
So that's how I saw the data.
And I was actually, I included,
a slide about that, but I didn't have much time
developing these slides.
But, anyway, yeah, most of the tasks that have been
assigned to AI
are basically related to boilerplate code testing and doing all
sorts of these,
what is it called, boring
tasks. Right? So the things that, nobody likes to do
and usually assign to junior developers.
That's basically
what is it more or most reliant on,
or what people mostly rely on because people doesn't yet
trust AI to just give it a prompt and do,
like, thousands
code line changes in their code bases.
And,
if you think about it, part of it because we
don't invest that much time into
giving AI the context that it needs and setting up
a workflow like we discussed in the previous talks.
Awesome.
Alright. Next question.
What is,
your opinion on tools like base
44? Have you had any experience with that?
I did come across it, but I haven't tried it
yet.
But but I wonder what's the the difference between such
a tool
and,
and Bolt and Lovable and other tools. Like, it's just
that I find all of them fascinating
and magical. That's that's my opinion on these tools, and
I find this is gonna
be
part of 1 of the new jobs that is gonna
rise in the near future where some people are very
excellent at using these tools
to an extent that it becomes a an speciality
or I don't know how to pronounce that, but it
becomes something that it's a dedicated job forward, that you're
a Vibe coder and you know how to deal with
these tools and get products out there. Because something here
that I would like to mention quickly is,
all of us are developers, and usually, we work for
companies that are focused on the output. So
you rarely find,
a company, for example, with a big code base, let's
say, that they are using
Nuxt 3. And then when Nuxt 4 comes out, the
CEO comes in and say, Nuxt 4 is out. Let's
upgrade. No. This never happens. You actually have to cry
about it for years
telling the leadership team how Nuxt 4 is gonna speed
up the process so that we need to spend some
time to upgrade, and then the leadership team would say,
no. In that time, we can ship more useful products
out there and earn more. Right? And if you think
about it, neither is wrong. A developer wants a better
tool to develop better and write better code. And at
the same time, the leadership team wants to,
invest in the output and get more,
revenue from from, from the company and for the company
and so on. So it's the same exact concept. It's
output. So if these tools getting us good output quickly,
yes.
Do they do it perfectly? No. But there might be
a time where Vibe coders are a real thing. Those
tools are decent enough for people to actually be specialized
in that.
Yeah. For sure. I mean, the money flowing into tool
tools like Bolt and Lovable right now is just absolutely
insane. So there's very clearly,
a value prop there.
If Yeah. If anybody, by the way, has used Base
64,
love to hear your thoughts in the the chat.
Alright.
Actually, got
1 more question I think we can squeeze in time
that popped in last minute that I think is a
nice 1.
How do we motivate people to learn AIDD
when just wanting to code themselves?
Well,
I think
you have the freedom to code yourself
whatever and what whenever you want. Right? So, for example,
most of the side projects that we build and they
never see the light or they never go out there
is basically because we want to code them, and we
want to implement that logic, that crazy idea that we
have in mind, and no 1 would allow us to
work in their code base and do it for them.
Or the company that I'm working in, they're not buying
this idea, and they are not willing to do it.
And I just have it and it works fine in
my mind and I just code it. So you can
always do it and you're all like,
I cannot say for sure that AI is ending coding.
Like, no 1 can say that for sure. I know
that Justin likes to say that he knows everything, but
I don't think that no 1 knows for sure if
AI is ending coding eventually and just, like, a new
rise of new jobs and all the old ones gets
deprecated.
But,
yeah,
if you ask me about my personal opinion,
I would think coding
is not completely dying. It's still gonna be there, but
it's not gonna be
used
as frequent as it is right now.
For example, like I said previously in the panel, I
believe in the future,
I don't know when, 10 years, 20 years later, maybe
after new generation,
5 generations later, I don't know,
at that time,
maybe it's just gonna be it doesn't matter what programming
language you use. It might have its own programming language,
might take over the entire Internet and so on. But
still,
you
like, your role as a developer
or
the capability or the ability for you to do something,
to write code, it's never ending. That's how I think
of it. Like, it's never ending. It might not be
be needed as frequent as now, but I don't think
it's gonna be completely gone. Maybe a whole new thing
is gonna come out, and you're gonna be forced to
learn it, And then that's gonna be the new thing,
and it's gonna be very similar to coding. But the
coding concept itself, the enjoyment of coding, because I love
coding as well, it's not going for good. It's still
gonna be there. But this is just my personal opinion.
Cannot state for sure.
Yeah. Makes sense. Alright. Thank you so very much for
your, your time and your expertise, Mostafa. Great to have
you on, sir.
Thanks, Daniel. Thanks, everyone.
See you. Bye bye.
Alright.
Next up is the time
we've all been waiting for,
the wonderful,
break time. So I know we have been
going at it hard now for over 4 hours.
Let's give everybody a well deserved let me make sure
got my coffee break slide up here. A well deserved
break. So this time around, we are a little bit
behind schedule, so let's take 10 minutes
and be back here in 10
minutes. For me, it is 1 0 9 PM.
So we'll say at 1 20 PM, be back here.
So in the next 10, 11 minutes. See you soon.
Alright, everyone. We are back from our coffee break.
And next up,
I'm gonna talk some more.
Honestly, yeah, it is, it is my time to talk,
so I'm going to be doing more than just the
MC. I'm gonna share a little knowledge with you that
I have picked up along my AI driven
development journey.
Primarily, I'm gonna be talking about an introduction to AIDD
and how I use Cursor
to be more productive. So we're gonna start out pretty
basic.
And then very quickly,
we're going to move,
to some really practical tips that I found useful
for my everyday development
workflows.
I tried to kind of hit all the different levels
of audience attendees. That's extremely difficult to do.
So,
if you you know, we are gonna start start out
just a little bit slower to more base level.
Just want to mention that, you know, if this is
something you do already know, I think it actually breaks
down and helps you
teach other people
who might be on your team who aren't really sold
or bought into,
aidd yet.
Anyways, all that being said,
let's hop into my topic. I'm going to keep it
quick and snappy because I know we are just a
little bit behind schedule.
Okay.
So really quickly, what are developers
using AI for?
Well, there's tons of different tasks that developers are picking
up AI to do. Things like app ideation and planning.
I don't know how many times I've, you know, talked
with,
Cursor is my,
my weapon of choice, but I don't know how many
times I've talked to Cursor with the word brainstorm. Let's
brainstorm this new thing. Right? So,
app ideation and planning,
bootstrapping
new project code, getting, you know, dependencies set up, getting
a database,
you know,
a database model set up, things like that,
writing code for new features,
debugging,
refactoring,
reviewing code in PRs.
This is something that Garrison actually goes over a little
bit in his course. It's really interesting
how
AI can not only help us write new code, but
it can also kind of inject itself in other portions
of the development lifecycle.
Writing unit and integration tests.
Testing was really tedious for me and has been for
a long time. So much so that in some cases,
I choose not to do it,
when I when I really should have,
and don't have nearly the code coverage that I should.
But
given
the right parameters and and the right code to look
at, I find that AI is really good at writing
unit tests.
It's really great at explaining existing code.
Even learning from real world code, try this little exercise
sometime. Go to 1 of,
1 of the open source libraries that you admire or
that you use
and just grab some code from it. Like, grab the
raw
raw of a file in GitHub and throw that into
an LLM and ask them to explain to you how
it actually works. I've found myself learning some really interesting
concepts
by just throwing
real world production code from people I admire into those
LLMs.
Great for
writing documentation,
migrating dependencies to new major versions,
converting logic from 1 language to another.
I've actually done this with,
I've done this with Vue components, changing them to a
React component. As I mentioned a little bit earlier, it's
just it's given me so much more confidence
in being able to work in frameworks that I never
felt comfortable in before.
So yeah, converting components from 1 framework to another,
writing less human friendly code like
regexes.
I've used
it's it's not perfect,
but it gets me a lot of the way there
using AI to write a regex for me and then
asking it to write tests to validate that regex,
and then kind of looping between those 2 things until
I get the output
that I I want.
Things like rapid prototyping,
quickly copying designs.
So this is just
a good list of things that I've seen other developers
using AI for or things I myself have done with
AI.
So how are developers in the current landscape
interacting
with AI?
I think this is really important to to understand and
kind of gauge where
you're at on this spectrum
and where other members of your team are at on
this spectrum because it can help you see,
kind of what else you
can
can push to to adopt next. Right? It can help
you see,
where to go from from here.
So this is what I like to call the AI
assistance
spectrum.
K?
So level 1 of this AI assistance spectrum and this
is to describe how developers are using AI.
Level 1 of this AI assistant spectrum
is using browser based AI chats. This is the easiest
way to get started, of course, with AI. It's great
for planning and research. And actually, I do a fair
amount of planning and research inside inside of ChatGPT. There's
nothing wrong with me as a developer
jumping out of cursor and going into the 1 of
these
browser based,
interfaces for an LLM.
I know some developers who are like, oh, that's, you
know, that's what the laypeople do.
That's what the people who aren't developers do. But it
really does still have great use cases. Right? ChatGPT is
a great brainstorming partner.
But level 1,
is where developers really get stuck here, is they stay
in the the browser.
And, really, the biggest downside of this is the context
is limited to what you put in. Right?
So moving up to level 2
in the AI assistant spectrum is adopting
AI as an in context
Copilot.
Right? That's things like,
that's things like chat inside of Cursor or
Versus Code, GitHub Copilot, just kinda where you're asking questions
about
the code that you're you're working with,
or having inline autocomplete. We've all seen this. This has
been around for quite a while now.
But this I would consider to be a a fairly
base level usage of AI, k, in in level 2.
So the benefit of level 2 though over level 1
is that now we have more context about the projects
we're actually working on. These tools like GitHub Copilot and
Cursor can smartly add that context
based on an indexed code base. By the way, inside
of the AIDD course, 1 of the,
or the master class, 1 of the courses I'm working
on right now
is how to implement a RAG pipeline
and use it to augment,
augment generation
with your own,
data. Maybe company proprietary data. Maybe just, you know, data
specific to the,
use case,
you're you're working with.
That same type of RAG pipeline
that I teach you how to build inside of the
AIDD course is actually what's,
what's powering
the search, the retrieval capabilities
inside of tools like Cursor and GitHub Copilot.
It's how those tools know how to pull in the
relevant context.
Okay? So it's very real, very powerful stuff.
And this InContext
AI Copilot,
also takes into account custom rules saved within the repo,
and the developer still retains a very high level control
at level 2 of the AI
assistant spectrum.
K? Level 3 of this spectrum, though, is what I
like to call supervised
agentic coding or what I think the,
I think the the CEO of Perplexity called flow.
In other words, AI is given goals to achieve
and tools to execute those goals,
and, then you kind of let it let it go.
Right? This is essentially using agent mode in something like
GitHub Copilot or Cursor or using Claude and sitting there
and watching it, you know, as it goes through and
checking it as it's as it's writing. Right?
In this level, AI is given some more freedom
to act on its own while human while there's still
a human there to provide attentive feedback, as soon as
it you see it going off the rails, you can
hit the stop button, and you can, you can intervene
and and get it back on track. Right?
That's what I consider to be
level 3 of this AI assistant spectrum.
And then we finally, we have level 4. Okay? This
is where we get into
autonomous agents
that run all by themselves. They're not necessarily supervised. You
really only check up on them at the very end.
These agents are given a goal. It's a well laid
plan. It's something that's more than a sentence or 2,
probably
several paragraphs or more with maybe some lists in there
about, hey. These are the things I want done. K?
And they're given tools.
Tools are extremely, extremely important in this level of the
AI assistance spectrum. Why?
Because that autonomous agent must have a way to get
feedback in order to course correct. Right?
The really cool part here is that these autonomous agents
can be run-in
parallel.
I have had some success,
with that. My success is limited compared to the success
of some other people.
So results may vary. Right?
But that is
the future, I think. It's where we're headed as as
developers. I don't think it's a hundred percent there right
now, but I think level 4 is is really where
we want to be eventually.
K?
Professionally and just as
a,
as a as a whole. What's the word I'm looking
for? As a profession.
Right?
So what I wanna ask you now in honest introspection
is
what level of this AI assistant spectrum
are you currently at? What have you been so bold
to actually
adopt?
Feel free to throw it in the chat if you
if you want.
I'm not gonna make you do that. But I do
think it's important for you to truly ask yourself, which
level are you at? And if you're not
at maybe level 3
or and even dabbling there in level 4,
I think you're a little bit behind.
K?
I'm just being perfectly honest. I think you need to
be fairly comfortable with level 3 at this point in
time, and you need to be dabbling
and aiming to get to level 4.
Alright?
So just to get a little bit more
a little better idea of level 3, some of the
tools
that are available,
for use in level 3, things like Cursor, GitHub Copilot,
JetBrains IDE with Juni,
Windsurf, Cline, RooCode,
all of these things that you can
give it a task, but still keep an eye on
it as it's working.
CLIs,
can be used in both level 4 and level
3,
but some of them are listed here. You've also got
some web based tools that I think also kind of
fall into this level 3 category.
And then some of the tools for these autonomous agents,
that I personally like.
Cursor background agents is something that I've found some good
success with. And the thing I like about cursor background
agents over something like Cloud Code yes. I think you
can use Cloud Code as an autonomous agent that works
totally in the background.
But,
I think there's more security concerns on that because it
still all runs on your machine. Now, of course, you
could run call code on a remote machine, and then
I think you're 1 step further to a better workflow.
But cursor background agents,
they run on a remote machine by by default. So
there's a lot less,
surface area of risk for that that agent going rogue
and stealing information outside of, you know, the project that's
on your current machine.
And of course, cloud code has
breaks and stops and protections in there too, but a
lot of that relies on,
you know, you giving it access, you giving it permissions.
I think the ability to run these agents
on a remote server is a huge
plus for security, and that's something that Cursor Background Agents
gives you out of the box. So I really like
Cursor Background Agents.
Okay.
So that is the quick high level overview.
Just checking my time here. Don't have a lot of
time left. Man, that moved faster than I thought it
did. Okay. I'm going to try to be quick. So,
some quick
prompt engineering and AI agent tips for developers, I. E.
Some ways that I've made Cursor
work better for me.
So some general prompting tips that I have found helpful.
We'll start by listing out some simple rules and then
take a look at a more practical use case,
like case study example of these rules in action. Okay?
So,
as Phil mentioned a little earlier on,
prompting as a whole
is dead, but I think you still need,
to to know some general best best practices. And these
are ways that Cursor has
made better for me. K?
So provide examples.
Provide examples when you're trying to get agents to do
something. They have these terms called 0 shot, which is
no example, 1 shot, which is 1 example, and then
few shot, which is several examples. Okay? I've found that
for,
for a lot of use cases, 1 shots work great.
Providing no example over,
or providing 1 example over no example usually produces the
best results. For a few shots, several examples,
these are things that often live inside of my rules
files, right?
I'm gonna skip past those
there for the sake of time.
But another
prompting rule that has been very helpful for me is
using XML
to group concerns. This is something that actually comes directly
from Anthropic's prompting guide, and I found it applies pretty
well in terms of coding,
use cases as well.
So I don't know how many times I'll find myself
copying and pasting, you know, something
from somewhere on the Internet into my my code and
putting it inside of these brackets to say, hey. These
are the docs that I was referring to or something
like that.
Yeah. Markdown headings also are good for grouping.
Cursor rules, whatever your
your flavor is,
These past 2 tips honestly are even more useful in
those rules files than directly inside of prompts.
I don't think we need a further explanation on cursor
rules.
Don't let AI start from scratch
every single time.
We're going shotgun approach here, going through a lot at
1 time. Don't let AI start from scratch every single
time. Start with,
start with existing
libraries, frameworks, tools
that have been vetted by human beings
and and tell the AI about those and let those
let the AI use those,
responsibly.
K?
Another way that you can not start from scratch or
not let the AI start from scratch is
give it a way to generate things from a template.
K? This is something I found to be quite,
successful.
1 thing I've done in a few little apps is
download a,
a CLI tool called HyGen.
There are other alternatives
alternatives to this. HyGen is a bit abandoned at this
time, so I wouldn't necessarily recommend this at this point.
But there are alternatives, but this has worked,
for me for a long time. And what this HiGen
allows me to do
is actually, create some templates, and then I can just
tell
my AI agents to use those templates and start from
those starting places so that it it kind of knows
the pattern of things,
you know, the patterns within my my project, and it
doesn't start from scratch.
So since agents can run commands,
just make sure your your cursor rules
know about the different hygiene commands
that you have available
and when they are useful.
Mention files as examples.
A lot. Like, this is maybe even the biggest
thing.
If, you know, let's say you wanna add a calendar
component to your application.
You could say just add a calendar component,
right?
That works sometimes.
And if you have a thinking model, it might even,
it might even, you know, scroll through your other components
and kinda see some patterns that you're used to using.
And in that case, it might work.
But a a better prompt might be create a calendar
component and read 2 to 3 of the existing component
files for inspiration,
especially
at whatever this other component is that I think is
very similar. Right? And in this case, you might even
be able to,
reduce your your model power. Right? Use something like auto
as opposed to cloth sonnet
and, save a little money, but still get a a
good output. You'll have to play with that and tweak
it a little bit for your needs.
But I found that pointing it to good examples, if
you already know in your head what a good example
is, then why not just go ahead and mention it
and save, you know, some thinking power on the AI
side?
Limit your request scope. Don't ask for the full on
app with all the bells and whistles. Go 1 step
at a time.
Why?
Because you have control issues?
No. Because when you ask for a scoped and limited
result, we've talked about this already, I think, the AI
does things quite well. If you can define a very
definitive output,
what the result needs to be,
it actually does things quite well. So if you define
an output this vague or way too overblown,
then the AI touches a lot of different aspects of
that request, but usually never completes any of them adequately.
So limit your request
scope.
Okay. So very quickly here,
let's, let's take a look at a cup at a
use case example of some of these prompting
tips. Here is a simple blog. Alright? This is a
blog application that I booted up with AI not too
long ago. Okay?
And,
typically, I like to start by brainstorming
as my initial prompt. Since this was a blog and
a pretty common project,
I did just go ahead and skip this step.
By the way, yes, I know a blog is pretty
simple, but I've used the same or similar rules in
other larger production contexts successfully.
Just the scope of, you know, keeping it something simple
everybody understands
is often better for teaching, and that's why I choose
to do that.
Okay, so here's my prompt to start out this this
blog.
First, I want to create a dummy blog. I want
to store a post in a database table.
Could you help me set up the schema and the
seeds? Alright. And then I point
to
the database
dot mdc file. So this is my rules for working
with the database. I'm still kind of in control. I'm
telling it what I want it to do. Right?
And notice I'm also limiting the scope to just the
database concerns.
I'm not saying,
boot up the database. I'm being specific about what I
want. Help me set up the schema and the seeds
so that it doesn't make up its own agenda.
K?
This is the database rules for this prompt, by the
way. So this project uses Drizzle as a database RRM.
The schema can be found here.
On the server side, you can call an auto imported
useDB
function in order to interact with that database. And seed
should only be created as Nitro tasks. So notice how
my rules files are very specific
to my
the patterns within my project. Right? But also notice they're
very simple. Here we have a
1 shot example
of how it should create a seed.
And up here, we're not explaining what Drizzle is. We're
not,
you know, we're not explaining what a Nuxt task is.
We're we're being very simple, and and we're expecting that
the LLM is trained enough on what these different,
libraries do
externally, these popular libraries, and really scoping it down to
just what's unique about our project. Right?
If you find that there's a gap in the LLM's
knowledge about some open source tool you're using, that's when
you decided to add in a rule. K? Not before.
Okay.
What was the result?
Well, I got a TS error,
unfortunately,
but that was easily fixed by hand after a couple
of failed attempts from the AI.
Then I was able to Drizzlekit push, Drizzlekit studio, and
I had my database
set up. K? Took just a few minutes.
So that was 2 prompts, 1 manual fix, about 10
minutes worth of work. This is something that probably would
have taken me, you know, I don't know, 20, 30,
maybe even 40 minutes doing it all by hand.
This is something I
I,
this is something I I want to emphasize to people
is AI isn't a magic bullet. Right? It's not just
going to do everything for you. A lot of people
come in thinking, you know, they're gonna build this production
worthy app and a few prompts.
No.
But scoping things down and these wins turning 20, 30,
40 minutes into 10 minutes, like, that's not trivial. That's
a huge win. Right? And that's what I find,
AI to be to be really good at.
Okay.
So for some API endpoints, I'm going
to stop after just a couple more slides because we
are short on time, but I I want to get
through this 1 here.
So for the API
endpoints,
I said, okay. Forget the TS issue because I fixed
it. This is something I found that is important when
working with an agent in Cursor. Sometimes you gotta tell
it, hey. I fixed this. It's changed. Don't worry about
it. Okay? I manually did that. Otherwise, it'll try and
go off and fix it again itself because it's something's
different.
Now let's set up the API endpoints for each of
these,
post resources.
Give me the full CRUD operations.
And I mentioned a specific rules file
that,
deals with my preferences for
for MVC or for APIs.
Okay. And so this is what that API rules file
looks like.
All API endpoints follow the OpenAI
specification.
When creating new resources like users, posts, etcetera,
use the following command to bootstrap all the CRUD API
endpoints. So notice how I'm using,
that template approach here to get the files started
that the AI can then just fill in based on
the the use case. K?
Always read the created files after running the hygien command.
And if you need to create any API endpoint that
supports streaming,
don't use 1 of the hygien commands anyways, just exceptions
there.
Okay.
And my result after a single prompt,
because I used a template with a placeholder content in
it for OpenAPI,
documentation,
I immediately
had
not only a working API route, but I also had,
documentation,
Swagger based documentation
for that API route. So this is 1 of the
1 of the big differences between, like, vibe coding and
an AI driven dev. Right?
We as developers
know that it's
there are things beyond just making it work that are
important. There are things like documenting things along the way,
things like making
API docs.
And
and you want to bake those kinds of concerns
into your AI interactions.
Right?
And that's what I was able to do there with
with Kurtruser.
Okay. Great. Going through, the rest of these slides just
really quickly, I will share these on my social media
after,
after the event. So if you do wanna look into
these other ones,
you can take a look
at them
because I think they are super useful.
But, yeah, for the sake of time, I just don't
want to,
to hold up the other speakers.
So just a few miscellaneous prompting tips that I've heard
or experienced
before I leave you.
Number 1, just ask if the AI is familiar with
the technology before you ask it to develop with that.
Right? You may know,
a certain tech stack,
top and bottom,
and so, therefore, you start out with that stack in
mind,
but it may not be something that LLM has really
been trained on.
So you you need to vet upfront.
Is the AI going to be helpful in this stack?
Right?
And and really kinda gear yourself towards something that the
LLM can be more helpful with.
Honestly, at this point, I think it's more important that
the LLM be familiar with the stack than necessarily you
be familiar with it. Now
well, I won't go into that, but you get the
idea. Make sure the LLM is,
familiar with your stack. Okay?
Checklist for complex tasks. This is built into a lot
of IDEs now.
Add links to GitHub Readmes. Use context 7. Anything you
can do to get that context
into
the,
into the agent.
Use NI to avoid mix ups about package managers. I've
adopted this wholesale. It's made my workflow easier, not just
with agents, but even just personally with me typing in
the command line. I don't have to remember, am I
using PMPM? Am I using NPM? Am I using yarn?
Maybe this is more of a me problem because I
work on a lot of different, you know, educational kind
of projects, and maybe you're just working on 1 or
2 at work. But n I is like a replacement
that just automatically detects
which package manager is being used
for that project and it does the thing in the
proper, you know, with the proper command.
So I actually even have this as a global rule
inside of my user rules and in Cursor. So you
can you can create rules inside of Cursor that apply
to every single project.
That exists under the Cursor Rules page, then the Rules
and Memories tab. And these user rules right here,
are applied to every single project that you work on.
K? So I just tell it use NEAT and then
tell it a little bit about how NEAT works.
So we already talked about that.
I've I've heard a lot of people say using TDD,
asking agents to use TDD works really well. I haven't
really tried that myself, but I've heard a lot of
good things.
Do use TypeScript
and ESLint
as much as possible.
Why? Well, because
these tools bring relevant context to the agents at just
the right time. These these ESLint rules, these TypeScript,
you know, type checking, they're so much more they're so
much better than just rules files alone because they bring
the context
at the exact right time. Right? So learn TypeScript if
you don't know TypeScript,
install ESLint,
even create your own custom ESLint rules for specific project
patterns. I've experimented with this a little bit as well,
and it works decently.
Okay.
Let's
move through
because we are almost out of time here. Yep.
Okay. Thank you very much, everyone, for your time. If
you did enjoy this talk or this event at all,
we'd love to hear from you on Trustpilot.
Us here at Bitterbrains at AIDD,
we'd love to get your feedback
there. I will provide a link to this in the
chat
shortly,
after I get off of stage.
Thank you all very much for joining my talk, and,
I look forward to the next 1.
All right.
Alright. I'm gonna just grab 1 quick question here from,
the chat once again because of time.
Yeah.
Okay. Alright.
Clarification on the AIDD
course. Is the content set, or will it still evolve
in the future? And will the Insiders get access to
the updates
without additional,
purchases?
That's a great question. Yes, we will keep the content
within that AIDD Masterclass
fresh insomuch
as,
you know, insomuch that if content changes
that we have that we have produced and something is
out of date and something is, you know, no longer
accurate, we will give you better solutions,
and we will update those to the to the latest
things. But we won't necessarily just continue adding new courses,
you know, forever for all the new tools and and
things like that. Right? So we will strive to keep
it, up to date and and accurate.
But the next gen dev platform as a whole is
really an expansion of AIDD,
and we will forever be adding, like, new content there
for,
you know, for new tools,
for exciting new things,
like that. I hope that makes sense.
Okay.
Alright.
Checking my chat here.
Yes. I I
will, of course, share,
share my slides. So I am I will add it
in the chat. I am Daniel Kelly,
underscore IO on,
on Twitter or x. Twitter is really where I'm most
active. I'll share my slides there on Twitter as well
as if you just search for Daniel Kelly like View
School or Bitter Brains on LinkedIn,
you can find me there as well. I'll share my
slides there too.
Awesome. Thank you all so very much. So let's get
on to,
let me check my,
my schedule here.
Alright.
Next up,
I am going to give a quick shout out 1
more time
because I don't want y'all to miss this. I think
we've we've worked really, really hard on,
this AI DD course, and and I just want to
throw it out there 1 more time. Alright. The AI
Driven Development Masterclass.
Our goal is really to teach you to integrate AI
into your coding loop with practical tools
and workflows. Remember,
that is launching on September thirtieth. And as Mostafa just
mentioned
just a little while earlier, we are opening the Insiders
Pass for a little bit longer.
Make sure to grab
your
grab that link from the chat. My colleague Kiara
will place that in there again.
Alright. Next up,
I am so excited to invite Benedikt
Stimelt
to the stage. In fact, first time all day. Let
me actually
go ahead and invite him and let him start getting
on
while I am introducing him. Give me 1 moment.
Benedict.
Alright. And we'll remove these others.
Excellent. Alright. Inviting Benedicte to the stage.
Now, Benedicte
is a seasoned Agentic software engineering advocate. He's a software
architect, a full stack developer, and conference speaker with over
20 years of experience
in enterprise engineering leadership
with a focus
on
AI. I've
heard from from Benedicte, on several occasions. He's given me
some really good advice on
some Agintiq workflows before. We've chatted a little bit on
on LinkedIn,
and I am happy to have him
on the stage with us today.
Alright. That timing was almost perfect.
Hello, Benedek.
Hello, everybody.
Sorry. I I didn't get the last part because I
needed to leave and and join again because of this,
issue with the co hosting thing. I I hope everybody
can hear and or see me.
I think so. I can hear you and see you
fine. Awesome. And let's see.
Yeah. I can't share my screen, though. There is no
the button is grayed out. Maybe it's a permission thing
here in my MacBook. What about now? Now now I
can I can share it? Yeah. Now I can share
it. Excellent. K. Awesome. Alright. And I'll call you back.
Yeah. Hello, everybody. I I'm, as you can see from
my background,
I'm kind of, like,
in a in a hotel room here. So sorry about
that.
But I'm trying to to give you a very great
talk. You don't need to see me actually, only my
screen.
I am trying to share that now.
Maybe there's a permission issue, but let me see. Can
you yeah. You should see it. Right? I can see
it as well.
You could see my terminal. Right? Is it working? Yes.
Okay. There's a yes in the chat. Okay.
So I have, actually, I have no slides.
I I wanna share some thoughts and and,
show you a little bit about permission management
and,
how to actually be able to to run agents,
more safely than than you are probably doing at at
the moment.
Not not sure what your favorite tool is at the
that you are using. Mine is, in this example, the
Cloud Code.
You've you've probably seen a lot of different tools today,
but, yeah, I'm I'm at the moment using Cloud Code
a lot.
And, just to give you
a a quick example,
how this this works if you haven't seen it yet,
but I'm I'm I'm sure you have, but, let me
just,
in any case, just just open cloud. This is how
it looks,
and then you can, like, ignore the failing MCP server.
It doesn't really matter. And then you can just say,
see that there are different modes here.
You can plan stuff. You can just ask stuff or
you can auto accept edit.
Let me just quickly see if it works.
What is this project about?
And in a few seconds, he would, like, tell us
a little bit about the project,
that I'm that I brought with me to
actually show you
the features here.
Okay. So he's kicking off sequential thinking.
Whatever. Why? I'm not sure why he's doing that, but
okay.
So you can see there's an answer here
with, the information about this this project.
Doesn't really matter. I just wanted to show you that
that the setup is is working, and and he's, he's
able to to use,
Cloud Code here.
So
and whenever I I wanna make edits,
I can go into this auto accept edit mode. Right?
And and, then he's going to allow some tools to
write or read files, for example,
but other tools that he's going to run-in the bash
are are not allowed.
And he's going to ask me,
on whether it's okay to to run that or not.
And
this is kind of getting annoying when you are using
these tools
a lot and and you want to make them run
on on their own a lot. Right? And and you
you wanna go away, do something else, and not always
check back at your windows and say, yeah, that you
are allowed, you are allowed, you are allowed.
So, basically, what you wanna do is you wanna run
Claude and,
let me do it manually and, like,
what the fuck.
I I'm going to do it like this.
So I have this quick alias here called Claude,
which is basically doing this, dangerously skip permissions.
And whatever tool you're using,
every every client, every tool, every agent has something like
that, some some way of using this dangerously skip permission
mode.
And then you kind of have this mode here,
to skip every permissions. And that is quite dangerous. Right?
So
so if you do something now, you don't really know
what Claude is going to do. Right? So, it could
just delete any files of your system.
It could delete,
like,
anything,
that you have worked on. It could actually also happen
to me. Delete the whole gift history and force push
it. So that was kind of a bummer,
because, like, without a gift history, kind of bad.
So,
what you wanna do is you wanna kind of, like,
create an environment where this thing is not
doing something that it's not supposed to do.
And,
yeah, 1 way to do that is
just don't use it and
stick to the old way.
But when sticking to the old way,
you need to configure a lot.
Just as an example,
here in the settings, Jason, of, of of Claude, you
you would probably need to configure a lot of
permissions that this thing is actually allowed to do. Right?
So copy command is allowed, Python is allowed, and so
on. And it's just going to keep to ask and
and, add on top of that. So, basically, you would
need to,
you basically would need to know
all the stuff that you wanna call in advance,
or you would always need to accept
and accept and accept, and and this white list is
going to grow and grow and grow. But I don't
really like this approach.
I kind of, like,
think that's that at a certain point or you should
create an environment where the agent can just YOLO and
and run and do whatever
it wants to do,
but not in a way that my computer gets completely
wiped.
So there are actually ideas on how to do that.
So 1 is just back up your computer
a lot. So it's a MacBook here in my case,
and it's backing up, like, every hour or so. Not
that bad if something goes missing. I can just recreate
it from backup,
but that's still
also,
like, quite quite a quite a lot of work to
do to to just reset the whole computer.
That's why some very
smart people came up with the idea
of creating a a docker container. So just,
or
just like this is the, URL for the repository,
which is called Cloudbox.
And the idea of Cloudbox is,
kind of similar to dev containers. If you've heard of
dev containers,
I will tell you a little bit more about dev
containers later as well. But the idea of Cloudbox is
to provide an environment in in Docker
that you are able to work in the Docker container,
and this Cloud
CLI tool is just contained in the container,
not able to break out of it,
so that your computer is not getting
wiped.
And this tool is actually pretty easy. I mean, you
can look at the GitHub page when you wanna know
how to install it. I just wanna give it a
quick demo.
So what you can do is you can run,
Cloudbox,
and,
Cloudbox will work quite similar to,
Cloud,
only that this 1 sets up a door container.
So
yeah.
So as you can see here,
if I start it up, it now tells me that
there's no available slot.
So what does it mean?
When we look at, our docker images, we can see
that that there's actually a Cloudbox core image here and
a specific
Cloudbox image for another 1 of my projects,
but there is none for this particular
project I'm in. So you can see that because it
says, like, hackers and wizards and demo in it, and
this 1 here is for hackers and wizards fanatics demo
dot com for my website.
So, yeah, I can also, like, see
existing slots.
Oh, sorry. That was wrong.
Now he kicked off the warp agent.
And for this project, there is, no
slot available.
So I just need to create 1,
and that is what we are going to do here.
And now he created a slot,
and and this 1 is unauthenticated.
That means that my cloud code is not authenticated there
yet,
but it kind of, like, created a slot for us
that we can use.
So when I use Cloudbox now, it should find a
slot.
And, yeah, as you can see here, it is kind
of starting up the, the environment. And as it's in
in Docker, and it's kind of, like, downloading some stuff,
and here we go.
We have a fresh
cloud code instance with no settings whatsoever. So let me
just configure it.
And here you can see that I need to,
subscribe now. I will do that. So this is not
going to work,
opening the browser because that is in the container.
So what I need to do is I need to
copy that thing here,
open it in my browser.
You will not be able to see that because I'm
doing it in the background.
And then it gives you
a key, and then you can just add the key
here and press okay.
And then, you are basically locked in here.
Locked in was successful.
Yeah. This is like the standard security note in the
beginning of Claude code,
and then you,
need to
allow,
to to use this
existing
settings
that are here.
And here we go. Finally, we have,
our Cloudbox ready.
And as you can see, it looks
exactly the same. So let me just,
like, clear here
to Cloudbox.
You will see that it actually
just looks like Cloudcode that we've seen before,
and we still have only the 3 modes here,
but I can just use Cloudbox with dangerously sticky permissions.
So it has like, all arguments are passed to to
Claude, basically.
Yeah. Need to sorry. Let me clear it again. Needed
to allow it to do that. Not that important.
But, yeah, now we have this dangerously skip permissions mode
here again.
And, yeah, this is running in the Docker container. So
when I open a new
terminal window here, we can see that there's actually a
container running at the moment. And in this container, this
docker this cloud code CLI is now running in. So
every time this thing tries to break out, it's not
able to do that because in this Docker container, there's
a lot of configurations
regarding network is blocked,
file access is blocked, and so on. It can only
access the stuff that is in the Docker container,
and the only thing that is mounted is basically my
project,
my project's code. So if we ask it what is
this project about,
it will tell it to me again
because it has kind of, like, mounted and copied the
the current project in here.
Yeah. It's going to take a while, but, yeah, as
you can see here, it, like, gives a little bit
of a different output, like like always, but, yeah. And
then there's an interesting thing
here. There's an error,
because a f play was not found. So what is
what is going on here?
When we see let me open a new terminal window.
When we look into my
configuration
of cloud code, we will see that I have a
hook configured here.
Hooks are basically in cloud code a way to kind
of,
if there's something happening in cloud code, you can just
say, what is going to happen if that happens. Right?
So event is something like a notification or
the stop for when it's when cloud code is done,
you can tell them to do something like a command.
And I told him to play a sound.
I find that, like, really helpful
when I'm doing a little bit of of multiple coding
and and multiple cloud instances or even just 1, and
I have it in the background. And I wanna know
when it's done. I can hear the sound, and then
I know it's it's,
it's finished. And it uses AF Play for that. That's
a
command that is available on MacBooks.
But this cloud
box is running in this,
docker container on a Linux machine, so this thing doesn't
really exist.
So this hook is not going to work in here.
And all other commands are also not going to work.
Like, for example, nodes.
Nodes.
Okay. Node is working. So it seems to have Node
in the Docker container.
But what about,
Java?
Okay. So Java. No Java in the Docker container. So
if I have a project
working on Java code, it does not have that in
the Cloudbox.
Yeah. So now I have these types of issues, like,
running it safely in YOLO me mode means, like, running
it safely in this virtual computer box. And now I
need to set up this virtual computer box to include
all the tools that I need.
And also for that, there is a
let me quickly grab that out of my
notes.
There's a way to do that.
You can actually
set up
a profile
for
Cloudbox. So,
let me get out of here again. You can use
this profile command and with profile
oh, sorry.
Sometimes he's kicking off the agent mode here in the
terminal.
You can actually see that there are profiles or they
are there are no profiles.
And you can actually also write your own profiles, but,
Cloudbox provides some
for you as well that you can use.
For example, it has,
Java or
or JavaScript.
So, yeah, basically, we can we can add Java. Oh
my god. Sorry.
The detection here
is not very good. You
can add Java
and
k. So as you can see on the next run,
you will rebuild the Docker container and add Java to
it. So let's see if that actually is going to
happen.
And this time, I'm going to invoke
the terminal
by hand.
Okay. So as you can see, it now tries to
install it. And for some reasons, probably because I'm kind
of like
I don't know why, but
update to okay. So for some reason, it's failing, but
doesn't really matter because what I wanted to show you
is that,
that you can actually configure this kind of, like, environment
to suit your needs and to be able to install
all of the,
con the CLI tools that you need in this container.
And 1 last thing that I wanted
to mention or or show you
is that this is actually not only helpful
for for the YOLO mode to contain it in the
Docker container,
but if you know
if you know dev containers, this is also quite helpful
to share your development environment.
Because what is currently hell happening is you are setting
up the environment for yourself. Right? And if you are
working on a team, your teammates will probably also wanna
have,
the same,
environment that that you have and also be able to
run,
cloud code like you do in the same setup with
the same MCP servers, same agents, and also same available
CLI tools on the machine.
And this is not related specifically to AI or not
relate not not really something new, like, developers wanted for
for a while now.
And,
a few years ago, someone,
kind of came up with the idea of,
of having, something called,
a dev containers. And the dev container is basically something
very similar to this Cloudbox.
It is a
container
where you can,
yeah, that you can use, to run all your
language servers of Versus Code in there. So you are
on the local OS, you are basically using only the
UI and,
the visual
representation
of of Versus Code or any other IDE
like we've seen with Cloud Code.
It's a little bit different there. And in the in
the background,
you can see there's the language server and the computation
and everything,
the running application and so on is running in the
container.
And the very interesting thing here now is that, you
can actually
set up a dev containers by creating
a file inside.
So let me see if I can find it here.
Sorry. You can hear as you can see here, you
can see set up a dev container JSON file
inside your repository.
So, basically,
in the root directory of your,
of your project.
And you can configure
the environment here,
that you wanna use to run this, project or to
be able to develop this project. And most IDEs
support this JSON file, and the second they see it,
they and and you kind of, like, want to work
with it. For example, you open Visual Studio Code, and
it it realizes there's a dev container file.
It then boots up the the setup,
like described here, that it boots up the Versus code
in the container and showing you only the front end.
And you can combine this with the clogged,
box now. So you can use the instead of, like,
here,
use this image, you can use your clogged box image.
There's actually also an attached,
extension
for dev containers to work specifically with with Cloudbox.
And then everybody who will open the project
will immediately when they when they run it,
they will immediately have the setup of using cloud code,
with with all the available MCP servers and so on.
And that's quite helpful to, yeah, to work as a
team and to work safely in this contained,
environment.
Yeah. That's basically
what I wanted to show you,
and,
I hope I'm I'm not,
too far out of the time box,
not too fast.
But,
yeah, if you have any questions, anything you wanna discuss,
I'm very happy to to answer a few of them
or to show you something something more.
Awesome. Thank you very much.
Alright, Benedekt.
Thank you very much. I do have a couple questions
for you here.
Let me
change the view here. And then,
okay,
can you share any real world stories you've heard about
where an AI agent became a security issue?
Mhmm.
So I don't I don't know, like, how to specifically
view security issue. Right? But,
a real world example is that that for
a so for for me, when I'm using this this,
dangerously skipped permissions mode, I often encounter the the issue
that it tries to delete files or move files around
that would destroy my,
my my current computer. Right?
So that happens a lot,
because it at a certain point, it kind of sometimes
goes into a wrong direction and creates a temporary file
in the temp folder, and then it says, well, now
let's delete the temp folder and, like, everything is going
to shits.
I wouldn't say that this is, like, specifically security issue
in terms of someone else is trying to hack your
computer and get out,
stuff. But,
yeah, this is what I see a lot.
That's why I'm quite fond of this Cloudbox setup.
And I've read stuff in in our local,
online news,
feeds
where, people were able to extract user data and so
on, but I haven't
specifically seen that myself. It's only from the from the
news.
Yeah. Sure. Makes sense.
Alright.
1 more question.
It might be a little
broad here, but we will go for it.
How should I approach data privacy when agents interact with
proprietary code or customer data?
So really more in the, like, kind of integration
realm of AI, not necessarily coding with AI. Yeah. This
is the topic that I'm quite familiar with because,
I'm working with clients here in in Germany
to help them,
use these tools for their own developers for the departments.
And a lot of them have this concern as well.
Right? Because, we we Germany is, like, big on data
privacy, which is totally,
good.
And and,
we kind of not try to, like, use anthropic,
cloud code without any restrictions or anything else.
So there are a lot of ways to do that.
Right? You could use, Google Cloud or AWS to run
your own inference models there.
You could even run,
the clock models there,
then you could use your own hardware. Right?
And,
then there are already some some governance tools,
that that will help you kind of restrict
who is allowed to,
access which models. For example, there's a tool called Light
LLM, which which helps you to to do that.
Also budgeting,
in terms of for us, Benedikt? Oh, yeah. Light Light
LLM. I can paste it in the chat as well.
No problem.
It's a it's a governance tool on on,
like,
for for adding guardrails and for adding,
budgets so that individuals
are are not able to overshoot the budgets and so
on.
And,
yeah, I mean, if you are running it on on
your own hardware, then you don't have this proprietary
data problem because it's your own hardware.
And these models are actually also working quite well if
you if you are I mean, you can't run cloud
code on your local hardware, but you could use k
2 or something.
They they are quite good. The newest version from a
few days ago
even said to surpass
Sonnet or Oppos. Not sure.
Yeah. And I mean, it's, like, it's always like a
a, you know, like yeah. That's Yeah. And hearing sometimes
isn't always, like, a very straightforward thing. Yeah. Not really.
Yeah. Yeah. Yeah. And it depends also on the specific
use case and whatever and then yeah. And there are
other tools,
like open router or,
open,
router that you could use together with open code
to then,
also route,
your
your prompts to different LLMs in the background. Right?
So yeah. There's a lot of stuff going on. This
but this is also not all not that stable. So
it's there's a lot of, like, changing in tools and
so on.
Yeah. That's
Yeah. That's my take on that. Yeah. Okay.
Alright.
So any of those tools that you just mentioned, Benedicte,
that, if you have links to it, if you could
throw that in the chat Oh, yeah. Sure. That would
be excellent.
But once again, thank you so very much for being
here. And,
yeah, man. Great talk. Really enjoyed having you.
Yeah. Thank you so much.
I'll be here a few minutes. So, you can I
will chat some in the chat? So if you have
more questions,
don't hesitate to ask me. Thank you so much. Perfect.
Thanks. See you later.
Alright.
Next up,
let me
prepare my screen here.
Next up, we have
We have Garrison
Snelling. Alright. Garrison is the founder of Compute SDK,
and he's going to be talking about Autopilot
AI agents
at the right time,
building agents that actually
know your your code base. So I think this is
gonna be an extremely interesting talk.
A little bit about Garrison before we get into it.
Garrison
builds tools devs actually love to use. Actually, before I
finish that
I almost did it. Let me go ahead and invite
Garrison
to the stage while we're finishing the intro.
Alright.
Garrison, you have your invite if you are listening.
So as the founder of Compute SDK,
Garrison is rethinking how compute plugs into apps and AI
agents, pushing the edge of what developer
platforms
can do.
Before that,
at Stackblitz and at LocalStack,
he led teams, solved
tough tough cloud challenges,
and shared insights that continue to inspire developers and make
their work feel lighter,
smarter, and more fun.
Who doesn't like a little bit more fun in their
life? Alright.
We have now
Garrison
Snelling. Let's
see if he has got the invite yet.
Awesome. Here we have Garrison. Hey, Garrison.
Oh, muted.
Yeah.
Oh,
you froze for a second, Garrison, but you're still muted.
And Garrison will be back.
This is would this would be a great time for
1 of those jokes. Anybody get got a good joke
for the chat?
Jokes? Jokes? Anyone?
2 drums and a cymbal fell off a cliff.
Hey. I pulled that 1 out from childhood. That's good
stuff.
Alright. Garrison, you back with us?
Not yet.
I read his lips.
He said, no. I'm not.
Oh, that's great.
Alright, y'all.
I see the chat lighting up now. We've got people
typing.
Oh, no.
We know his computer works. He made it in for
the panel earlier.
I did have a good childhood, I think.
This sounds like it's gonna turn into a therapy session.
I'm not gonna go down that road.
Anna mentions, how do you console
excuse me. How do developers comfort bugs? They
console them.
That's a good 1.
That is a good 1.
Alright.
Trying to think while we are waiting.
Uh-huh.
I've got it. We're getting near the end of the
day. Oh,
scare some back?
Oh, no. It says you're muted, but I can't hear
you. Oh, wait. Maybe.
You hear me now? I can hear you now.
Okay. It's like a Verizon commercial. It's like a no
yeah.
Awesome, man. Alright. Stage is all yours, my friend.
Thank you. Thank you. Thank you.
Alright.
I am going to share my screen.
I, noticed a comment from Arjun, that said that,
it was a it was a bold move wanting
to
live use cloth because it was down earlier. I didn't
notice that earlier today, and I was actually pretty scared.
I was like, oh, this is really gonna mess up
my talk. So,
let's let's dive right in.
If I remember correctly, I think I have 2 hours.
No. I'm kidding, guys. Don't worry.
Just just joking.
Alright.
So, when I I sent over some text to,
the AIDD folks, and I said, you know, this talk
is gonna be about autopilot
AI agents at the right time, building agents that actually
know your code base,
which is awesome. It's exactly what this is gonna be
about. And then I saw this 1, which I wrote.
I mean, this this was in fact me. I said,
autopilot onboarding agents that replace the weeks of back and
forth Slack messages.
Intelligent con contextual guidance. So we're actually gonna we're
probably
not gonna get to the last last 1. But, really,
what I wanna do is I actually wanna take a
specific repo, and I'll show you what that is in
a few minutes.
And I wanna break down kind of the architecture of
how I think about actually getting
a agent at just the right time. A strategic agent
is kinda how we refer to them,
in the actual AIDD core, courses.
That can actually help you with onboarding. And this one's
a bit of a unique,
it's it's it's interesting. We we we can get into
that. But before
we get into that, what I wanna talk about is
actually, this sentence here.
You've mastered the basics of AI coding assistance,
but are you still treating them like fancy autocomplete?
Most developers are stuck in the AI chat moment. And
I alluded to this in the q and a section
or the the the the live panel.
But,
hash brown dot dev is actually a,
which I'll x out of this. And I'll post this
link
in chat if I can find chat. I was going
to oh, I found chat. We did it.
Push push that link in chat. And, essentially, Hash Brown
is actually it helps you build generative user interfaces. It's
it's, pretty pretty interesting.
What it allows you to do is it actually allows
you to,
convert your existing components. They have support for React and
Angular.
I don't think they actually have Vue yet, so we
need to get on them about that, guys. So let's
reach out to them.
And it's very interesting because,
if if I I'll put this demo here for just
for a split second, see if it can load.
Because what it oh, I'm gonna mute that. What it
actually what it actually does, it allows you to actually
connect these components to as a tool. So in this
case,
Mike's actually connecting these lights as tools to the LLM,
directly. And so this is, just a little snippet. You
should go check this out. Hashtag guys are great,
as far as, kinda creating novel and interesting UIs, which
is really cool. But
we wanna talk about
strategic
agents.
And, specifically, we wanna actually look at some onboarding problems,
which is
kind of an interesting topic.
I, recently hired somebody, and I made a template inside
of Basecamp,
which is the project management tool that we use.
And I wanted to kinda, you know, outline some some
some items for them. And but then I also wanted
to make it a reusable template.
And so I did exactly that. But when I was
approaching this problem set, I I thought back to kind
of this moment, and I thought, well, this is interesting
because
my business is going to change. Our needs are going
to change. And wouldn't it be interesting
if actually the tasks themselves
adjusted as we update our documentation?
And now we're not, you know, we're we're an early
stage start up very early. Compute SDK dot com. Go
check us out, by the way.
I should pull this up faster. Boom.
Essentially, you can, work with,
Sandboxes directly. We, it's multiple providers. It's pretty sweet.
So we don't have a lot of documentation,
but there are some companies, and ironically, I'm alluding to
this already, who have their handbooks actually public and in
Git.
And what I really like about this artifact,
is is a is a few things. Number 1, this
is kind of not a typical use case because it's
actually a handbook for an a company. And so you
can kind of go 1 of 2 ways with this.
You can actually,
look at
kinda the
you can actually look
at this from a normal from
a everyday person perspective.
Specifically, this is just a handbook, and you can look
at this from a developer's perspective. And these are kind
of the moments in time where I really like to
kinda guide people and and
and show exactly where these sort of strategic agents work.
They don't just work for developers. They also work for
kind of, maybe a a whole company of some kind.
So what we're gonna do is we're actually going to,
excuse me, fork this repo,
and I'm gonna put it inside of compute SDK.
And we're gonna call it VC handbook, and we're just
gonna create the fork real fast.
And what I did previously was I actually came up
with a bit of a prompt, and I I I
I I I gave this a try kind of entirely
on my own. And so,
actually, before we get to that, let's let's do some
drawing. So we have,
kind of a handbook.
And the way that this is going to
what I have in my mind, the architecture that I
kinda have,
is this concept of step number 1 would be, number
1,
create
a
onboard
onboarding GitHub issue,
and step number 2 would trigger
a GitHub action workflow.
This workflow would then
call out to Claude
or anthropic rather. Let's let's use the use the right
names and would then
actually
make
workflow.
Got a typo.
Make a series
of GitHub issues.
And so what we see kinda over here, what I
initially came up with was I came up with the
concept of an issue template, which is just a generic
kind of,
GitHub thing. Right? That that that's that's a service that
they provide. But the thing that's interesting is that we
can actually feed in the context of this whole repo,
into our actual,
job itself. And so what what we're gonna do and
we're obviously not we're we're actually not gonna code this.
We're going to try and get,
Claude and and Propic to actually make this for us.
And so,
let's see. Kind of the the idea that well, let's
let's get some boxes on this page.
So
task
or, actually,
GitHub issue.
And we're gonna be maybe more specific than this. GitHub
issue, GitHub
onboarding issue. Make this a little wider.
And this will sit inside of the actual
repo.
And then what we can do is we can attach,
an arrow here, and we'll get another box going.
And then, essentially, what we wanna do is we want
to fire off this
GitHub
action.
And then, actually, even inside of this GitHub action, we
kinda wanna make this a little bit broader,
which my boxes are getting a little messy here. Sorry.
Oh, we're gonna do this, and that arrow needs to
point to that box. Nice.
And, actually, I'm gonna delete this Basecamp handbook here because
we don't really need that. Oh,
at this point, you get get the idea. So from
here, what we wanna do inside of the GitHub action
itself is we actually want to,
you know,
check out
actually, wait.
Let's delete that. Let's move this in here. So inside
of the actual GitHub action, we want to
already accounting for that. We want to trigger GitHub action.
So that GitHub action actually is triggered,
when a user would create an issue in theory. And
then,
work out workflow call is anthropic.
So then what we need next is
checkout code base.
And then finally, what we wanna do is what's the
what what what what's what's the next step here?
And 1 of the key things that I'm doing here
is I wanna show you kinda how I think about
actually building,
kind of in real time these sorts of architecture
strategic agents. And so what you're kinda seeing is exactly
how I think about,
building the GitHub action for
a onboarding issue. And you're also seeing that I really
care about,
typos, and that I also do a lot of typos,
which is interesting.
Check out code base, workflow,
call outs anthropic,
make a series of GitHub issues.
We probably want to convert the code
base to,
some sort of JSON or, you know, to some extent.
I'm not necessarily super tied to that. So at this
point, what what I think I I have here is
I think I have enough
in my mind to kinda
be able to switch over to actually prompting something. So
we have kind of this this basic sketch.
The thing that I'm a little bit unsure about is
actually how we're gonna go from essentially anthropic giving us
bad API results and then making,
get get out of issues. So let's go take a
look at that. So I went ahead and had
the I started out with just kind of a a
a basic kind of
prompt that I that that I had built, and that
actually ended up kinda working on some. So let's take
a look at that.
So intelligent automation with cloud API. And so what we're
seeing here is we have a core concept, repository aware
task creation task generation. Excuse me. And so, essentially, the
the and this is why this model kinda works pretty
pretty beautifully.
It's this concept of essentially
creating 1 task that ends up creating many tasks, which
is pretty sweet. So this this doesn't just
work for the the base camp handbook. It works for
a lot of different code bases.
And so let's take a look at this.
So the phase 1 is actually repository collection.
And so that's what kind of what we outload outlined
here with checkout code base. Right? And then
phase 2 or, actually, let let's review. So it's looking
at the
getting some context, which is great.
Reference our anthropic,
API key and then
employee name or role.
And then let's keep go oh, and there was a
curl request in here. So the curl is actually call.
So we're using curl as a response. Not super in
love. When I see this sort of thing in a
prompt, I'm not super in love with it, because
curling does work, but, eventually, I do prefer it end
up in, like, a proper,
you know, structured language of some kind, JavaScript.
And then enhanced issue template. This one's pretty self explanatory.
Okay. I I kinda like that. I I'll I'll end
up glancing over that. That's that's interesting.
Dynamic context aware prompt. So this is pro this is
actually super important. You're an expert, so we're telling the
so this is probably where I wanna really clue in
actually into
kind of building the strategic agent. You're an expert in
task generation
test analyze this repository's current state and generate actionable,
task. And then we pass in the repo context, which
was previously defined. Okay. And, of course, all this is
kinda pseudo code because it is, a prompt, and we're
gonna fill them all the details when we actually start
to create something.
And then you'll notice here that and this is something
that I thought was especially interesting. Their GitHub has a
get a g h CLI, which essentially allows you to
get all of the GitHub u UI, but on inside
of your CLI. And so you can imagine a world
where you actually you know, in this scenario where we're
trying to make onboarding tasks based on a bait on
a on a manual, we actually could run GH create
issue,
and then not actually have to use any UI. So
you can actually further automate this even more so. There's
always the OktaKit API as well.
Let's see. We already have our andropic key set, so
that's good. Okay.
Let's
copy this in,
and we are going to go over to
actually, I need to
clone that repository, don't I?
Clone.
Get clone. We'll clone this real fast, and and then
we will CD into BC handbook,
and then we're gonna open up open code. Open code
is kinda my preferred agent right now. It's pretty pretty
interesting.
And we're really hoping that anthropic is up, folks. So
if it's if it's not,
let me know quickly.
We're gonna hop back over to our prompt here, copy
it in. And before we get started with this prompt,
I like to kinda
you know I'm in plan mode. That's 1 of the
things I wanna highlight, which is which is pretty sweet.
And so I before we kinda get started actually writing
code,
I wanna get some version of a
it's kind of a bad question here, but here is
my existing
prompt.
What do you think about it and how it works
with this project?
And so I'm kind of getting the baseline. I'm telling
it to, of course, review the review the actual,
project itself, and then I'm giving it my prompt. And
so I'm kinda doing 2 things at once, which is
great.
There are critical misalignments.
So, again, we're running this live, and so this is
this is something interesting.
Static documentation versus active code based. Your prompt assumes a
software project with active development, but this is the 37
signals employee handbook, a collection of static markdown files
that code package dot JSON.
So I'm scrolling down.
Instead of your technical onboarding system, this project needs policy
based task generation, create tasks. No.
So the so this is this actually shows you exactly
kind of a misalignment. Right? The thing that I have
in my mind is for us to be able to
take this handbook and actually create,
an onboarding guide, a custom onboarding guide based on the
employee handbook,
for new employees. And so it has not been you
know, I have not successfully
transferred this to the actual LLM. So, what we now
need to do is actually to clarify. Right?
I would actually
like for this to work with
the 37
signals
employee
handbook,
Much like a traditional
code base,
I think
it would actually
be cool
if you could
create
an issue
in GitHub
and then end up creating
a whole dynamic
let's misspelled that. Dynamic,
set of tasks.
Essentially
based
on the actual
context of the policies.
So if I would have actually told it to go
and generate,
I'm I'm gonna kick this off. If I would have
told it to use that prompt,
it would have gone off on a rabbit trail of
essentially viewing the actual,
37 signals handbook as a proper code base, and it
would have misinterpreted it. So you the critical misalignment that
it called out is actually, it's kind of ridiculously important.
Let's let's look at what it's doing.
That's a brilliant adaption here. Yeah. See, 1 of the
things that I like about AI is that it tells
me that I'm brilliant. That's nice. That's a brilliant adaption.
You're absolutely right. An AI powered onboarding system for an
employee handbook could be incredibly valuable.
Thank you.
Now I see the rich policy content. Yep. Core insight.
Okay. So now we're we're kinda continuing with the plan,
put the plan here.
Intelligent task generation. Yep. That's right.
Good.
Oh, 1 thing that I do need to do actually
is I need to go back over to because we
are going to try and 1 1 shot this. We'll
see how that goes, by the way.
I need to enable issues so that it can make
us some CICD,
and and it can actually do the stuff that we
need it to do. Okay.
We're out of build mode now. Build mode's down here.
Are you ready to build the CICD
pipeline
that allows me so I'm actually reinforcing the principle again
because the the it it kinda perceives the last right.
It's bay the basic kinda skills behind prompting are that
the LOM perceives the, first item in the stream of
stream of history, right, which was our first message and
then the last stream, and then it sorts to the
middle. And so I'm reinforcing exactly what I wanted to
do here, in case there's any sort of miscommunication here,
in case I haven't communicated it clearly. Okay. Are you
ready to build a CICD pipeline that allows me to
create GitHub
issues,
a that allows me to create
a GitHub issue for a new employee
onboarding and then generates
additional
sub issues
for
employee
let's see. And,
for the
employee
to,
complete.
Let's see if we can actually get this to to
work.
We're still in planning mode. That's good.
Alright. So the first thing we're gonna do is we're
gonna create a GitHub issue template. And while we're doing
this, I wanna actually be able to show you,
c l s, oops, c d,
b c handbook l s. And I wanna show you
exactly what we're doing, so we'll open that up. I
think we're gonna need to show you this. Okay.
So what we have here is we have the actual,
GitHub issue template, which is great. So that means that,
you know, we can kinda select exactly what,
yep, what, input input we need, and this will actually
allow for you know, it allows for some version of
a schema,
when you're creating these sorts of,
GitHub issues. I don't know if you've I'm sure you've
used an open source project that used these, which is,
pretty sweet.
And so now let's go back over to see what
it's doing. Now let me create a GitHub workflow directory.
So now we're creating the actual directory itself.
Let me open up that explorer again. We'll do shift
h.
And we don't have it yet, but we will soon.
So here, it's actually writing the CICD pipeline for us,
to make it so that we can actually call out
to Anthropic.
Just FYI, you are going to need you know,
I know this and, and for that you already do,
but you do need an Anthropic key to make this
work. I have it set on my organization level,
so I'm actually not gonna, really try and,
use this too much.
And so we are preparing for it to write.
Let's see.
Uh-oh.
It's taking a little while.
Oh, now we're planning. Okay. Did did it work? It
worked. Okay. Good. I was worried that we were about
to lose lose the anthropic.
So what do we do? We
so when an issue is open, we run this. If
it contains a particular onboarding label, we do that. That's
great. Let's check. Oh, wait. Wait a minute. Let's close
that. And then it has permission to re write and
read, and then we check out the repository. We do
do a bunch of magical stuff.
And then right here, we collect the context information. Nice.
We're actually collecting all the markdown files. That's right. That
feels good. And then we're passing that in, and then
we,
get the GitHub issue body, parsing employee information from issue.
Sweet. Sweet.
Sweet.
And now we're gonna go down to the bottom and
kinda take a look at the actual,
and drop a key. Yep. Okay. So now we passed
in all the handbook information, which we created.
I think I do actually like the fact that it's
using a lot of shell commands, by the way.
I was thinking I want JavaScript, but this is kinda
nice. So we actually have now a prompt.
That's interesting.
Good.
Task generation requirements, each yep. That's great. Now how are
we going so now now now I'm wondering how we're
going to create the task themselves. So we call out
the cloud there, but we're going to need to get
the issues into GitHub. So let's review that work real
fast.
Okay. So we're using a token. We're actually using the
GHCLI.
Okay. Interesting.
And let's see how this works. Are we done yet?
We're not done.
Issue close. Issue number. Close the main onboarding issue since
the tasks are now created.
Interesting. Okay. I didn't I wouldn't have thought of that.
I would have just left the getting the main 1
open, but we can go with that. Let's see. Okay.
The system will generate tasks. Perfect. Ready to test it.
Create an onboarding issue. Okay. Now we're gonna get it
to test it. Okay. Now can
you test it? And right here, we just 1 shotted
it, so we're gonna see if it worked or not.
And this is going to kinda, you know, show you
just exactly how,
how how how good our prompting was. Let me test
the AI powered onboarding system. First, let me check if
we're gonna get repository. We are. And we have this
and we have a, yep. Good. Good. Good. You're ready
to rock and roll.
Yes. We do need to commit our changes. That is
something, in fact, that I did forget in case you're
wondering.
So we committed it. Great.
Push. Yep.
Great as well.
Now we're planning.
Update and plan.
We do have,
I have the secrets
to find in the org.
Just make the issue.
Sometimes I read this like, just make the issue.
But I hope I hope it knows that I don't
mean it like that. You know?
And now let's go see if that issue was created.
Oh, we do need to create the onboarding label. Yep.
Here we go.
Yeah. Generated onboarding. Let's go look at the action. See
if see see oh, we almost got a 1 shot,
but we oh, we have an error. Okay. We can
we can fix this.
Let's go ahead and tell it. It looks like
we have a
error.
Now I know what you're thinking,
and I'm unfortunately about to do something very unsatisfying for
you.
And
you're going to have to come look at the AIDD
courses in order to see
exactly how we solve for these strategic agent
strategic agents.
We did in fact 1 shot this, and this was
live. And so you may not get the actual result
that you're you're looking for, which is a completed agent.
Let's see. Refresh.
Because it's not looking like it.
Perfect. Now let me create oh, alright. We're gonna do
1 more try at this at this,
at this,
this actual agent here and see if, see see if
it works. Oh, we got another issue. It happened too
fast. We have another syntax error.
Alright. We tried to 1 shot it, but the errors
were too much.
But the cool thing that you're seeing here is exactly
kinda my thought process just to go through this on
what how to take a,
kind of a pretty simple repo and then take take
things from a rich context and actually kind of make
agents,
which is pretty cool.
The thing that I do wanted to close this off
with is 1 of the things that we have included
in the strategic agents piece of the AIDP course is
actually proper code review. So you'll see actually, make GitHub
action that does exactly this. You'll also see how to
use CodeRabbit
as well, which is a pretty sweet tool.
And
additionally,
lane, you'll see more and more stuff,
like this. So it's, pretty
pretty pretty pretty amazing here.
Daniel, am I at time?
You are at time.
I knew it. I was so close. I was really
hoping we were gonna 1 shot this, but I don't
think it's gonna work out. No worries. No worries. This,
this is the the problem sometimes with teaching, about AI.
It's, it's just a little unpredictable.
So
no worries at all, my friend.
Do tune in to AIDD where,
we have a little bit more controlled environment,
not live.
And,
you know, we we can see this getting getting going.
Alright.
Garrison,
let me shoot a couple of quick
questions at you. But first,
do you mind, closing your screen out? Yeah. There we
go.
You know it's gonna work the second that I stop
sharing the screen.
That's right. It's exactly right.
Alright.
So let's do this 1.
When integrating into CICD,
how do you prevent agents from accidentally breaking builds
or
introducing unreviewed changes?
Cool thing about CICD is that and the reason why
it's a great artifact, in general, is because it actually
has all those things built into it already. Like, a
CICD job actually needs explicit permission on to commit back.
An example of this is when you have, like, a
release channel.
A lot of times, you might have, like, release
lock files for lack of a better word that you
wanna commit back to it. And then those specific instances,
when you need CICD, you have to explicitly ask for
access.
And so,
the way that you prevent it is you actually use
kind of just healthy hygiene, healthy,
CICD practices
to not let it happen. It's a it's it I
think it's 1 of the reasons why it's such a
beautiful
artifact for agents.
Sure.
I I've definitely found that marrying, like, good traditional programming
with AI stuff. Like, don't let the AI do AI
do everything. We traditional programming does a lot of things
really well too. So that's a really good
good good point in that, hey. This is something that's
already built into the the CICD stuff.
Awesome. Next up,
last question.
What's
good about
open code?
Oh,
what's good about open code? The the main thing that
it that is particularly good about open code is that
it has language services built into it, or,
LSPs. And so it it gets pretty raw feedback,
when you're actually, like,
you know, working with Go or TypeScript, for instance. So
it knows and it prioritizes
those errors,
as they come in.
So it it it doesn't a lot of times what
you'll see in like, for instance, in this,
system that we just built,
which now we're getting a proper error, which is nice.
What you'll see is that because we use curl, the
actual responses,
are hard to follow.
And so and and when when you're using TypeScript or
Go, when you have access to the language services,
it really makes the actual, context a lot more valid.
Yeah. Makes sense. Very interesting.
I yeah.
Cursor's Cursor's pretty good about that as well. And I
I really like the
I I don't know about Cloud Code. I don't use
it as much as I should. But, it's very helpful
to have that kind of feedback.
Yeah. And the the the interesting thing about OpenCode is
obviously that it has, like, direct access to the file
system, and there's no intermediary. So it it just, like,
can run, you know, like the
I mean, it it it there are a lot of
similarities between cursor, of course. But Sure. But, yeah, it's,
it's it's it's nice. Everybody should use it.
Awesome.
Awesome. Alright. Thank you very much, Garrison.
Enjoyed having you both for the talk and for the
panel as well.
Yeah. I hope you have a great rest of your
day, sir.
Absolutely. See you there. See you.
Alright.
Taking that off the screen.
Alright. So we are sadly
near the end of our event,
but we're going to go out with a bang because
we have
a really phenomenal last speaker for you today.
I will
bring up
his image here.
Alright.
So our last talk for the day will be given
by,
Tejas Kumar.
He's going to talk about how to thrive
as
a developer.
And, I'm going to be
playing a video of this,
talk for him. He's not actually able to make it
live with us here today. So let me go ahead
and bring that up.
Give me just a moment, y'all.
Alright.
And let me
not share my screen here, but we're gonna share a
tab,
because I have to share the tab
to get the audio.
Excellent.
Hope everybody can see this.
Alright.
I think we are good to go. Let me
introduce
Tejas. So, Tejas is a developer advocate at IBM.
He has extensive experience in building developer relations teams and
has previously
led initiatives
at xeta.
Io and Spotify.
He's passionate about mentoring and technology.
Tejas is also an investor and advisor to multiple startups
actively contributing to tech
and the AI
community.
So without further ado, let me,
let you hear from Tejas.
By the way, y'all let me know if you cannot
hear something in the chat.
I'm gonna start playing it. Let me know if y'all
can't hear it.
Hi. I'm Tejas Kumar, and I work on AI developer
relations for Langflow over at IBM.
Today, I'm really excited to talk to you about
AI development and how you can thrive as a developer
in the age of AI. In fact, not just as
a developer, but as a professional.
In our time together, we're going to look at the
shift that's happening,
between AI and more, let's say, traditional ways of working,
specifically around coding, but it applies to a broad variety
of disciplines. We're also going to look at this through
the lens of science and research
to ground ourselves in in reality
as opposed to,
falling prey to sensationalism
and hype. And so,
I I'm really excited to take you on this journey
with me. And towards the end, hopefully, we can uncover
clear ways that we can thrive instead of just survive
in an age of AI. To get started,
I'd love to just start by drawing a parallel,
historically
to the pre computer days, back when there was a
lot of paintings. Let's talk about the Renaissance.
The Renaissance in human history was a period of of
artistic
flourishing. I mean, we've we've all seen, hopefully,
various Renaissance paintings.
For example,
the 1 with with God reaching his hand out to
Adam, in the in the basilica of of, the Vatican.
Anyway,
the Renaissance art, if you haven't seen it, Google it.
It's it's got it's got this trademark
feel to it and that's because it was a time
in history where everyone was enlightened and they wanted to
paint. Right? It was the romantic era.
They didn't know though, right, about about the innovation
that was coming, the innovation of cameras.
And and this is I I I draw a parallel
here because that's kinda where we are today. Right? Is,
you have coding agents. You've got Claude Code. You've got
Cursor.
They do a lot of the job of an engineer.
They just write code and and sometimes it's even better
than stuff I write. Right?
Especially if you're learning a new discipline, like my background
is is front end or web engineering and,
you know, if if I wanted to get into Kubernetes,
I would just have Cloud Code do it and then
explain what it's doing. I actually know Kubernetes. I have
a Kubernetes cluster in production,
because of Claude code, right? That that was better than
me and that taught me how to do this. And
so,
we're facing a similar thing as painters to cameras,
where we're coders and there's coding agents.
No doubt if we rewind to the Renaissance, there would
have been painters who, you know, saw the first cameras
and and must have thought, oh my goodness,
this may replace me similar to us.
But then I I wanna, on a positive note,
also address
some of those painters who
saw the camera and thought,
that's a nifty
new tool
to do the same thing I do. Right? Because if
you if you consider the painter
and the camera,
both of them
do essentially the same thing. They they capture light
and represent that light on some type of medium, a
canvas, a photograph, a paper. In fact, some can say
painters do superior work to a camera because a camera
just observes and captures photons,
where painters can synthesize
photons. They can synthesize they can draw things that are
are not capturable in reality, right? Similar to diffusion models
like Flux by Black Forest Labs or Stable Diffusion, etcetera.
So, AI now has this capability also to synthesize
art as opposed to just capture light.
But if we consider portrait artists, right, they either paint
the king or they take a photo of the queen.
If we consider those skill sets, there was definitely a
replacement.
The point we're trying to make here is
tools and and this is maybe 1 of the, I'd
say,
leading ideas of this presentation
is that tools may vary,
but there are also invariants.
And by definition, invariants do not vary. They are constant.
So you've got variance that are the tools, and you've
got invariants that are the constants. So if we pull
on this thread of painters and photographers,
the tool
is the paintbrush paints canvas.
Similarly, the tool is the camera, the flash, the lens.
What is the invariant? And this is where we start
to understand where we sit as developers in an AI
world.
In the photography example, or photography parable, if you will,
there are invariants. The invariants usually come from first principles
thinking, which is really when you strip a concept down
to its invariants.
Okay? So if we take a first principles approach
to painting
and photography,
trying to identify the invariance,
we'll see that the main invariant here
is just capturing
and retransmitting
light.
Right? And so let's pull on that 1 level more.
What is the invariant here? The invariant is, or the
invariants are
light in the world. There's just there's light. There's light
around us.
We need some type of medium that encodes
this light,
and that for us is air. It's literally the scattering
of photons through air. In fact, the planet Earth appears
blue because of atmospheric scattering. Right? And so we have
this medium that encodes it,
and we need a medium that decodes it. And in
our case of light,
the human visual system or the mammalian visual system, mammals,
even the inset like, all visual systems decode light in
their own way. And so there's 3 invariants here, light
in the world,
a medium that can encode it, and a medium that
can decode it. These invariants
are invariant. They're constant. And we can have a variety
of tools
to serve humanity
along these invariants. The paintbrush,
canvas, or cameras, lenses, or even diffusion models and AI
models that can synthesize art. All but the point I'm
trying to make is the tools vary,
but there are invariants. And as long as we understand
this, as long as we understand invariant
versus tool, we're already starting to set ourselves up for
success.
Everything else on top of the invariants
are just implementation details with trade offs. An example of
the trade off of a camera is you get photons
exactly as they are.
So you can't be creative. Even with filters, you can't
really be that creative.
A trade off with paint brushes is it's not as
precise because you're not directly capturing photons,
but the trade off is you can be as creative
as you want. Right? And and like Dali, for example,
super creative, artist.
And so
the the invariants are constant,
but the the modalities of tools can vary. Okay. This
is so important. That's why I'm spending a lot of
time there. I mean, it's been like 7 minutes, but
this is why we're we're doing this. Okay? So now
what we need to figure out if we want to
thrive in a world where maybe we're concerned something's gonna
take our job is we need to hyperfixate on first
principles in variance
and adjust the tools. It's worth taking a little sidebar
here and acknowledging
AI
will never
be an invariant.
AI will never be an invariant. Because if we think
about invariance, they're typically laws of the universe. Their their
gravity is an invariant.
Everything,
even outside the Earth, experiences gravity. Right? The Earth is
held in place by the sun's gravity. So mass and
its gravity effects
are invariant to their laws.
Light is invariant.
Decoding light, encoding light, these mechanisms that have evolved over
millennia are invariant. Now if we put AI into this,
say, chat room,
is it invariant? Absolutely not. AI varies so much that
there's a new
tool breakthrough, etcetera, every week. Right? Like, I I've heard
a lot of people say they're fatigued. They struggle to
keep up keep up. Excuse me. So, how can we
AI is not invariant and will never be invariant. Okay.
So then if it's not invariant by our current chain
of reasoning,
then it has to be what?
A tool. Indeed. It is absolutely a tool. And since
it's a tool, there's good news for us because we
can just use it to solve invariant problems from invariant
first principles thinking. So then the question becomes, okay, if
AI is a tool, how do we find
invariants?
I've already identified a few, but how do we, like,
understand where an invariant is? And the best way to
do this is to invoke what is called Noether's or
Noether's in German, theorem.
Noether's theorem states that when you see a symmetry
from a variety of angles,
it means that there's an invariant there.
And this may be too abstract, but let me let
me put this into real if if you, for example,
observe a spinning top, right, these things are just like
spinning on a tabletop. If you look at it from
the left side, it's still spinning. If you look at
it from the right side, it's still spinning.
That symmetry is called rotational symmetry. No matter from what
angle you look at a spinning top, it's still spinning
in a circle.
That is rotational symmetry. And according to Noether's theorem,
there's a law of the universe. There's an invariant there.
Indeed, the invariant is the inertia and momentum that the
top experiences.
And similarly,
there's
directional symmetry with gravity. Everything comes down. Therefore,
there's a
lot, AKA gravity.
Noether's theorem is, look, where you see symmetry,
there there is an invariant. Okay. Let's we've
been kind of in the cloud so far, let's bring
this down for us as developers.
What are some invariants that we solve,
as developers?
And if we 0 in on the invariance that we
solve as developers,
we will thrive and we will be irreplaceable
because we're solving against
invariance.
And we're using,
tools to do that, fallible variable tools to do that.
Okay. So,
what are some invariants we can identify as developers according
to Noether's theorem? Well,
1 very strong invariant
is agency.
Is it like, human beings, no matter from what angle
you look at it,
human beings
tend to do better with agency and want a sense
of agency. We want
to be able to trust
systems and and trust things and to trust people, and
we want to have agency over our time as well.
Right? Like, I I want to know like, the times
where I've been the most depressed as anecdotal,
was when I feel like I am just spending all
my time at work and then somebody else needs my
time and this and everything's on fire and low agency
leads to negative health outcomes.
Similar for identity. Agency and identity are invariant with human
beings. In fact, 2021,
paper out of Harvard University and the University of British
Columbia,
titled The Positive Influence of Sense of Control on Physical,
Behavioural, and psychosocial health in older adults, an outcome wide
approach. This paper from Hong and colleagues,
was was tremendous to prove this. It was an 8
year long study,
by Harvard's human flourishing lab.
And it looked at
12998
people. Just imagine, 12998
people. All of these people were adults over the age
of 50. And usually they chose that because adults over
the age of 50 typically tend to have less agency,
than than others. Right? They like when you're young, you
feel like, oh, I can do this. I can go
here. I can do this. When you're when you're older
and you're retired, you tend to feel like you maybe
you're you're past your prime and so on. So they
chose this and it's a massive cohort, and they took
8 years.
And what they did was in the first as soon
as they started, they took an essay of these people's
sense of agency. Hey, how in control do you feel?
How much agency do you feel you have over your
time, over your identity, and so on?
4 years later,
they took another survey, same thing. How how's your agency
and so on. And then 4 years after that, so
in the eighth year, they identified
these people's physical and mental health capabilities.
And what they found was people who reported
relative to control, so within 4 years if they reported
I feel more agency,
these people also
lived longer, had better psychosocial
health and better physical health as well. Like every every
marker went up. And so we can observe this, I
mean, 12998
people over 8 years, like that's many, many angles. And
if we observe this over many angles, we can see
an invariant, no doubt, is human agencies. I want to
feel in control.
Pause and consider yourself. Like the reason you get frustrated
when you have to click on cookie banners, right? Is
because you value your time, you want to do, you
wanna get back as much time as you can. So
then you have the agency
to do whatever you want, spend it with family, go
out with friends, whatever it may be, right?
And And so for us as human beings, we agency
is a strong invariant. We want to reduce
uncertainty
and risk
so that we can preserve our resources. I don't wanna
lose a bunch of money accidentally, and then I can
use my agency with the resources I have to do
what I want to be happy.
We want to get back more time again because time
is very closely correlated with agency. If I have a
week free, I can do whatever I want literally. I
I want a week to do whatever I want. That
language is agency.
There was a there was a study by by Google,
in 2009,
led by Brutlag that shows
that when they so they did an experiment, they doubled
the latency,
on their search there. So from 200 milliseconds, they raise
it to 400 milliseconds just to see what would happen.
And 0.6 percent,
of people just stopped using search because it was too
slow. Right? Because it impinged on the agency and said,
hey, you don't you don't actually you you lose time
here. And if you lose time, you don't have as
much time to do whatever you want, etcetera, etcetera. So
agency is is the invariant
we protect. And and it usually,
manifests through risk and uncertainty,
or time
or resources like money and so on. Let's let's make
it really practical
and think about this
from a software engineering perspective. This means we provide as
less friction as possible in terms of UX
and allow users to do the most
meaningful trustable work in the lowest amount of time.
Right? That's the whole invariant we solve as software engineers.
So let's consider a calendar app. Let's make it super
practical. We're building a calendar application, and and there's a
demo coming up here.
We build a calendar application.
And
people don't use a calendar application just to browse and
book. Like, it's not that simple. It's not like I
just wanna look at my calendar and book something. It's
more I people, we all use it to find the
best time
and then to protect me
in case somebody cancels or if plans change. Like, it's
the need a calendar app solves is not just browsing
book, it's find the ideal time and protect me in
case of uncertainty.
If we consider this example even for like travel booking,
right, it's the same thing. It's not just like I'm
gonna go find a flight and pay for it. It's
more I need to find the best flight at the
right price point, and I need to be covered in
case the flight is canceled. Like we always want the
most optimal thing that preserves our agency, and we need
to cons continually preserve our agency in case of uncertainty,
meaning I need some type of insurance. I need to
know that this is the best 1, the best time
slot. And in case someone cancels, I'm covered. I get
a notification,
they don't book an event
in parallel to mine, etcetera. So those are the needs.
Those are the invariants we solve.
If we identify those invariants,
we can do so with software. We can solve them
with software either by writing code like Google Calendar. Google
Calendar is a great tool, or this is where we
can even use AI agency,
for ourselves to thrive as developers. And so, let's explore
both of those now just like by way of a
demo here. So I've got my let me let me
open up my calendar.
This is my calendar. Welcome.
And as you can see, I have quite a bit
of agency.
And so,
you know, I can I could find okay? What's the
best slot for lunch with Paulie? It's probably here. So
I'm just gonna do, like, lunch,
lunch
with Paulie right here.
K. So now I did that, and I found the
best slot, and I know, like, if it's uncertain. So
that's I've met my need. Right? I've I've done it.
That's that's 1 tool which use software, or we can
use AI, which again is not invariant. So I can
do the same thing with AI. So let's let's go
back here, and this time I'm gonna use Langflow. A
great way I'm gonna build I'm going to literally just
build my own AI agent, my own personal AI agent
for me because I can with LifeFlow. LifeFlow is open
source. It runs locally.
You can host it if you want. It's really
you do you, you know? But
it also helps us reason about first principles agents. That
that's why I'm gonna use it here today. So,
let's go
and add a chat input.
I'm gonna zoom out to a hundred percent, and then
we'll do a chat output as well because I wanna
be able to talk to my agent, of course, and
then we'll do an agent,
right here. It's my agent.
Hi, agent. And I'm just gonna wrap up input and
output just like that.
And I'll go to the playground. I'll say, hi, agent.
Do you work?
Yeah. Cool. Seems to be working.
Now I'm going to give this agent access to my
calendar. Again, we're solving against the same invariant, but in
different ways. Okay? Let me give this access to my
calendar. So I'll come here, and I'll get Google Calendar
right here.
And this is using a great tool called Composia. So
now what what can I what do I want this
agent to do for me? And notice I'm using the
word agents here on purpose because
the purpose of AI this is the big thesis. Right?
This is the point. Don't miss it.
The purpose of any tool
is to serve an invariant. The invariant with human beings
and software is usually agency,
and that's why we have AI.
Agents. We have agents
to do the stuff for us to preserve our agency,
thus serving the invariant.
That's the point of this whole talk. I hope you
got it. Okay. So let's let's preserve my agency. So
I want you to be able to
update calendar list entries, and we'll turn on tool mode,
and we will
yeah. Now we can do many things. That's what I
wanted. So we can let's let's select none. Let's see.
You can update a list entry. You can update a
calendar.
You can create an event. Actually, I wanna scope this
a little bit.
You can list events. You can move events.
You can find events.
You can
Quick add events. I think this should be fine. Can
you insert events? Yeah. Cool. Let's so these are the
things you can do.
These are the things my agent can do. Check it
out. Just like that. Let's use 4 0 because it's
a bit more capable. And now, you know, I can
go here. Okay.
Sure. March September 20 25. Right?
And let's
and so it's gonna find an event that your lunch
with Polly is scheduled for today, September 9 20 25
from 1 to 2 PM European world. Cool. Let's say
now let's go back to the calendar. It's it is
1 to 2 PM. Let's say move it to from,
like, 12 to 1. Right? So we'll say move it
an hour earlier.
In fact, let's just open this in split view.
Right?
And the agent that I'm building for myself should just
do job. So,
let's watch here maybe.
There we go. So it created a duplicate
and made it slightly longer. Let's say,
delete the 1 at 1 to 2 PM.
And I I I wanna focus on this on purpose
because there's some really great points here.
See this? Do you see this? Delete the 1 starting
at 1 PM.
And now,
come
on. It's gonna find the event,
and
it seems it's already been deleted. I think it I
just didn't give it permission. So I I all of
that by the way was super intentional. I wanted you
to see that, because
what what let's go what what is the invariant? The
invariant is my agency and my time.
Did this
exercise, this demo
serve my time or my agency?
It yes and no. Right? That's kind of where AI
is today. It it it did in that cool, I
could make a calendar event by talking
or by typing.
No, because
the AI messed, I made a double 1 and then
didn't know how to delete it. And so then I
ended up losing time and losing agency. This is exactly
where we find ourselves with AI today. All of this
was on purpose. Right? Is
we get out of AI
service towards the invariant
as good as our input and our tools. This is
also where we find the space for something called context
engineering. Unfortunately, we don't have the time to go into
that. If this was like an hour presentation, we'd spend
a deep dive on context engineering here. But
with AI as a tool, we need to know that
garbage in equals garbage out. If you can provide really
great context, for example, I could have provided the right
context in my prompt
and said do not create a new event, move the
existing event, etcetera, etcetera. I could have done that, but
then there's also a time cost there that eats into
my invariant, my agency. So I want to do more
with less.
And in this case, if I think about context engineering,
it's doing more. Are you understanding
the nuance here?
I hope you are because that's the whole point of
this talk. Ultimately,
all software
must serve the invariant of human agency.
And and it does that by faithfully
representing
state.
This is your calendar.
Faithfully and reliably
transforming state. I'll make an event. I'll move an event.
And faithfully preserving
the main invariant in between state representation
and state transformation. I hope that's clear. That is, like,
the entire purpose.
The tools do not matter.
AI can do it.
It will be able to do it way better over
time. Right? People are saying AI is in a state
of exponential growth. So what you just saw will probably
not happen a week from now.
But where it is right now is it is a
tool and a means to serve the invariant. Right now,
you might decide, you know, cookie banners, Google Calendar is
better. Cool. That's fine because ultimately, we're focused on the
invariant.
As software serves the invariant,
you also need to have a deep your product, whatever
it is you're building as a developer,
needs to have
deep knowledge of the domain specific
invariants as well. Some things are just non negotiable. For
example, you're working in a bank,
and what must be in variant
is user privacy. Right? Like, you must not you must
store probably all your secure tokens on some type of
secure hardware enclave,
so that nobody can read or write, like, face ID
data. That's a domain specific invariant that you must also
preserve and protect.
Another invariant is that good software is typically idempotent because
clocks drift, packets are lost, networks partition,
and in the case of a message arriving twice, if
it's the same message, you need to maybe do the
operation once. You need to have idempotent and durable systems
that can recover from errors. Another invariant,
as we talk about laws of the universe,
is that things go wrong. Network connections drop,
packets get lost, clocks drift. And so in light of
those invariants,
how can we serve our users?
Finally, there's something to be said for authentication. What I
just ran this example was local on my device.
Nothing left my device except
an integration with Google Calendar,
but the entire AI agent, Langflow, whatever is running on
on my device. OpenAI just generated language. Right?
You may obviously need more security. Instead of talking to
OpenAI, you may need to use something like Ollama or
VLLM.
Langflow has support for Ollama,
but that's another invariant is identity.
How much identity are you willing to share, etcetera. Again,
this is gonna be different for health care versus, you
know, consumer apps and so on. So, that's something to
think of. I'd like to start wrapping up by giving
you a checklist for builders
across a variety of form factors,
really to serve
the main question
of how are we serving our invariant with software, which
is personal agency.
And and this is what you need to identify anytime
you want to solve a problem.
Keeping in mind that AI or Cursor is just a
tool,
you need to have answers for these. Question number 1,
what are our domain level in variance?
I just gave the example of banks and hospitals. What
in your specific domain
are in variance that you need to engineer against? Number
2,
where do we lose time
with authentication, with maybe language model generating a lot of
text? Where do we lose time? And how can we
buy it back? Because that again serves agency. The more
time you save your users, the more agency they have,
the better you build software. Number 3,
what in my application needs to be trustworthy and proven
to be trustworthy?
Something like authentication with passkeys,
you need to prove or something like end to end
encryption for a journaling application. Right? This needs to be
provably trustworthy. You need to say anyone
can verify the integrity of this that nobody can read
notes in transit, end to end encrypted notes. Okay? So
those are the 3 questions. What are my domain level
in variance?
Where do I lose time and how do I buy
it back? 2 that's 2 questions with 1. And third
is, what do I need to be trustworthy and approvable,
that that it's trustworthy? Okay?
Let's wrap up by talking about AI as a tool.
AI, no doubt is great.
But again, we're talking about AI in the context of
a tool and as a variance.
Many of you here are using cursor,
and let's think about this in in the context of
invariance, and first principles.
We use cursor
because the promise of Cursor is it preserves our agency
by giving us time back. Right? I don't have to
think a lot. I just like vibe code. I just
say, hey, make this thing and it makes it.
The the problem is,
while it does give you a 10 x speed boost,
you can say, it also gives you
a 10 x speed boost to ship the wrong thing.
Right? And so
we lose time curating cursors output,
which may be the same amount of time that we
spend hand coding things instead of vibe coding things. You
know what I mean? And so I would love to
see, maybe I perform it myself, a randomized controlled trial,
just a completely
clean research experiment,
done with a Vibe coding group and a non Vibe
coding group,
just to see, like, who
actually gets it done faster. Because my thesis right now
for complex problems is that Cursor
gets enough things wrong and you have to manage the
agent,
that eats similar amounts of agency or time as hand
coding things. Similar to this calendar example I shared here.
And so, are we going to be replaced by AI?
How well do we serve the invariant? That that should
be the question. Okay?
Similar, many of you are using Chargegpt.
Chargegpt serves the invariant of agency by giving you control.
For example, you ask a question about something you don't
know, explain to me the citric acid cycle. Right? And
JWGP will give you some authoritative answer, and you feel
like your agency is served because you have more of
a sense of control.
The danger here is
it can hallucinate and be confidently wrong. In fact, it
was in preparing for this, it was many, many times
confidently wrong. I I did use it. And so then
you've got to go check the sources. You've got to
verify.
You still lose time and control because you're just doing
the work anyway.
So what is the net agency win there? Right?
And this I I will I'm I'm wrapping up. I
promise.
The main question to ask
the main question to ask
when building software and when we want to thrive as
professionals
is
what is the net cost
to my invariant?
That's it. And if a tool has a lower net
cost or a higher net benefit, it wins. Right now
AI does not have a lower net cost, at least
in the examples I shared.
Don't even get me started on trust. Right? Like, a
lot of people ChatGPT has a great, agent feature where
it can literally, like, book tickets for you. People don't
use it because it's I mean, how do you trust
that with your credit card information? So there needs to
be more
engineering done towards trust, again, in service of the invariant,
aka personal agency. In fact, many of you listening to
this, I hope you feel entrepreneurial enough to build that
yourself, you can you can just build things and I
encourage it. Okay.
Last thing, before we wrap up,
I promise, is
I wanted to explore the question, maybe we'll do another
talk about this.
Taste, a lot of people say taste is is the
thing that separates humans from the machines taste, you know,
is taste,
invariant.
Meaning if you look at taste from a wide variety
of angles,
according to Noether's theorem,
is it constant?
And is there some type of law around taste?
I don't know.
But I do know that
Apple, for example, has been known for its taste. We
all love iPhones because of the taste that goes into
it. We love a specific style of UI and UX
because we can tell it's tasteful.
We like specific dishes and food and snacks because
they are to our taste.
And
it it's an interesting question because
is taste just the aggregation of the majority
or is it something more? That's a question for further
exploration. I have honestly nothing to share here. I just
thought it was an interesting 1. If you have any
thoughts, please come up to me and talk. I'm I'm
here, or,
if you wanted to
leave a comment, that that'd be great as well. Let's
let's wrap up and then and then we'll end this.
The main the main wrap up is this.
How do we thrive as professionals?
The main answer
is to identify
invariants
and then solve them from a first principles approach.
The the thesis of this talk is that the chief
invariant of software engineering
is personal agency,
and there are multiple tools to solve those problems. AI
is 1,
but there are others. If whoever can solve
against an invariant
for
the lowest possible cost
wins.
Thank you very much.
Alright.
Thank you very much,
Tejas, for
joining us via video here
at AIDD Day.
This is the end of our time together.
Before we close the day, I did I did just
wanna take a moment to thank everyone for
being here. Thank you for spending your time. I know
it was a
long day, 6 hours, probably,
actually closer to 6 and a half hours. So we
packed plenty in there. Thank you very much to all
the speakers
who,
offered up their time and expertise
as well.
We hope you all enjoyed the sessions.
We're able to pick out some,
some some really good tips and tricks along the way
to help you further your development
career.
And of course,
we do want to remind you at the end of
the event here that
the AIDD
Masterclass
is
a way that you can dive deeper into some of
these
AI driven development
topics.
I will share our little poster image here 1 more
time
because it looks better than I do.
But we do, we do really think that this course
is a great opportunity
to learn more about AI driven dev.
Have us, the course creators, you know, got absolutely everything
figured out? No. I don't think so. But we've we've
got a lot figured out. We've we've experimented. We've done,
you know, we've done development
real world AI development inside of our daily workflows.
A lot of the course contributors,
you know, do
real time dev every day and have figured out good
ways to save time and effort with with AI driven
development. And so we invite you to further your
further your experience
based off of the experience that we've had. Don't make
the same mistakes that we do that we've made. And,
yeah, just grow as a developer.
Thank you all so very much once again for being
here.
That's it. Signing off. Y'all have a wonderful
evening. If oh,
last thing.
Do, do make sure to go follow
AIDD
on LinkedIn, Twitter, whatever your social platform of choice is.
We will be sharing out the recordings to this
afterwards. I believe they're gonna go out via email, but
I'm sure we'll also share them,
here on the Circle platform as well in various places.
So just be on the lookout for those if you
missed any of the talks.
Again, thanks for being
here. See you later.